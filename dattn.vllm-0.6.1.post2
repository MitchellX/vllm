diff -Nur orig/CMakeLists.txt dattn/CMakeLists.txt
--- orig/CMakeLists.txt	2024-12-30 22:26:31.412992740 +0000
+++ dattn/CMakeLists.txt	2024-12-30 22:27:03.162886978 +0000
@@ -229,14 +229,32 @@
   # The CUTLASS kernels for Hopper require sm90a to be enabled.
   # This is done via the below gencode option, BUT that creates kernels for both sm90 and sm90a.
   # That adds an extra 17MB to compiled binary, so instead we selectively enable it.
-  if(${CMAKE_CUDA_COMPILER_VERSION} VERSION_GREATER 12.0)
-    set_source_files_properties(
-          "csrc/quantization/cutlass_w8a8/scaled_mm_c3x.cu"
-          PROPERTIES
-          COMPILE_FLAGS
-          "-gencode arch=compute_90a,code=sm_90a")
-  endif()
+  #if(${CMAKE_CUDA_COMPILER_VERSION} VERSION_GREATER 12.0)
+  #  set_source_files_properties(
+  #        "csrc/quantization/cutlass_w8a8/scaled_mm_c3x.cu"
+  #        PROPERTIES
+  #        COMPILE_FLAGS
+  #        "-gencode arch=compute_90a,code=sm_90a")
+  #endif()
+
+  #
+  # dattn extension (DATTN)
+  #
 
+  set(VLLM_DATTN_EXT_SRC
+      "csrc/dattn/torch_bindings.cpp"
+      "csrc/dattn/dattn.cu"
+  )
+
+  define_gpu_extension_target(
+    _dattn_C
+    DESTINATION vllm
+    LANGUAGE ${VLLM_GPU_LANG}
+    SOURCES ${VLLM_DATTN_EXT_SRC}
+    COMPILE_FLAGS ${VLLM_GPU_FLAGS}
+    ARCHITECTURES ${VLLM_GPU_ARCHES}
+    USE_SABI 3
+    WITH_SOABI)
 
   #
   # Machete kernels
@@ -350,6 +368,9 @@
   message(STATUS "Enabling C extension.")
   add_dependencies(default _C)
 
+  message(STATUS "Enabling dAttention extension.")
+  add_dependencies(default _dattn_C)
+  
   message(STATUS "Enabling moe extension.")
   add_dependencies(default _moe_C)
 endif()
diff -Nur orig/compile.sh dattn/compile.sh
--- orig/compile.sh	2024-12-30 22:26:31.412992740 +0000
+++ dattn/compile.sh	2024-12-30 22:27:03.162886978 +0000
@@ -1,7 +1,9 @@
 export VERBOSE=1
+export NVCC_THREADS=8
 export CUDA_HOME=/usr/local/cuda-12
 export TORCH_CUDA_ARCH_LIST="6.0"
 export CUDACXX=/usr/local/cuda/bin/nvcc 
 export PATH=/usr/local/cuda/bin:$PATH
+export PYTHONPATH=/usr/local/lib/python3.10/dist-packages/:$PYTHONPATH
 
-pip install -e . -vvv
+pip install -e . --no-build-isolation -vvv
diff -Nur orig/compile_a100.sh dattn/compile_a100.sh
--- orig/compile_a100.sh	1970-01-01 00:00:00.000000000 +0000
+++ dattn/compile_a100.sh	2024-12-30 22:27:03.162886978 +0000
@@ -0,0 +1,25 @@
+export VERBOSE=1
+#export NVCC_THREADS=8
+export CUDA_HOME=/usr/local/cuda-12
+export TORCH_CUDA_ARCH_LIST="8.0"
+export CUDACXX=/usr/local/cuda/bin/nvcc 
+export PATH=/usr/local/cuda/bin:$PATH
+export PYTHONPATH=/usr/local/lib/python3.9/dist-packages/:$PYTHONPATH
+
+#cd ./build
+#/usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DCUTLASS_ENABLE_DIRECT_CUDA_DRIVER_CALL=1 -DPy_LIMITED_API=3 -DTORCH_EXTENSION_NAME=_C -D_C_EXPORTS -I/root/vllm/csrc -I/root/vllm/build/_deps/cutlass-src/include -isystem /usr/include/python3.9 -isystem /usr/local/lib/python3.9/dist-packages/torch/include -isystem /usr/local/lib/python3.9/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include --expt-extended-lambda -O2 -g -Xptxas=-v --keep -std=c++17 --generate-code=arch=compute_80,code=[sm_80] -Xcompiler=-fPIC --expt-relaxed-constexpr -D_GLIBCXX_USE_CXX11_ABI=0 -MD -MT CMakeFiles/_C.dir/csrc/attention/attention_kernels.cu.o -MF CMakeFiles/_C.dir/csrc/attention/attention_kernels.cu.o.d -x cu -c /root/vllm/csrc/attention/attention_kernels.cu -o CMakeFiles/_C.dir/csrc/attention/attention_kernels.cu.o
+#ccache /usr/local/cuda/bin/nvcc -DPy_LIMITED_API=3 -DTORCH_EXTENSION_NAME=_dattn_C \ 
+#-D_dattn_C_EXPORTS -I/root/vllm/csrc -isystem /usr/include/python3.9 \
+#-isystem /usr/local/lib/python3.9/dist-packages/torch/include \
+#-isystem /usr/local/lib/python3.9/dist-packages/torch/include/torch/csrc/api/include \
+#-isystem /usr/local/cuda/include -DONNX_NAMESPACE=onnx_c2 \
+#--expt-relaxed-constexpr --expt-extended-lambda -O1 -std=c++17 \
+#"--generate-code=arch=compute_80,code=[sm_80]" -Xcompiler=-fPIC \
+#-DENABLE_FP8 -D_GLIBCXX_USE_CXX11_ABI=0 -MD -MT CMakeFiles/_dattn_C.dir/csrc/dattn/dattn.cu.o \
+#-MF CMakeFiles/_dattn_C.dir/csrc/dattn/dattn.cu.o.d -x cu -c /root/vllm/csrc/dattn/dattn.cu \
+#-o CMakeFiles/_dattn_C.dir/csrc/dattn/dattn.cu.o
+#CMAKE_EXPORT_COMPILE_COMMANDS=ON pip install -e . --no-build-isolation -vvv
+#cd ..
+#export TARGET_MODULES="_C"
+VERBOSE=1 pip install -e . --no-build-isolation -vvv
+#pip install -e . -vvv
diff -Nur orig/csrc/attention/attention_kernels.cu dattn/csrc/attention/attention_kernels.cu
--- orig/csrc/attention/attention_kernels.cu	2024-12-30 22:26:31.473992447 +0000
+++ dattn/csrc/attention/attention_kernels.cu	2024-12-30 22:27:03.164886968 +0000
@@ -21,7 +21,9 @@
 #include <ATen/cuda/CUDAContext.h>
 #include <c10/cuda/CUDAGuard.h>
 #include <algorithm>
-
+#include <cuda_fp16.h> 
+#include <cuda_bf16.h>
+#include <chrono>
 #include "attention_dtypes.h"
 #include "attention_utils.cuh"
 
@@ -42,6 +44,7 @@
 #define MAX(a, b) ((a) > (b) ? (a) : (b))
 #define MIN(a, b) ((a) < (b) ? (a) : (b))
 #define DIVIDE_ROUND_UP(a, b) (((a) + (b) - 1) / (b))
+using namespace std;
 
 namespace vllm {
 
@@ -81,6 +84,42 @@
   return VLLM_SHFL_SYNC(sum, 0);
 }
 
+template <int NUM_WARPS, int THREAD_GROUP_SIZE>
+inline __device__ float propogate_qk_max(float* red_smem, float qk_max) {
+  // Decompose the thread index into warp / lane.
+  int warp_idx = threadIdx.x / WARP_SIZE;
+  int lane = threadIdx.x % WARP_SIZE;
+  
+  // Perform reduction across the threads in the same warp to get the
+  // max qk value for each "warp" (not across the thread block yet).
+  // The 0-th thread of each thread group already has its max qk value.
+#pragma unroll
+  for (int mask = WARP_SIZE / 2; mask >= THREAD_GROUP_SIZE; mask /= 2) {
+    qk_max = fmaxf(qk_max, VLLM_SHFL_XOR_SYNC(qk_max, mask));
+  }
+
+  if (lane == 0) {
+    red_smem[warp_idx] = qk_max;
+  }
+  __syncthreads();
+
+  // TODO(woosuk): Refactor this part.
+  // Get the max qk value for the sequence.
+  qk_max = lane < NUM_WARPS ? red_smem[lane] : -FLT_MAX;
+#pragma unroll
+  for (int mask = NUM_WARPS / 2; mask >= 1; mask /= 2) {
+    qk_max = fmaxf(qk_max, VLLM_SHFL_XOR_SYNC(qk_max, mask));
+  }
+ 
+  // Broadcast the max qk value to all threads.
+  qk_max = VLLM_SHFL_SYNC(qk_max, 0);
+
+  return qk_max; 
+}
+
+__device__ unsigned long long firstTime = 0, newTime = 0, secondTime = 0, thirdTime = 0, fourthTime = 0, fifthTime = 0, sixthTime = 0, seventhTime = 0, totalTime = 0; 
+__device__ unsigned long myindex = 0; 
+
 // TODO(woosuk): Merge the last two dimensions of the grid.
 // Grid: (num_heads, num_seqs, max_num_partitions).
 template <typename scalar_t, typename cache_t, int HEAD_SIZE, int BLOCK_SIZE,
@@ -108,6 +147,7 @@
     const float k_scale, const float v_scale, const int tp_rank,
     const int blocksparse_local_blocks, const int blocksparse_vert_stride,
     const int blocksparse_block_size, const int blocksparse_head_sliding_step) {
+
   const int seq_idx = blockIdx.y;
   const int partition_idx = blockIdx.z;
   const int max_num_partitions = gridDim.z;
@@ -181,13 +221,16 @@
 #pragma unroll
   for (int i = thread_group_idx; i < NUM_VECS_PER_THREAD;
        i += NUM_THREAD_GROUPS) {
+    
     const int vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE;
+
     q_vecs[thread_group_offset][i] =
         *reinterpret_cast<const Q_vec*>(q_ptr + vec_idx * VEC_SIZE);
   }
   __syncthreads();  // TODO(naed90): possible speedup if this is replaced with a
                     // memory wall right before we use q_vecs
 
+
   // Memory planning.
   extern __shared__ char shared_mem[];
   // NOTE(woosuk): We use FP32 for the softmax logits for better accuracy.
@@ -293,6 +336,7 @@
       // This includes a reduction across the threads in the same thread group.
       float qk = scale * Qk_dot<scalar_t, THREAD_GROUP_SIZE>::dot(
                              q_vecs[thread_group_offset], k_vecs);
+
       // Add the ALiBi bias if slopes are given.
       qk += (alibi_slope != 0) ? alibi_slope * (token_idx - seq_len + 1) : 0;
 
@@ -306,28 +350,8 @@
       }
     }
   }
-
-  // Perform reduction across the threads in the same warp to get the
-  // max qk value for each "warp" (not across the thread block yet).
-  // The 0-th thread of each thread group already has its max qk value.
-#pragma unroll
-  for (int mask = WARP_SIZE / 2; mask >= THREAD_GROUP_SIZE; mask /= 2) {
-    qk_max = fmaxf(qk_max, VLLM_SHFL_XOR_SYNC(qk_max, mask));
-  }
-  if (lane == 0) {
-    red_smem[warp_idx] = qk_max;
-  }
-  __syncthreads();
-
-  // TODO(woosuk): Refactor this part.
-  // Get the max qk value for the sequence.
-  qk_max = lane < NUM_WARPS ? red_smem[lane] : -FLT_MAX;
-#pragma unroll
-  for (int mask = NUM_WARPS / 2; mask >= 1; mask /= 2) {
-    qk_max = fmaxf(qk_max, VLLM_SHFL_XOR_SYNC(qk_max, mask));
-  }
-  // Broadcast the max qk value to all threads.
-  qk_max = VLLM_SHFL_SYNC(qk_max, 0);
+ 
+  qk_max = propogate_qk_max<NUM_WARPS, THREAD_GROUP_SIZE>(&red_smem[0], qk_max);
 
   // Get the sum of the exp values.
   float exp_sum = 0.f;
@@ -377,6 +401,7 @@
 
   scalar_t zero_value;
   zero(zero_value);
+
   for (int block_idx = start_block_idx + warp_idx; block_idx < end_block_idx;
        block_idx += NUM_WARPS) {
     // NOTE(woosuk): The block number is stored in int32. However, we cast it to
@@ -398,7 +423,7 @@
     L_vec logits_vec;
     from_float(logits_vec, *reinterpret_cast<Float_L_vec*>(logits + token_idx -
                                                            start_token_idx));
-
+    
     const cache_t* v_ptr = v_cache + physical_block_number * kv_block_stride +
                            kv_head_idx * kv_head_stride;
 #pragma unroll
@@ -474,6 +499,7 @@
         const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
         if (row_idx < HEAD_SIZE && lane % NUM_V_VECS_PER_ROW == 0) {
           accs[i] += src[row_idx];
+
         }
       }
     }
@@ -625,6 +651,13 @@
     red_smem[warp_idx] = max_logit;
   }
   __syncthreads();
+
+  //if(blockIdx.x ==0 && blockIdx.y == 0 && blockIdx.z == 0 && threadIdx.x == 0) {
+    //printf("[%d, %d, %d, %d]: gridDim.x-%d, gridDim.y-%d,gridDim.z-%d, blockDim.x-%d\n", blockIdx.x, blockIdx.y, blockIdx.z, threadIdx.x, gridDim.x, gridDim.y, gridDim.z, blockDim.x); 
+  //int tbs = gridDim.x * gridDim.y * gridDim.z;
+  //  printf("reduced threadsblocks-%d\n", tbs);  
+    //printf(" threadBlocks-%d: gridDim.x-%d, gridDim.y-%d,gridDim.z-%d, blockDim.x-%d\n", blockIdx.x, blockIdx.y, blockIdx.z, threadIdx.x, gridDim.x, gridDim.y, gridDim.z, blockDim.x); 
+  //}
   // Reduce across warps.
   max_logit = lane < NUM_WARPS ? red_smem[lane] : -FLT_MAX;
 #pragma unroll
@@ -668,6 +701,406 @@
   }
 }
 
+template <typename scalar_t, typename cache_t, vllm::Fp8KVCacheDataType KV_DTYPE, 
+         int BLOCK_SIZE, int HEAD_SIZE,
+         int NUM_THREADS, bool IS_BLOCK_SPARSE, int PARTITION_SIZE = 0>   
+__global__ void dattention_kernel(
+  float* __restrict__ exp_sums,  // [num_seqs, num_heads, max_num_partitions]
+  float* __restrict__ max_logits,  // [num_seqs, num_heads,
+                                     // max_num_partitions]
+  scalar_t* __restrict__ out,  // [num_seqs, num_heads, max_num_partitions, head_size]
+  scalar_t* __restrict__ q, // [num_seqs, num_heads, head_size]
+  int layer_offset,        // layer offset in the units
+  int whole_block_size,    // whole block size (bytes), including KV of all layers together
+  int max_seq_len,
+  const int64_t* cache_row_mapping,  // [num_tokens]  record cache ptr for this token
+  const int64_t* cache_col_mapping,  // [num_tokens]  record token index of the sequence
+  const int* __restrict__ seq_lens,      // [num_seqs]
+  const int q_stride, 
+  const int num_kv_heads,               // [num_heads]
+  const float scale,
+  const float* __restrict__ alibi_slopes,  // [num_heads]
+  const float k_scale,
+  const float v_scale,
+  const int tp_rank,
+  const int blocksparse_local_blocks, const int blocksparse_vert_stride,
+  const int blocksparse_block_size, const int blocksparse_head_sliding_step) {
+  const int seq_idx = blockIdx.y;
+  const int partition_idx = blockIdx.z;
+  const int max_num_partitions = gridDim.z;
+  constexpr bool USE_PARTITIONING = PARTITION_SIZE > 0;
+  const int seq_len = seq_lens[seq_idx];
+  if (USE_PARTITIONING && partition_idx * PARTITION_SIZE >= seq_len) {
+    // No work to do. Terminate the thread block.
+    return;
+  }
+
+  const int num_seq_blocks = DIVIDE_ROUND_UP(seq_len, BLOCK_SIZE);
+  const int num_blocks_per_partition =
+      USE_PARTITIONING ? PARTITION_SIZE / BLOCK_SIZE : num_seq_blocks;
+
+  // [start_block_idx, end_block_idx) is the range of blocks to process.
+  const int start_block_idx =
+      USE_PARTITIONING ? partition_idx * num_blocks_per_partition : 0;
+  const int end_block_idx =
+      MIN(start_block_idx + num_blocks_per_partition, num_seq_blocks);
+  const int num_blocks = end_block_idx - start_block_idx;
+
+  // [start_token_idx, end_token_idx) is the range of tokens to process.
+  const int start_token_idx = start_block_idx * BLOCK_SIZE;
+  const int end_token_idx =
+      MIN(start_token_idx + num_blocks * BLOCK_SIZE, seq_len);
+  const int num_tokens = end_token_idx - start_token_idx;
+
+  constexpr int THREAD_GROUP_SIZE = MAX(WARP_SIZE / BLOCK_SIZE, 1);
+  constexpr int NUM_THREAD_GROUPS =
+      NUM_THREADS / THREAD_GROUP_SIZE;  // Note: This assumes THREAD_GROUP_SIZE
+                                        // divides NUM_THREADS
+  assert(NUM_THREADS % THREAD_GROUP_SIZE == 0);
+  constexpr int NUM_TOKENS_PER_THREAD_GROUP =
+      DIVIDE_ROUND_UP(BLOCK_SIZE, WARP_SIZE);
+  constexpr int NUM_WARPS = NUM_THREADS / WARP_SIZE;
+  const int thread_idx = threadIdx.x;
+  const int warp_idx = thread_idx / WARP_SIZE;
+  const int lane = thread_idx % WARP_SIZE;
+
+  const int head_idx = blockIdx.x;
+  const int num_heads = gridDim.x;
+  const int num_queries_per_kv = num_heads / num_kv_heads;
+  const int kv_head_idx = head_idx / num_queries_per_kv;
+  const float alibi_slope =
+      alibi_slopes == nullptr ? 0.f : alibi_slopes[head_idx];
+
+  // A vector type to store a part of a key or a query.
+  // The vector size is configured in such a way that the threads in a thread
+  // group fetch or compute 16 bytes at a time. For example, if the size of a
+  // thread group is 4 and the data type is half, then the vector size is 16 /
+  // (4 * sizeof(half)) == 2.
+  constexpr int VEC_SIZE = MAX(16 / (THREAD_GROUP_SIZE * sizeof(scalar_t)), 1);
+  using K_vec = typename Vec<scalar_t, VEC_SIZE>::Type;
+  using Q_vec = typename Vec<scalar_t, VEC_SIZE>::Type;
+  using Quant_vec = typename Vec<cache_t, VEC_SIZE>::Type;
+
+  constexpr int KV_HEAD_STRIDE = HEAD_SIZE * BLOCK_SIZE; 
+  constexpr int NUM_ELEMS_PER_THREAD = HEAD_SIZE / THREAD_GROUP_SIZE;
+  constexpr int NUM_VECS_PER_THREAD = NUM_ELEMS_PER_THREAD / VEC_SIZE;
+
+  const int thread_group_idx = thread_idx / THREAD_GROUP_SIZE;
+  const int thread_group_offset = thread_idx % THREAD_GROUP_SIZE;
+
+  // Load the query to registers.
+  // Each thread in a thread group has a different part of the query.
+  // For example, if the the thread group size is 4, then the first thread in
+  // the group has 0, 4, 8, ... th vectors of the query, and the second thread
+  // has 1, 5, 9, ... th vectors of the query, and so on. NOTE(woosuk): Because
+  // q is split from a qkv tensor, it may not be contiguous.
+  const scalar_t* q_ptr = q + seq_idx * q_stride + head_idx * HEAD_SIZE;
+  __shared__ Q_vec q_vecs[THREAD_GROUP_SIZE][NUM_VECS_PER_THREAD];
+#pragma unroll
+  for (int i = thread_group_idx; i < NUM_VECS_PER_THREAD;
+       i += NUM_THREAD_GROUPS) {
+    
+    const int vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE;
+
+    q_vecs[thread_group_offset][i] =
+        *reinterpret_cast<const Q_vec*>(q_ptr + vec_idx * VEC_SIZE);
+  }
+  
+  __syncthreads();  // TODO(naed90): possible speedup if this is replaced with a
+                    // memory wall right before we use q_vecs
+
+  // Memory planning.
+  extern __shared__ char shared_mem[];
+  // NOTE(woosuk): We use FP32 for the softmax logits for better accuracy.
+  float* logits = reinterpret_cast<float*>(shared_mem);
+  // Workspace for reduction.
+  __shared__ float red_smem[2 * NUM_WARPS];
+
+  // x == THREAD_GROUP_SIZE * VEC_SIZE
+  // Each thread group fetches x elements from the key at a time.
+  constexpr int x = 16 / sizeof(cache_t);
+  float qk_max = -FLT_MAX;
+
+  // blocksparse specific vars
+  int bs_block_offset;
+  int q_bs_block_id;
+  if constexpr (IS_BLOCK_SPARSE) {
+    // const int num_blocksparse_blocks = DIVIDE_ROUND_UP(seq_len,
+    // blocksparse_block_size);
+    q_bs_block_id = (seq_len - 1) / blocksparse_block_size;
+    if (blocksparse_head_sliding_step >= 0)
+      // sliding on q heads
+      bs_block_offset =
+          (tp_rank * num_heads + head_idx) * blocksparse_head_sliding_step + 1;
+    else
+      // sliding on kv heads
+      bs_block_offset = (tp_rank * num_kv_heads + kv_head_idx) *
+                            (-blocksparse_head_sliding_step) +
+                        1;
+  }
+
+  // NOTE: cache_row_idx or cache_col_idx can be -1 if the token is padded
+  const cache_t * cache_start = reinterpret_cast<cache_t *>(cache_row_mapping[seq_idx]) + layer_offset + kv_head_idx * KV_HEAD_STRIDE;
+
+  // Iterate over the key blocks.
+  // Each thread block will process one request's one head and one partition (up to 512 tokens)
+  // Each warp will process a block of keys for each iteration.
+  // Each thread group in a warp fetches a key from the block, and computes dot product with the query.
+  for (int block_idx = start_block_idx + warp_idx; block_idx < end_block_idx;
+       block_idx += NUM_WARPS) {
+    // NOTE(woosuk): The block number is stored in int32. However, we cast it to
+    // int64 because int32 can lead to overflow when this variable is multiplied
+    // by large numbers (e.g., kv_block_stride).
+    // For blocksparse attention: skip computation on blocks that are not
+    // attended
+    if constexpr (IS_BLOCK_SPARSE) {
+      const int k_bs_block_id = block_idx * BLOCK_SIZE / blocksparse_block_size;
+      const bool is_remote =
+          ((k_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0);
+      const bool is_local =
+          (k_bs_block_id > q_bs_block_id - blocksparse_local_blocks);
+      if (!is_remote && !is_local) {
+        for (int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
+          const int physical_block_offset =
+              (thread_group_idx + i * WARP_SIZE) % BLOCK_SIZE;
+          const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
+
+          if (thread_group_offset == 0) {
+            // NOTE(linxihui): assign very large number to skipped tokens to
+            // avoid contribution to the sumexp softmax normalizer. This will
+            // not be used at computing sum(softmax*v) as the blocks will be
+            // skipped.
+            logits[token_idx - start_token_idx] = -FLT_MAX;
+          }
+        }
+        continue;
+      }
+    }
+  
+    // computing the starting address of the block for the given layer
+    const cache_t * key_cache = cache_start + block_idx*whole_block_size;
+    //const cache_t * key_cache = cache_start + block_idx*whole_block_size + layer_offset;
+    
+    for (int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
+      // Load a key to registers. Inside a block, each thread group will fetch lane/THREAD_GROUP_SIZe
+      // Each thread in a thread group has a different part of the key.
+      const int physical_block_offset = (thread_group_idx + i * WARP_SIZE) % BLOCK_SIZE; // token index inside the block
+      const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
+      __restrict__ K_vec k_vecs[NUM_VECS_PER_THREAD];
+
+      const cache_t* k_ptr = key_cache + physical_block_offset * x;
+
+    #pragma unroll
+      for (int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+        const int vec_idx = thread_group_offset + j * THREAD_GROUP_SIZE;
+        const int offset1 = (vec_idx * VEC_SIZE) / x;
+        const int offset2 = (vec_idx * VEC_SIZE) % x;
+
+        if constexpr (KV_DTYPE == Fp8KVCacheDataType::kAuto) {
+          k_vecs[j] = *reinterpret_cast<const K_vec*>(
+              k_ptr + offset1 * BLOCK_SIZE * x + offset2);
+        } else {
+          // Vector conversion from Quant_vec to K_vec.
+          Quant_vec k_vec_quant = *reinterpret_cast<const Quant_vec*>(
+              k_ptr + offset1 * BLOCK_SIZE * x + offset2);
+          k_vecs[j] = fp8::scaled_convert<K_vec, Quant_vec, KV_DTYPE>(
+              k_vec_quant, k_scale);
+        }
+      }
+
+      // Compute dot product.
+      // This includes a reduction across the threads in the same thread group.
+      float qk = scale * Qk_dot<scalar_t, THREAD_GROUP_SIZE>::dot(
+                             q_vecs[thread_group_offset], k_vecs);
+
+      // Add the ALiBi bias if slopes are given.
+      qk += (alibi_slope != 0) ? alibi_slope * (token_idx - seq_len + 1) : 0;
+
+      if (thread_group_offset == 0) {
+        // Store the partial reductions to shared memory.
+        // NOTE(woosuk): It is required to zero out the masked logits.
+        const bool mask = token_idx >= seq_len;
+        logits[token_idx - start_token_idx] = mask ? 0.f : qk;
+        // Update the max value.
+        qk_max = mask ? qk_max : fmaxf(qk_max, qk);
+      }
+    }
+  }
+  
+  // Perform reduction across all threads in the same thread block
+  qk_max = propogate_qk_max<NUM_WARPS, THREAD_GROUP_SIZE>(&red_smem[0], qk_max);
+
+  // Get the sum of the exp values.
+  float exp_sum = 0.f;
+  for (int i = thread_idx; i < num_tokens; i += NUM_THREADS) {
+    const float val = __expf(logits[i] - qk_max);
+    logits[i] = val;
+    exp_sum += val;
+  }
+  exp_sum = block_sum<NUM_WARPS>(&red_smem[NUM_WARPS], exp_sum);
+
+  // Compute softmax.
+  const float inv_sum = __fdividef(1.f, exp_sum + 1e-6f);
+  for (int i = thread_idx; i < num_tokens; i += NUM_THREADS) {
+    logits[i] *= inv_sum;
+  }
+  __syncthreads();
+
+
+  // If partitioning is enabled, store the max logit and exp_sum.
+  if (USE_PARTITIONING && thread_idx == 0) {
+    float* max_logits_ptr = max_logits +
+                            seq_idx * num_heads * max_num_partitions +
+                            head_idx * max_num_partitions + partition_idx;
+    *max_logits_ptr = qk_max;
+    float* exp_sums_ptr = exp_sums + seq_idx * num_heads * max_num_partitions +
+                          head_idx * max_num_partitions + partition_idx;
+    *exp_sums_ptr = exp_sum;
+  }
+
+  // Each thread will fetch 16 bytes from the value cache at a time.
+  constexpr int V_VEC_SIZE = MIN(16 / sizeof(scalar_t), BLOCK_SIZE);
+  using V_vec = typename Vec<scalar_t, V_VEC_SIZE>::Type;
+  using L_vec = typename Vec<scalar_t, V_VEC_SIZE>::Type;
+  using V_quant_vec = typename Vec<cache_t, V_VEC_SIZE>::Type;
+  using Float_L_vec = typename FloatVec<L_vec>::Type;
+
+  constexpr int NUM_V_VECS_PER_ROW = BLOCK_SIZE / V_VEC_SIZE;
+  constexpr int NUM_ROWS_PER_ITER = WARP_SIZE / NUM_V_VECS_PER_ROW;
+  constexpr int NUM_ROWS_PER_THREAD =
+      DIVIDE_ROUND_UP(HEAD_SIZE, NUM_ROWS_PER_ITER);
+
+  // NOTE(woosuk): We use FP32 for the accumulator for better accuracy.
+  float accs[NUM_ROWS_PER_THREAD];
+#pragma unroll
+  for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
+    accs[i] = 0.f;
+  }
+ 
+  scalar_t zero_value;
+  zero(zero_value);
+
+  for (int block_idx = start_block_idx + warp_idx; block_idx < end_block_idx;
+       block_idx += NUM_WARPS) {
+    // NOTE(woosuk): The block number is stored in int32. However, we cast it to
+    // int64 because int32 can lead to overflow when this variable is multiplied
+    // by large numbers (e.g., kv_block_stride).
+    // For blocksparse attention: skip computation on blocks that are not
+    // attended
+    if constexpr (IS_BLOCK_SPARSE) {
+      int v_bs_block_id = block_idx * BLOCK_SIZE / blocksparse_block_size;
+      if (!((v_bs_block_id + bs_block_offset) % blocksparse_vert_stride == 0) &&
+          !((v_bs_block_id > q_bs_block_id - blocksparse_local_blocks))) {
+        continue;
+      }
+    }
+
+    // Load a key to registers. Inside a block, each thread group will fetch lane/THREAD_GROUP_SIZe
+    // Each thread in a thread group has a different part of the key.    
+    const int physical_block_offset = (lane % NUM_V_VECS_PER_ROW) * V_VEC_SIZE;
+    const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
+    __restrict__ L_vec logits_vec;
+    from_float(logits_vec, *reinterpret_cast<Float_L_vec*>(logits + token_idx -
+                                                           start_token_idx));
+
+    // computing the starting address of the block
+    const cache_t* v_ptr = cache_start + block_idx*whole_block_size + whole_block_size/2;
+    //const cache_t* v_ptr = cache_start + block_idx*whole_block_size + whole_block_size/2 + layer_offset + kv_head_idx * KV_HEAD_STRIDE;
+#pragma unroll
+    for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
+      const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
+      if (row_idx < HEAD_SIZE) {
+        const int offset = row_idx * BLOCK_SIZE + physical_block_offset;
+        __restrict__ V_vec v_vec;
+
+        if constexpr (KV_DTYPE == Fp8KVCacheDataType::kAuto) {
+          v_vec = *reinterpret_cast<const V_vec*>(v_ptr + offset);
+        } else {
+          V_quant_vec v_quant_vec =
+              *reinterpret_cast<const V_quant_vec*>(v_ptr + offset);
+          // Vector conversion from V_quant_vec to V_vec.
+          v_vec = fp8::scaled_convert<V_vec, V_quant_vec, KV_DTYPE>(v_quant_vec,
+                                                                    v_scale);
+        }
+        if (block_idx == num_seq_blocks - 1) {
+          // NOTE(woosuk): When v_vec contains the tokens that are out of the
+          // context, we should explicitly zero out the values since they may
+          // contain NaNs. See
+          // https://github.com/vllm-project/vllm/issues/641#issuecomment-1682544472
+          scalar_t* v_vec_ptr = reinterpret_cast<scalar_t*>(&v_vec);
+#pragma unroll
+          for (int j = 0; j < V_VEC_SIZE; j++) {
+            v_vec_ptr[j] = token_idx + j < seq_len ? v_vec_ptr[j] : zero_value;
+          }
+        }
+        accs[i] += dot(logits_vec, v_vec);
+      }
+    }
+  }
+
+  // Perform reduction within each warp.
+#pragma unroll
+  for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
+    float acc = accs[i];
+    #pragma unroll
+    for (int mask = NUM_V_VECS_PER_ROW / 2; mask >= 1; mask /= 2) {
+      acc += VLLM_SHFL_XOR_SYNC(acc, mask);
+    }
+    accs[i] = acc;
+  }
+
+  // NOTE(woosuk): A barrier is required because the shared memory space for
+  // logits is reused for the output.
+  __syncthreads();
+
+  // Perform reduction across warps.
+  float* out_smem = reinterpret_cast<float*>(shared_mem);
+#pragma unroll
+  for (int i = NUM_WARPS; i > 1; i /= 2) {
+    int mid = i / 2;
+    // Upper warps write to shared memory.
+    if (warp_idx >= mid && warp_idx < i) {
+      float* dst = &out_smem[(warp_idx - mid) * HEAD_SIZE];
+#pragma unroll
+      for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
+        const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
+        if (row_idx < HEAD_SIZE && lane % NUM_V_VECS_PER_ROW == 0) {
+          dst[row_idx] = accs[i];
+        }
+      }
+    }
+    __syncthreads();
+
+    // Lower warps update the output.
+    if (warp_idx < mid) {
+      float* src = &out_smem[warp_idx * HEAD_SIZE];
+#pragma unroll
+      for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
+        const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
+        if (row_idx < HEAD_SIZE && lane % NUM_V_VECS_PER_ROW == 0) {
+          accs[i] += src[row_idx];
+        }
+      }
+    }
+    __syncthreads();
+  }
+
+  // Write the final output.
+  if (warp_idx == 0) {
+
+    __restrict__ scalar_t* out_ptr =
+        out + seq_idx * num_heads * max_num_partitions * HEAD_SIZE +
+        head_idx * max_num_partitions * HEAD_SIZE + partition_idx * HEAD_SIZE;
+#pragma unroll
+    for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
+      const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
+      if (row_idx < HEAD_SIZE && lane % NUM_V_VECS_PER_ROW == 0) {
+        //printf("[%d, %d, %d]: seq_idx-%d, row_idx-%d, accs[%d]:%f\n",blockIdx.x, blockIdx.y, threadIdx.x,seq_idx,row_idx,i,accs[i]); 
+        from_float(*(out_ptr + row_idx), accs[i]);
+      }
+    }
+  }
+}
 }  // namespace vllm
 
 #define LAUNCH_PAGED_ATTENTION_V1(HEAD_SIZE)                                \
@@ -735,6 +1168,7 @@
   dim3 block(NUM_THREADS);
   const at::cuda::OptionalCUDAGuard device_guard(device_of(query));
   const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
+
   switch (head_size) {
     // NOTE(woosuk): To reduce the compilation time, we only compile for the
     // head sizes that we use in the model. However, we can easily extend this
@@ -823,7 +1257,7 @@
     const int64_t blocksparse_vert_stride, const int64_t blocksparse_block_size,
     const int64_t blocksparse_head_sliding_step) {
   const bool is_block_sparse = (blocksparse_vert_stride > 1);
-
+  
   DISPATCH_BY_KV_CACHE_DTYPE(query.dtype(), kv_cache_dtype,
                              CALL_V1_LAUNCHER_BLOCK_SIZE)
 }
@@ -892,6 +1326,7 @@
   // For paged attention v2 kernel.
   dim3 grid(num_heads, num_seqs, max_num_partitions);
   int shared_mem_size = std::max(logits_size, outputs_size);
+
   // For paged attention v2 reduce kernel.
   dim3 reduce_grid(num_heads, num_seqs);
   int reduce_shared_mem_size = 2 * max_num_partitions * sizeof(float);
@@ -996,6 +1431,256 @@
                              CALL_V2_LAUNCHER_BLOCK_SIZE)
 }
 
+#define LAUNCH_DATTENTION(HEAD_SIZE, IS_BLOCK_SPARSE)   \
+  if(use_reduce) { \
+    VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize( \
+      ((void*)vllm::dattention_kernel<scalar_t, cache_t, KV_DTYPE, BLOCK_SIZE, HEAD_SIZE, NUM_THREADS, IS_BLOCK_SPARSE, PARTITION_SIZE>), \
+      shared_mem_size);  \
+    vllm::dattention_kernel<scalar_t, cache_t, KV_DTYPE, BLOCK_SIZE, HEAD_SIZE, NUM_THREADS, IS_BLOCK_SPARSE, PARTITION_SIZE> \
+        <<<grid, block, shared_mem_size, stream>>>( \
+              exp_sums_ptr, max_logits_ptr, tmp_out_ptr,\
+              query_ptr, \
+              layer_offset, \
+              whole_block_size, max_seq_len, \
+              row_ptr, \
+              col_ptr, \
+              seq_lens_ptr, \
+              q_stride, num_kv_heads, scale,  \
+              alibi_slopes_ptr, k_scale, v_scale,  \
+              tp_rank, blocksparse_local_blocks,   \
+              blocksparse_vert_stride, blocksparse_block_size, \
+              blocksparse_head_sliding_step); \
+    vllm::paged_attention_v2_reduce_kernel<cache_t, HEAD_SIZE, NUM_THREADS,     \
+                                         PARTITION_SIZE>                       \
+      <<<reduce_grid, block, reduce_shared_mem_size, stream>>>(                \
+          out_ptr, exp_sums_ptr, max_logits_ptr, tmp_out_ptr, seq_lens_ptr,    \
+          max_num_partitions); \
+  } \
+  else { \
+    VLLM_DevFuncAttribute_SET_MaxDynamicSharedMemorySize( \
+      ((void*)vllm::dattention_kernel<scalar_t, cache_t, KV_DTYPE, BLOCK_SIZE, HEAD_SIZE, NUM_THREADS, IS_BLOCK_SPARSE>), \
+      shared_mem_size);  \
+    vllm::dattention_kernel<scalar_t, cache_t, KV_DTYPE, BLOCK_SIZE, HEAD_SIZE, NUM_THREADS, IS_BLOCK_SPARSE> \
+        <<<grid, block, shared_mem_size, stream>>>( \
+              nullptr, nullptr, out_ptr,\
+              query_ptr, \
+              layer_offset, \
+              whole_block_size, max_seq_len, \
+              row_ptr, \
+              col_ptr, \
+              seq_lens_ptr, \
+              q_stride, num_kv_heads, scale,  \
+              alibi_slopes_ptr, k_scale, v_scale, \
+              tp_rank, blocksparse_local_blocks,   \
+              blocksparse_vert_stride, blocksparse_block_size, \
+              blocksparse_head_sliding_step); \
+        }
+          
+template <typename scalar_t, typename cache_t, vllm::Fp8KVCacheDataType KV_DTYPE, 
+          int BLOCK_SIZE, int NUM_THREADS = 128, int PARTITION_SIZE = 512>
+void dattention_launcher(
+  torch::Tensor& output,    // [num_seqs, num_heads, head_size]
+  torch::Tensor& exp_sums, 
+  torch::Tensor& max_logits,
+  torch::Tensor& tmp_out,
+  torch::Tensor& query,     // [num_seqs, num_heads, head_size]
+  bool use_reduce, 
+  int layer_idx,
+  int num_layers, 
+  int max_seq_len, 
+  torch::Tensor & seq_lens,
+  torch::Tensor & cache_row_mapping, 
+  torch::Tensor & cache_col_mapping,  
+  int num_kv_heads,
+  double  scale,
+  const c10::optional<torch::Tensor>&  alibi_slopes,
+  double k_scale, double v_scale, const int tp_rank, const int blocksparse_local_blocks,
+  const int blocksparse_vert_stride, const int blocksparse_block_size,
+  const int blocksparse_head_sliding_step) {
+  const bool is_block_sparse = (blocksparse_vert_stride > 1); 
+  const int num_seqs = query.size(0);
+  const int num_heads = query.size(1);
+  const int head_size = query.size(2);
+
+  int max_num_partitions = 1; 
+  if(use_reduce)
+	  max_num_partitions = DIVIDE_ROUND_UP(max_seq_len, PARTITION_SIZE);
+
+  const int key_block_size = (num_heads * head_size) * BLOCK_SIZE;
+  const int layer_block_size = key_block_size * 2;  
+  const int whole_block_size = layer_block_size * num_layers;  
+  const int layer_offset = layer_idx * key_block_size; 
+
+  // NOTE: alibi_slopes is optional.
+  const float* alibi_slopes_ptr =
+      alibi_slopes
+          ? reinterpret_cast<const float*>(alibi_slopes.value().data_ptr())
+          : nullptr;
+
+  scalar_t* out_ptr = reinterpret_cast<scalar_t*>(output.data_ptr());
+  float* exp_sums_ptr = nullptr;
+  float* max_logits_ptr = nullptr; 
+  scalar_t* tmp_out_ptr = nullptr;
+  if(use_reduce) {
+    exp_sums_ptr = reinterpret_cast<float*>(exp_sums.data_ptr());
+    max_logits_ptr = reinterpret_cast<float*>(max_logits.data_ptr());
+    tmp_out_ptr = reinterpret_cast<scalar_t*>(tmp_out.data_ptr());
+  }
+  
+  scalar_t* query_ptr = reinterpret_cast<scalar_t*>(query.data_ptr());
+  int* seq_lens_ptr = seq_lens.data_ptr<int>();
+  int64_t * row_ptr = reinterpret_cast<int64_t*>(cache_row_mapping.data_ptr());
+  int64_t * col_ptr = reinterpret_cast<int64_t*>(cache_col_mapping.data_ptr());
+
+  int q_stride = query.stride(0);
+  constexpr int NUM_WARPS = NUM_THREADS / WARP_SIZE;
+
+  int logits_size;
+  if(use_reduce) {
+    logits_size = PARTITION_SIZE * sizeof(float);
+  }
+  else {
+    int padded_max_seq_len =
+      DIVIDE_ROUND_UP(max_seq_len, BLOCK_SIZE) * BLOCK_SIZE;
+    logits_size = padded_max_seq_len * sizeof(float);
+  }
+
+  int outputs_size = (NUM_WARPS / 2) * head_size * sizeof(float);
+  int shared_mem_size = std::max(logits_size, outputs_size);
+
+  dim3 grid(num_heads, num_seqs, max_num_partitions);
+
+  // each thread block will be 128 threads
+  dim3 block(NUM_THREADS);
+  dim3 reduce_grid(num_heads, num_seqs);
+  int reduce_shared_mem_size = 2 * max_num_partitions * sizeof(float);
+
+  const at::cuda::OptionalCUDAGuard device_guard(device_of(query));
+  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
+
+  switch (head_size) {
+    // NOTE(woosuk): To reduce the compilation time, we only compile for the
+    // head sizes that we use in the model. However, we can easily extend this
+    // to support any head size which is a multiple of 16.
+    case 64:
+      if(is_block_sparse) {
+        LAUNCH_DATTENTION(64, true);
+      } else {
+        LAUNCH_DATTENTION(64, false); 
+      }
+      break;
+    case 80:
+      if(is_block_sparse) {
+        LAUNCH_DATTENTION(80, true);
+      } else {
+        LAUNCH_DATTENTION(80, false);
+      }
+      break;
+    case 96:
+      if(is_block_sparse) {
+        LAUNCH_DATTENTION(96, true);
+      } else {
+        LAUNCH_DATTENTION(96, false);
+      }
+      break;
+    case 112:
+      if(is_block_sparse) {
+        LAUNCH_DATTENTION(112, true);
+      } else {
+        LAUNCH_DATTENTION(112, false);
+      }
+      break;
+    case 128:
+      if(is_block_sparse) {
+        LAUNCH_DATTENTION(128, true);
+      } else {
+        LAUNCH_DATTENTION(128, false);
+      }
+      break;
+    case 192:
+      if(is_block_sparse) {
+        LAUNCH_DATTENTION(192, true);
+      } else {
+        LAUNCH_DATTENTION(192, false);
+      }
+      break;
+    case 256:
+      if(is_block_sparse) {
+        LAUNCH_DATTENTION(256, true);
+      } else {
+        LAUNCH_DATTENTION(256, false);
+      }
+      break;
+    default:
+      TORCH_CHECK(false, "Unsupported head size: ", head_size);
+      break;
+  }  
+}
+
+void dattention(
+    torch::Tensor& output,    // [num_seqs, num_heads, head_size]
+    torch::Tensor& exp_sums,    // [num_seqs, num_heads, max_num_partitions]
+    torch::Tensor& max_logits,  // [num_seqs, num_heads, max_num_partitions]
+    torch::Tensor& tmp_out,    // [num_seqs, num_heads, max_num_partitions, head_size]
+    torch::Tensor& query,     // [num_seqs, num_heads, head_size]
+    bool use_reduce, 
+    int64_t layer_idx,
+    int64_t num_layers, 
+    int64_t block_size,
+    int64_t max_seq_len, 
+    torch::Tensor & seq_lens,
+    torch::Tensor & cache_row_mapping, 
+    torch::Tensor & cache_col_mapping,  
+    const std::string& kv_cache_dtype,
+    int64_t num_kv_heads,
+    double scale,
+    const c10::optional<torch::Tensor>&  alibi_slopes,
+    double k_scale, double v_scale,
+    const int64_t tp_rank, const int64_t blocksparse_local_blocks,
+    const int64_t blocksparse_vert_stride, const int64_t blocksparse_block_size,
+    const int64_t blocksparse_head_sliding_step) { 
+  assert(block_size == 16 || block_size == 32);
+
+  if (kv_cache_dtype == "auto" && block_size == 16) {                                                 
+    if (query.dtype() == at::ScalarType::Float) {               
+      dattention_launcher<float, float, vllm::Fp8KVCacheDataType::kAuto, 16>( 
+          output, exp_sums, max_logits, tmp_out, query, use_reduce, layer_idx, num_layers, max_seq_len,  
+          seq_lens, cache_row_mapping, cache_col_mapping, 
+          num_kv_heads, scale, alibi_slopes, k_scale, v_scale,
+          tp_rank, blocksparse_local_blocks, blocksparse_vert_stride,
+          blocksparse_block_size, blocksparse_head_sliding_step);
+    } else if (query.dtype() == at::ScalarType::Half) {
+        dattention_launcher<uint16_t, uint16_t, vllm::Fp8KVCacheDataType::kAuto, 16>( 
+          output, exp_sums, max_logits, tmp_out, query, use_reduce, layer_idx, num_layers, max_seq_len,  
+          seq_lens, cache_row_mapping, cache_col_mapping, 
+          num_kv_heads, scale, alibi_slopes, k_scale, v_scale,
+          tp_rank, blocksparse_local_blocks, blocksparse_vert_stride,
+          blocksparse_block_size, blocksparse_head_sliding_step); 
+    } 
+  }
+  else if (kv_cache_dtype == "auto" && block_size == 32) {                                                 
+    if (query.dtype() == at::ScalarType::Float) {               
+      dattention_launcher<float, float, vllm::Fp8KVCacheDataType::kAuto, 32>( 
+          output, exp_sums, max_logits, tmp_out, query, use_reduce, layer_idx, num_layers, max_seq_len,  
+          seq_lens, cache_row_mapping, cache_col_mapping, 
+          num_kv_heads, scale, alibi_slopes, k_scale, v_scale,
+          tp_rank, blocksparse_local_blocks, blocksparse_vert_stride,
+          blocksparse_block_size, blocksparse_head_sliding_step);
+    } else if (query.dtype() == at::ScalarType::Half) {
+        dattention_launcher<uint16_t, uint16_t, vllm::Fp8KVCacheDataType::kAuto, 32>( 
+          output, exp_sums, max_logits, tmp_out, query, use_reduce, layer_idx, num_layers, max_seq_len,  
+          seq_lens, cache_row_mapping, cache_col_mapping, 
+          num_kv_heads, scale, alibi_slopes, k_scale, v_scale,
+          tp_rank, blocksparse_local_blocks, blocksparse_vert_stride,
+          blocksparse_block_size, blocksparse_head_sliding_step); 
+    } 
+  }
+  else {                     
+    printf("errors for dattention_launcher: dtype: %s, block_size %ld!!\n", kv_cache_dtype.c_str(), block_size);
+    exit(0);
+  }
+}
+
 #undef WARP_SIZE
 #undef MAX
 #undef MIN
diff -Nur orig/csrc/cache.h dattn/csrc/cache.h
--- orig/csrc/cache.h	2024-12-30 22:26:31.474992442 +0000
+++ dattn/csrc/cache.h	2024-12-30 22:27:03.164886968 +0000
@@ -6,7 +6,7 @@
 #include <vector>
 
 void swap_blocks(torch::Tensor& src, torch::Tensor& dst,
-                 const torch::Tensor& block_mapping);
+                  const torch::Tensor& block_mapping);
 
 // Note: the key_caches and value_caches vectors are constant but
 // not the Tensors they contain. The vectors need to be const refs
@@ -31,3 +31,14 @@
 // Just for unittest
 void convert_fp8(torch::Tensor& dst_cache, torch::Tensor& src_cache,
                  const double scale, const std::string& kv_cache_dtype);
+
+// new add for dAttention
+void reshape_and_cache_dattn(
+    torch::Tensor& key,    // [num_tokens, num_heads, head_size]
+    torch::Tensor& value,  // [num_tokens, num_heads, head_size]
+    int64_t layer_idx,     // which layer to reshape
+    int64_t num_layers,    // number of layers
+    int64_t block_size,    // size for each layer's cache block (including kv cache)
+    torch::Tensor& cache_row_mapping,  // [num_tokens]  record key/value write to which batch row in cache
+    torch::Tensor& cache_col_mapping,  // [num_tokens]  record key/value write to which token col in cache
+    const std::string& kv_cache_dtype);
\ No newline at end of file
diff -Nur orig/csrc/cache_kernels.cu dattn/csrc/cache_kernels.cu
--- orig/csrc/cache_kernels.cu	2024-12-30 22:26:31.473992447 +0000
+++ dattn/csrc/cache_kernels.cu	2024-12-30 22:27:03.164886968 +0000
@@ -21,6 +21,7 @@
 typedef __hip_bfloat16 __nv_bfloat16;
 #endif
 
+
 void swap_blocks(torch::Tensor& src, torch::Tensor& dst,
                  const torch::Tensor& block_mapping) {
   torch::Device src_device = src.device();
@@ -50,6 +51,7 @@
   const at::cuda::OptionalCUDAGuard device_guard(
       src_device.is_cuda() ? src_device : dst_device);
   const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
+  
   // NOTE(woosuk): This can be slow if the number of blocks is large.
   const int64_t num_blocks = block_mapping.size(0);
   for (size_t i = 0; i < num_blocks; i++) {
@@ -245,6 +247,92 @@
     }
   }
 }
+
+template <typename scalar_t, typename cache_t, Fp8KVCacheDataType kv_dt>
+// TODO: make block_size, kv_block_size, head_size, key_stride to be constant number (constexpr)
+// Then we can save some computation overhead during execution
+__global__ void reshape_and_cache_dattn_kernel(
+    const scalar_t* __restrict__ key,    // [num_tokens, num_heads, head_size]
+    const scalar_t* __restrict__ value,  // [num_tokens, num_heads, head_size]
+    int heads_per_thread_block,  // number of heads for each thread block 
+    int64_t block_size,          // number of tokens inside a block
+    int64_t kv_block_size,    // key or value block size in number of bytes for each layer
+    int64_t layer_offset,        // layer offset in the units
+    int64_t whole_block_size,    // whole block size (bytes), including KV of all layers together
+    const int64_t* cache_row_mapping,  // [num_tokens]  record cache ptr for this token
+    const int64_t* cache_col_mapping,  // [num_tokens]  record token index of the sequence
+    const int key_stride, const int value_stride,
+    const int head_size) {
+  // The index of the token
+  const int64_t index = blockIdx.x;
+  // The total number of heads for the model
+  const int64_t num_heads = gridDim.y * heads_per_thread_block; 
+  constexpr int x = 16 / sizeof(cache_t);
+ 
+ //   printf("[blockIdx.x %d, blockIdx.y %d, thread %d], cache_row_mapping %p\n", blockIdx.x, blockIdx.y, threadIdx.x, cache_row_mapping); 
+ //   printf("[blockIdx.x %d, blockIdx.y %d, thread %d], cache_col_mapping %p\n", blockIdx.x, blockIdx.y, threadIdx.x, cache_col_mapping); 
+
+  // NOTE: cache_row_idx or cache_col_idx can be -1 if the token is padded
+  const int64_t cache_address = cache_row_mapping[index];
+  // token index in the sequence, which determins the position of kv cache 
+  const int64_t token_idx = cache_col_mapping[index];
+  if (cache_address <= 0 || token_idx < 0) {
+    return;
+  }
+
+  // Note: each thread block is in charge of heads_per_thread_block of the same token
+  // Therefore, each warp is in charge of 1 head of the same token 
+  int64_t block_idx = token_idx / block_size;
+
+  // token index inside the current block: [0, 16)
+  int64_t block_offset = token_idx % block_size;    
+  
+  // Get the block index for the current thread block
+  const int64_t thread_block_idx = blockIdx.y;
+  const int64_t warp_idx = threadIdx.x / WARP_SIZE; //[0,3]
+
+  // The workload for each warp (assuming there are 4 warps in one thread block)
+  const int warps_per_thread_block = blockDim.x/WARP_SIZE;
+  const int64_t heads_per_warp = heads_per_thread_block/warps_per_thread_block; 
+  
+  // starting head_idx for this warp 
+  int64_t head_idx = thread_block_idx * heads_per_thread_block + warp_idx * heads_per_warp;
+
+  // kv_block_size == head_size * block_size (without considering the type)
+  // Compute the start address of the head of the block for KV cache
+  int64_t head_start = block_idx * whole_block_size + layer_offset + head_idx * kv_block_size; 
+  
+  int64_t thread_idx_in_warp = threadIdx.x % WARP_SIZE;
+
+  scalar_t* dest_key = reinterpret_cast<scalar_t*>(cache_address) + head_start;  
+
+  // whole_block_size: 2 * (num_heads * head_size * block_size * layers) 
+  scalar_t* dest_value = dest_key + whole_block_size/2; 
+                                              
+  // Each thread block will copy one token's 4 heads, while each warp will copy one token's one head only
+  // since key: [num_tokens, num_heads, head_size]
+  //int64_t src_offset = index * num_heads * head_size  + head_idx * head_size; 
+  int64_t src_offset = index * key_stride + head_idx * head_size; 
+  scalar_t* src_key = const_cast<scalar_t*>(key + src_offset);
+  scalar_t* src_value = const_cast<scalar_t*>(value + src_offset);
+
+  // Each warp will handle only one token's one head 
+  for (int i = thread_idx_in_warp; i < head_size*heads_per_warp; i += WARP_SIZE) {
+    // i == head_offset 
+    // We are going to transfer [0,head_size) to [head_size/x, block_size, x]
+    int x_idx = i / x;
+    int x_offset = i % x;
+
+    // [num_blocks, num_heads, head_size/x, block_size, x]
+    int64_t tgt_key_idx = x_idx * block_size * x + block_offset * x + x_offset;     
+    int64_t tgt_value_idx = i * block_size + block_offset; 
+    dest_key[tgt_key_idx] = src_key[i];
+
+    // [num_blocks, num_heads, head_size, block_size]
+    dest_value[tgt_value_idx] = src_value[i]; 
+  }
+}
+
 }  // namespace vllm
 
 // KV_T is the stored data type of kv-cache.
@@ -329,6 +417,65 @@
                              CALL_RESHAPE_AND_CACHE_FLASH);
 }
 
+#define CALL_RESHAPE_AND_CACHE_DATTN(KV_T, CACHE_T, KV_DTYPE)         \
+  vllm::reshape_and_cache_dattn_kernel<KV_T, CACHE_T, KV_DTYPE>             \
+      <<<grid, block, 0, stream>>>(                                   \
+          reinterpret_cast<KV_T*>(key.data_ptr()),                    \
+          reinterpret_cast<KV_T*>(value.data_ptr()),                  \
+          heads_per_thread_block, block_size,                         \
+          kv_block_size, layer_offset,                                \
+          whole_block_size,                                           \
+          cache_row_mapping.data_ptr<int64_t>(),                      \
+          cache_col_mapping.data_ptr<int64_t>(),                      \
+          key_stride, value_stride,                                   \
+          head_size);
+          
+void reshape_and_cache_dattn(
+    torch::Tensor& key,    // [num_tokens, num_heads, head_size]
+    torch::Tensor& value,  // [num_tokens, num_heads, head_size]
+    int64_t layer_idx,  // which layer to reshape
+    int64_t num_layers, // number of layers
+    int64_t block_size, // the number of tokens inside a block
+    torch::Tensor& cache_row_mapping,  // [num_tokens]  record key/value write to which batch row in cache
+    torch::Tensor& cache_col_mapping,  // [num_tokens]  record key/value write to which token col in cache
+    const std::string& kv_cache_dtype) {
+
+  if (kv_cache_dtype != "auto") {
+    TORCH_CHECK(false, "Unsupported data type of kv cache: ", kv_cache_dtype);
+  }
+  const int num_tokens = key.size(0);
+  const int num_heads = key.size(1);
+  const int head_size = key.size(2);
+
+  const int key_stride = key.stride(0);
+  const int value_stride = value.stride(0);
+
+  //fprintf(stderr, "hihihi, layer_idx %d, num_tokens %d, num_heads %d, head_size %d, key_stride %d,  value_stride %d\n", layer_idx, num_tokens, num_heads, head_size, key_stride, value_stride);
+  // We will dynamically decide heads_per_thread_block
+  constexpr int heads_per_thread_block = 4;
+  assert(num_heads % heads_per_thread_block == 0);
+
+  const int sm_for_heads = num_heads/heads_per_thread_block; 
+
+  const int64_t kv_block_size = head_size * block_size;
+  const int64_t whole_block_size = kv_block_size * num_heads * num_layers * 2;  
+  const int64_t layer_offset = layer_idx * kv_block_size * num_heads; 
+
+  //fprintf(stderr, "newwww block_size %d, key_block_size-%d, whole_block_size-%ld, num_layers-%d, sm_for_heads %d\n", block_size, kv_block_size, whole_block_size, num_layers, sm_for_heads); 
+  dim3 grid(num_tokens, sm_for_heads);
+
+  // each thread block will be 128 threads
+  dim3 block(128);
+
+  const at::cuda::OptionalCUDAGuard device_guard(device_of(key));
+  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
+
+  DISPATCH_BY_KV_CACHE_DTYPE(key.dtype(), kv_cache_dtype,
+                             CALL_RESHAPE_AND_CACHE_DATTN)
+  
+  //cudaDeviceSynchronize();  
+}
+
 namespace vllm {
 
 template <typename Tout, typename Tin, Fp8KVCacheDataType kv_dt>
diff -Nur orig/csrc/dattn/dattn.cu dattn/csrc/dattn/dattn.cu
--- orig/csrc/dattn/dattn.cu	1970-01-01 00:00:00.000000000 +0000
+++ dattn/csrc/dattn/dattn.cu	2024-12-30 22:27:03.170886939 +0000
@@ -0,0 +1,703 @@
+/*
+ Copyright (c) ByteDance Inc.
+ Authors: 
+  - Tongping Liu (tongping.liu@bytedance.com)
+ */
+#include <torch/all.h>
+#include <ATen/cuda/CUDAContext.h>
+#include <c10/cuda/CUDAGuard.h> 
+#include <torch/torch.h>
+#include <c10/core/ScalarType.h>
+#include <cstdint>
+#include <cstdio>
+#include <string>
+#include <cuda_runtime.h>
+#include <ATen/cuda/CUDAContext.h>
+#include <Python.h>
+#include <pthread.h>
+
+#include "dattn.h"
+
+
+#define KV_UTILIZATION_RATE (0.9)
+
+constexpr int64_t MEGABYTES=1048576;
+constexpr int64_t PAGE_SIZE=(MEGABYTES * 2); 
+constexpr int64_t GIGABYTES=(MEGABYTES * 1024);
+
+/* 
+  In this allocator, we only have the following concepts, but without the concept of tokens.
+  The python portion should convert the number of tokens to tokens depending on their cache_block_size (e.g., 16)
+  Region: virtual address space for a request. Currently, we support the space for max_seq_len.
+ */
+static uint64_t roundup(uint64_t size, uint64_t align_size) {
+  return ((size + align_size - 1)/align_size) * align_size; 
+}
+
+using PhysicalBlock = struct {
+    CUmemGenericAllocationHandle handle;
+};
+
+class PhysicalBlocksManager {
+public:
+  // Available blocks will be placed in this pool 
+  std::vector<PhysicalBlock> block_pool; 
+  // All in-use blocks will be placed in the map. 
+  std::unordered_map<void *, PhysicalBlock> block_map;
+  int64_t block_size;
+  int64_t free_blocks; 
+  int64_t total_size;
+  int64_t max_allowed_size; // maximum allowed size for KV cache
+  int64_t page_size;  
+  int64_t incremental_size; 
+  int64_t tofree_blocks_watermark; 
+  int64_t num_tofree_blocks; 
+  CUmemAllocationProp prop;
+
+  PhysicalBlocksManager(); 
+  ~PhysicalBlocksManager();
+  void initialize(size_t max_allowed_size, size_t total_memory, size_t block_size);
+
+  PhysicalBlock allocate();
+  void record(void * virtual_address, PhysicalBlock block);
+
+  void free(void * virtual_address);
+
+  void cleanup();
+
+private:
+  void _free_blocks_from_pool(int64_t num_blocks);
+  void _increase_blocks(int64_t num_blocks);
+
+}; 
+
+
+
+static PhysicalBlocksManager _block_manager;
+
+/*
+ * In this file, there are three concepts:
+    page_size: the actual page size of the underlying hardware, typically 2MB for cuda GPU
+    cache_block_size: the original size of each block (16 tokens). However, this size (e.g., 5M) may not be aligned well with pages. 
+    physical_block_size: the actual size used in managing of pysical blocks, which is consisted of multiple of cache_block_size. 
+                      In particular, this size is algined with page_size.  
+ */
+PhysicalBlocksManager::PhysicalBlocksManager() {
+  this->prop = {};
+  this->prop.type = CU_MEM_ALLOCATION_TYPE_PINNED;
+  this->prop.location.type = CU_MEM_LOCATION_TYPE_DEVICE;
+  this->prop.location.id = 0;
+
+  // Each time, the size of physical blocks is to be increased whenever no objects is available
+  this->incremental_size = 2 * GIGABYTES;
+  this->total_size = 0;
+  this->block_size = 0; 
+  this->free_blocks = 0;  
+}
+
+PhysicalBlocksManager::~PhysicalBlocksManager() {
+}
+
+void PhysicalBlocksManager::_increase_blocks(int64_t num_blocks) {
+    CUresult result;
+    //fprintf(stderr, "_increase_blocks num_blocks-%d\n", num_blocks); 
+    for (size_t i = 0; i < num_blocks; i++) {
+        CUmemGenericAllocationHandle handle;
+        result = cuMemCreate(&handle, this->block_size, &this->prop, 0);
+        if (result != CUDA_SUCCESS) {
+          fprintf(stderr, "Failed to create memory allocation, i-%d, with result %ld\n", i, result);
+          exit(-1);        
+        }
+
+        block_pool.emplace_back(PhysicalBlock{handle});
+    }
+    // Update the number of blocks
+    this->free_blocks += num_blocks; 
+}
+
+void PhysicalBlocksManager::initialize(size_t max_allowed_size, size_t total_memory, size_t block_size) {
+    CUresult result;
+    size_t page_size; 
+
+    // Getting the granularity of page isze. 
+    result = cuMemGetAllocationGranularity(&page_size, &this->prop, CU_MEM_ALLOC_GRANULARITY_MINIMUM);
+    if (result != CUDA_SUCCESS) {
+        throw std::runtime_error("Failed to get page size");
+    }
+
+    assert(page_size == PAGE_SIZE); 
+    assert(total_memory % GIGABYTES == 0);
+
+    // We assume that cache_block_size is multiple of megabytes here.  
+
+    // Allocate the initial blocks based on user's specification
+    this->block_size = block_size; 
+
+    // Since there are some issues when block_size is not aligned with page_size, 
+    // let's have some buffer here. 
+    this->max_allowed_size = max_allowed_size + block_size * 40; 
+    this->tofree_blocks_watermark = (this->incremental_size * 2)/block_size; 
+    this->num_tofree_blocks = this->incremental_size/block_size;  
+
+    // Allocate the physical memory with specified size 
+    int64_t to_allocate_memory = min(total_memory, max_allowed_size); 
+    this->total_size = to_allocate_memory; 
+    size_t num_blocks = to_allocate_memory / block_size;
+    fprintf(stderr, "total_memory %lx, max_allowed_size %lx num_blocks-%ld\n", total_memory, max_allowed_size, num_blocks);
+    _increase_blocks(num_blocks);
+}
+
+PhysicalBlock PhysicalBlocksManager::allocate(void) {
+    if(this->free_blocks == 0) {
+      assert(this->block_pool.size() == 0); 
+
+      // Keeping increase the memory if the GPU memory is sufficient
+      int64_t allow_size = this->max_allowed_size - this->total_size;
+      int64_t alloc_size; 
+      //fprintf(stderr, "alloc_size %lx allow_size %lx total_size %lx\n", alloc_size, allow_size, this->total_size);
+
+      if(allow_size <= 0) {
+        fprintf(stderr, "There is no sufficent GPU memory now. ");
+        exit(0);         
+      }
+
+      if (allow_size > this->incremental_size) {
+        alloc_size = this->incremental_size; 
+      }
+      else {
+        // Less than the incremental_size. 
+        alloc_size = roundup(allow_size, this->block_size);  
+      }
+
+      int64_t blocks = alloc_size/this->block_size; 
+
+      _increase_blocks(blocks);
+      this->total_size += alloc_size; 
+    }
+
+    assert(this->free_blocks > 0); 
+    assert(this->block_pool.size() > 0); 
+
+    PhysicalBlock block = block_pool.back(); 
+    block_pool.pop_back();     
+    this->free_blocks--; 
+    return block; 
+}
+
+void PhysicalBlocksManager::free(void * virtual_address) {
+  PhysicalBlock block;
+  bool is_exist = false; 
+
+  if(block_map.count(virtual_address)) {
+    block = block_map[virtual_address];
+
+    block_map.erase(virtual_address); 
+    is_exist = true; 
+  }
+  
+  if (!is_exist) {
+    fprintf(stderr, "Wrong: virtual_address-%p does not exist\n", virtual_address);
+    exit(-1); 
+  }
+
+  // Adding this block to the block_pool
+  block_pool.push_back(block); 
+  this->free_blocks += 1; 
+
+  if(block_pool.size() > this->tofree_blocks_watermark) {
+    _free_blocks_from_pool(this->num_tofree_blocks);
+  }
+}
+
+void PhysicalBlocksManager::_free_blocks_from_pool(int64_t num_blocks) {
+    for(int i = 0; i < num_blocks; i++) {
+      PhysicalBlock block = block_pool.back(); 
+      CUresult status = CUDA_SUCCESS;
+      if((status = cuMemRelease(block.handle)) != CUDA_SUCCESS) {
+        fprintf(stderr, "cuMemRelease failed, err code: %d\n", status);
+      } 
+      block_pool.pop_back(); 
+    }
+
+    this->free_blocks -= num_blocks; 
+}
+
+void PhysicalBlocksManager::cleanup() {
+    for (auto& block : block_pool) {
+      CUresult status = CUDA_SUCCESS;
+      if((status = cuMemRelease(block.handle)) != CUDA_SUCCESS) {
+        fprintf(stderr, "cuMemRelease failed, err code: %d\n", status);
+      } 
+    }
+    block_pool.clear();
+}
+
+void PhysicalBlocksManager::record(void * virtual_address, PhysicalBlock block) {
+  block_map[virtual_address] = block; 
+}
+
+static CUmemAccessDesc _accessDescr = {};
+ 
+
+/*
+** kvCacheRegion functions implementation
+*/
+kvCacheRegion::kvCacheRegion(int64_t cache_block_size, int64_t physical_block_size, CUdeviceptr ptr) {
+  this->cache_block_size = cache_block_size;
+  this->physical_block_size = physical_block_size;
+  this->dptr = reinterpret_cast<char*>(ptr);  
+  this->nextUnmappedAddr = reinterpret_cast<char*>(ptr); 
+  this->mapped_size = 0;
+}
+
+// Decontructor: release all physical pages of this region
+kvCacheRegion::~kvCacheRegion() {
+  freeAllPhyMemory(); 
+  // Note that since the region is detroyed, 
+  // no need to clear other counters. 
+}
+
+CUdeviceptr kvCacheRegion::getStartPtr(void) {
+  return reinterpret_cast<CUdeviceptr>(this->dptr); 
+} 
+
+/*
+  kvCacheRegion function: allocate cached blocks  
+    if the return value > 0, then it is succesful. 
+ */ 
+void kvCacheRegion::updateBlocks(uint64_t blocks) {
+  uint64_t newSize = blocks * this->cache_block_size;
+  newSize = roundup(newSize, this->physical_block_size); 
+
+  int64_t distance; 
+
+  // No need to allocate if size is not changed
+  if(newSize == this->mapped_size) {
+    return; 
+  }
+  else if (newSize < this->mapped_size) {
+    // Shrink the memory for this region
+    distance = this->mapped_size - newSize; 
+    int64_t blocks_num = distance/this->physical_block_size; 
+
+    char * addr = this->dptr + newSize; 
+    this->nextUnmappedAddr = addr; 
+
+    // Unmap unnecessary memory
+    CUresult res; 
+    res = cuMemUnmap(reinterpret_cast<CUdeviceptr>(addr), distance); 
+    if(res != CUDA_SUCCESS) {
+      const char* errorStr;
+      cuGetErrorString(res, &errorStr);
+      fprintf(stderr, "cuMemUnmap failed when deallocating ptr %p and size %lx with error %s\n", addr, distance, errorStr);
+      exit(-1);
+    }       
+
+    //fprintf(stderr, "reduceBlocks, newSize: %lx, addr: %p, distance-%lx, blocks %ld, this->mapped_size: %lx \n", newSize, addr, distance, blocks_num, this->mapped_size);
+    for(int i = 0; i < blocks_num; i++) {
+      // Free the actual physical memory by putting it back to the pool
+      _block_manager.free(addr); 
+
+      addr += this->physical_block_size; 
+    }
+  }
+  else {
+    // Increase the memory for this region
+    distance = newSize - this->mapped_size; 
+    int64_t blocks_num = distance/this->physical_block_size; 
+
+    char * addr = this->nextUnmappedAddr;
+
+    //cudaDeviceSynchronize();
+
+    // Map new memory
+    CUresult res; 
+    int64_t size = this->physical_block_size;  
+    //fprintf(stderr, "increaseBlocks newSize: %lx, addr: %p, distance-%lx, blocks %ld, this->mapped_size: %lx\n", newSize, addr, distance, blocks_num, this->mapped_size);
+    for(int i = 0; i < blocks_num; i++) {
+      // Allocate a physical block 
+      PhysicalBlock block = _block_manager.allocate();
+      if ((res = cuMemMap(reinterpret_cast<CUdeviceptr>(addr), size, 0ULL, block.handle, 0ULL)) == CUDA_SUCCESS) {
+        if ((res = cuMemSetAccess(reinterpret_cast<CUdeviceptr>(addr), size, &_accessDescr, 1)) != CUDA_SUCCESS) {
+          fprintf(stderr, "cuMemMap success,but cuMemSetAccess failed!, err code: %d\n", res);
+          cuMemUnmap(reinterpret_cast<CUdeviceptr>(addr), size);
+          exit(-1);
+        }
+      }
+      else {
+        const char* errorStr;
+        cuGetErrorString(res, &errorStr);
+        fprintf(stderr, "cuMemMap failed when deallocating ptr %p res %d with error %s\n", addr, res, errorStr);
+      }
+
+      _block_manager.record(addr, block); 
+
+      // Update addr to the next block
+      addr += this->physical_block_size; 
+    }
+    this->nextUnmappedAddr = addr; 
+  }
+
+  this->mapped_size = newSize; 
+}
+
+void kvCacheRegion::freeAllPhyMemory(void) {
+  //fprintf(stderr, "freeAllPhyMemory dtpr %p mapped_size %lx\n", this->dptr, this->mapped_size);
+  assert (this->mapped_size > 0);
+
+  char * addr = this->dptr;
+  CUresult res = cuMemUnmap(reinterpret_cast<CUdeviceptr>(addr), this->mapped_size); 
+  if(res != CUDA_SUCCESS) {
+    const char* errorStr;
+    cuGetErrorString(res, &errorStr);
+    fprintf(stderr, "cuMemUnmap failed when deallocating ptr %p and size %lx with error %s\n", reinterpret_cast<CUdeviceptr>(addr), this->mapped_size, errorStr);
+    exit(-1);
+  }       
+
+  int64_t blocks_num = this->mapped_size/this->physical_block_size; 
+  for(int i = 0; i < blocks_num; i++) {
+    // Free the actual physical memory by putting it back to the pool
+    _block_manager.free(addr); 
+
+    addr += this->physical_block_size; 
+  }
+
+  // Note that we don't actually release virtual memory (cuMemAddressFree)
+  this->nextUnmappedAddr = this->dptr; 
+  this->mapped_size = 0; 
+}
+
+/*
+** kvCacheAllocator functions implementation
+* TODO: we may need to remove some details from the allocator, such as max_seq_length, layers_num. 
+*       But instead, we should add the initial allocation size, or we can use number of blocks (allocated size, so that )
+*/
+kvCacheAllocator::kvCacheAllocator(int64_t max_gpu_memory_size, int64_t cache_block_size, int64_t region_cache_size) {
+  CUdevice device;
+  
+  CHECK_RT(cudaFree(0));
+
+  CHECK_DRV(cuCtxGetCurrent(&this->origContext));
+  if(this->origContext == nullptr) {
+    fprintf(stderr, "Context is nullptr\n"); 
+    exit(-1);
+  }
+
+  size_t free_memory, total_memory;
+  CHECK_RT(cudaMemGetInfo(&free_memory, &total_memory)); 
+
+  CUresult result;
+  size_t page_size; 
+
+  CUmemAllocationProp prop = {};
+  prop.type = CU_MEM_ALLOCATION_TYPE_PINNED;
+  prop.location.type = CU_MEM_LOCATION_TYPE_DEVICE;
+  prop.location.id = 0;
+
+  _accessDescr.flags = CU_MEM_ACCESS_FLAGS_PROT_READWRITE;
+  _accessDescr.location = prop.location;
+
+  // Getting the granularity of page isze. 
+  result = cuMemGetAllocationGranularity(&page_size, &prop, CU_MEM_ALLOC_GRANULARITY_MINIMUM);
+  if (result != CUDA_SUCCESS) {
+    fprintf(stderr, "Failed to get page size!\n");
+    exit(-1);
+  }
+
+  assert(page_size == PAGE_SIZE); 
+
+  int64_t to_allocate_memory = 2 * GIGABYTES; 
+  if(free_memory < to_allocate_memory) {
+    fprintf(stderr, "Insufficient gpu memory\n");
+    exit(-1);
+  }
+
+  int64_t physical_block_size = cache_block_size; 
+
+  while(physical_block_size%page_size != 0) {
+    physical_block_size *= 2; 
+
+    // Adding an explicit checking. 
+    if(physical_block_size > 40*MEGABYTES) {
+      fprintf(stderr, "Invalid physical_block_size %lx, with cache_block_size-%lx!!", physical_block_size, cache_block_size);
+      exit(-1);
+    }
+  }
+  this->physical_block_size = physical_block_size; 
+
+  //fprintf(stderr, "cache_block_size-%lx, this->physical_block_size-%lx\n", cache_block_size, physical_block_size);
+  // Initialize block manager
+  // max_allowed_size should be related to num_blocks, initialized GPU memory, cache_block_size
+  _block_manager.initialize(max_gpu_memory_size, to_allocate_memory, physical_block_size);
+
+  this->page_size = PAGE_SIZE;
+  this->region_size = region_cache_size;
+  this->cache_block_size = cache_block_size;
+
+  this->manager_running = false;
+
+  // Initialize of mutex lock and condition
+  pthread_mutex_init(&mutex_manager, NULL); 
+  pthread_cond_init(&cond_manager, NULL); 
+  manager_running = false; 
+
+  pthread_attr_t attr; 
+  pthread_attr_init(&attr);
+  // Set the thread to be detached
+  pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_DETACHED);
+
+  if(pthread_create(&this->thread_id, &attr, kvCacheAllocator::memoryManagerThread, this) != 0) {
+    fprintf(stderr, "thread creation failed!"); 
+    exit(-1); 
+  }
+}
+
+int64_t kvCacheAllocator::getPageSize() {
+  return this->page_size;
+}
+
+// reserve function, reserve virtual address space for a request
+int64_t kvCacheAllocator::reserveRegion(int64_t region_id) {
+  CUdeviceptr ptr;
+  kvCacheRegion * region = nullptr;
+
+  // The expensive way to get a new region. Only invoked when no cached regions
+  // Allocate the virtual address for this region
+  CHECK_DRV(cuMemAddressReserve(&ptr, this->region_size, 0ULL, 0ULL, 0ULL));
+
+  // Create a new region from the scratch
+  region = new kvCacheRegion(this->cache_block_size, this->physical_block_size, ptr);
+
+  // Allocate one block the first region
+  if(region_id == 0) {
+    region->updateBlocks(1); 
+  }
+
+  // Record the region information
+  this->active_regions_map[region_id] = region; 
+
+  return static_cast<int64_t>(ptr);
+}
+
+
+// FIXME: ideally there is no need to allocate num_caches with the same 
+// size, but the management of CPU caches can be more complicated. 
+// Fixing this problem in the future if the CPU memory size is a problem 
+std::vector<int64_t> kvCacheAllocator::allocCPUCaches(int64_t num_caches, int64_t cache_size) {
+  std::vector<int64_t> cache_addresses; 
+  
+  for(int i = 0; i < num_caches; i++) {
+    void * address;  
+    cudaError_t err = cudaHostAlloc(&address, cache_size, cudaHostAllocDefault);
+    if (err != cudaSuccess) {
+        std::cerr << "cudaHostAlloc failed: " << cudaGetErrorString(err) << std::endl;
+        exit(-1);
+    }
+    cache_addresses.push_back((int64_t)address); 
+  }
+
+  return cache_addresses; 
+}
+
+// Release the region with the given region_id
+void kvCacheAllocator::_releaseRegion(int64_t region_id) {
+  // Find the region corresponding to the given region_id
+  if(this->active_regions_map.count(region_id) == 0) {
+    fprintf(stderr, "ERROR in release: region_id-%ld does not exist at all.!\n", region_id);
+    exit(-1); 
+  }
+
+  //std::lock_guard<std::mutex> lock(this->mutex);
+  kvCacheRegion * region = this->active_regions_map[region_id];
+
+  //fprintf(stderr, "before release region %ld, blocks %d\n", region_id, _block_manager.block_pool.size()); 
+  // Note that as we don't actually release physical cache blocks. 
+  // Therefore, we don't need to change the active_blocks here. 
+  region->freeAllPhyMemory();
+}
+
+
+// Allocate cache blocks for a range of requests. Each request information will be an vector, with
+// the request id as the first, and then number of blocks as the second. 
+void kvCacheAllocator::updateBlocks(std::vector<std::vector<int64_t>> update_blocks) {
+  for(auto row : update_blocks) {
+    uint64_t region_id = row[0]; 
+    uint64_t blocks = row[1]; 
+
+    assert(this->active_regions_map.count(region_id) > 0);
+    kvCacheRegion * region = this->active_regions_map[region_id];
+    region->updateBlocks(blocks);
+    //fprintf(stderr, "after region-%ld allocates %ld blocks. free_blocks-%ld\n", region_id, blocks, _block_manager.block_pool.size()); 
+  }
+
+  //fprintf(stderr, "NNNNNN after updateBlocks, handling %ld request\n", update_blocks.size());
+  return; 
+}
+
+// This is a separate thread that performing both synchronous and asynchronous 
+// memory management operations. 
+void * kvCacheAllocator::memoryManagerThread(void * arg) {
+  kvCacheAllocator * instance = static_cast<kvCacheAllocator *>(arg); 
+  
+  // It is optional to set current context to be the same as origContext 
+  CUresult result = cuCtxSetCurrent(instance->origContext);
+  if (result != CUDA_SUCCESS) {
+    const char* error_string;
+    cuGetErrorString(result, &error_string);
+    std::cerr << "CUDA error: " << error_string << std::endl;
+    exit(-1);
+  } 
+  
+  //at::cuda::OptionalCUDAGuard device_guard(device); 
+  cudaStream_t asyncStream; cudaStreamCreate(&asyncStream);
+  //cudaStream_t computeStream = at::cuda::getCurrentCUDAStream();
+
+  while(true) {
+    pthread_mutex_lock(&instance->mutex_manager); 
+
+    // We will wait if manager_running is true (didn't finish last memory management operations)
+    // or there is no need to perform memory management
+    while(!instance->manager_running) {
+      pthread_cond_wait(&instance->cond_manager, &instance->mutex_manager); 
+    }
+   
+    // Perform memory management asynchronously
+    instance->swapOutCache(instance->swap_out_caches, asyncStream);
+    instance->updateBlocks(instance->update_blocks);
+    // Swap in cache must be done after allocating cache blocks, as 
+    // we may reuse an existing cache but with the expansion of its blocks 
+    instance->swapInCache(instance->swap_in_caches, asyncStream);
+
+    //pthread_mutex_lock(&instance->mutex_manager); 
+    instance->manager_running = false; 
+    pthread_cond_signal(&instance->cond_manager);
+    pthread_mutex_unlock(&instance->mutex_manager); 
+  }
+
+  return NULL;
+}
+
+void kvCacheAllocator::updateCacheBlocks(bool immediate_allocate, 
+                                         std::vector<std::vector<int64_t>> to_update_blocks,
+                                         std::vector<std::vector<int64_t>> to_swap_out,
+                                         std::vector<std::vector<int64_t>> to_swap_in) {
+    //fprintf(stderr, "NNNNNNNNN in handling the request updateCacheBlocks!!!!!, immediate_allocate-%d\n", immediate_allocate);
+    pthread_mutex_lock(&this->mutex_manager);
+    
+    // If the manager has not finished, waiting on the condition 
+    while(this->manager_running) {
+      fprintf(stderr, "waiting for the virtual memory management in asyn mode\n"); 
+      pthread_cond_wait(&this->cond_manager, &this->mutex_manager); 
+    }
+
+    this->update_blocks.clear();
+    this->swap_out_caches.clear();  
+    this->swap_in_caches.clear();  
+
+    for(auto cacheInfo: to_swap_out) {
+      this->swap_out_caches.push_back(cacheInfo); 
+    }
+
+    for(auto cache_block: to_update_blocks) {
+      this->update_blocks.push_back(cache_block); 
+    }
+
+    for(auto cacheInfo: to_swap_in) {
+      this->swap_in_caches.push_back(cacheInfo); 
+    }    
+    
+    this->manager_running = true; 
+    this->immediate_allocate = immediate_allocate; 
+    
+    // Wake up the manager thread to perform virtual memory management
+    pthread_cond_signal(&this->cond_manager); 
+
+    if(immediate_allocate) {
+      // We will wait until the manager thread finishes its job
+      while(this->manager_running) {
+        //fprintf(stderr, "immediate waiting for the virtual memory management in asyn mode\n"); 
+        pthread_cond_wait(&this->cond_manager, &this->mutex_manager); 
+      }
+    }
+
+    pthread_mutex_unlock(&this->mutex_manager); 
+}
+
+// Release regions specified in the vector
+void kvCacheAllocator::releaseRegions(std::vector<int64_t> regions) {
+  for(auto region : regions) {
+    //fprintf(stderr, "release region-%d\n", region); 
+    _releaseRegion(region);
+  }
+}
+
+// Swap out the caches listed in src_to_dests (from Device to Host)
+void kvCacheAllocator::swapOutCache(std::vector<std::vector<int64_t>> swap_caches, cudaStream_t stream) {
+  bool to_sync = false; 
+
+  // Checking every item in swap_caches
+  for(auto item: swap_caches) {
+    int64_t region_id = item[0]; 
+    int64_t dest_ptr = item[1]; 
+    int64_t size = item[2]; 
+
+    assert(this->active_regions_map.count(region_id) != 0);
+    to_sync = true; 
+
+    kvCacheRegion * region = this->active_regions_map[region_id];
+    CUdeviceptr src_ptr = region->getStartPtr(); 
+
+    cuMemcpyDtoHAsync(reinterpret_cast<void*>(dest_ptr), src_ptr, size, stream); 
+  }
+
+  if(to_sync) {
+    // We need to synchronize here, since partial pages can be munmaped (which can cause 
+    // issue if without synchronization)
+    cudaError_t err = cudaStreamSynchronize(stream);
+    if (err != cudaSuccess) {
+      std::cerr << "Stream synchronization failed: " << cudaGetErrorString(err) << std::endl;
+      err = cudaGetLastError();
+      if (err != cudaSuccess) {
+        std::cerr << "CUDA error after synchronization: " << cudaGetErrorString(err) << std::endl;
+      } 
+      exit(-1);
+    } 
+  }   
+}
+
+// Swap in the caches listed in swap_caches (from Host to Device)
+void kvCacheAllocator::swapInCache(std::vector<std::vector<int64_t>> swap_caches, cudaStream_t stream) {
+  bool to_sync = false; 
+ 
+  for(auto item: swap_caches) {
+    int64_t src_ptr = item[0]; 
+    int64_t region_id = item[1]; 
+    int64_t blocks = item[2]; 
+
+    to_sync = true; 
+
+    // Allocate physical memory at first
+    kvCacheRegion * region = this->active_regions_map[region_id];
+
+    int64_t size = blocks  * this->cache_block_size; 
+
+    //fprintf(stderr, "SWPAIN allocation regionid-%ld, blocks %ld, size: %lx\n", region_id, blocks, size);
+    
+    CUdeviceptr dest_ptr = region->getStartPtr(); 
+
+    cuMemcpyHtoDAsync(dest_ptr, reinterpret_cast<const void*>(src_ptr), size, stream);
+  }
+
+  if(to_sync) {
+    cudaError_t err = cudaStreamSynchronize(stream);
+    if (err != cudaSuccess) {
+      std::cerr << "Stream synchronization failed: " << cudaGetErrorString(err) << std::endl;
+      err = cudaGetLastError();
+      if (err != cudaSuccess) {
+        std::cerr << "CUDA error after synchronization: " << cudaGetErrorString(err) << std::endl;
+      } 
+      exit(-1);
+    }
+  }
+  //fprintf(stderr, "After SWPAIN, free blocks %ld\n", _block_manager.block_pool.size());
+}
\ No newline at end of file
diff -Nur orig/csrc/dattn/dattn.h dattn/csrc/dattn/dattn.h
--- orig/csrc/dattn/dattn.h	1970-01-01 00:00:00.000000000 +0000
+++ dattn/csrc/dattn/dattn.h	2024-12-30 22:27:03.170886939 +0000
@@ -0,0 +1,145 @@
+#pragma once
+/*
+ Copyright (c) ByteDance Inc.
+ Authors: 
+  - Tongping Liu (tongping.liu@bytedance.com)
+  - https://github.com/vllm-project/vllm/pull/6102/commits
+ */ 
+//#include <torch/script.h>
+#include <iostream>
+#include <cuda.h>
+#include <cuda_runtime_api.h>
+#include <cstdint>
+#include <cstdio>
+#include <cassert>
+#include <cstddef>
+#include <deque>
+#include <unordered_map>
+#include <torch/custom_class.h>
+#include <c10/util/intrusive_ptr.h>
+#include <pthread.h>
+
+
+#define _MB (1 << 20)
+
+using namespace std;
+
+static inline void
+checkRtError(cudaError_t res, const char *tok, const char *file, unsigned line) {
+    if (res != cudaSuccess) {
+        std::cerr << file << ':' << line << ' ' << tok
+                  << "failed (" << (unsigned)res << "): " << cudaGetErrorString(res) << std::endl;
+        abort();
+    }
+}
+
+#define CHECK_RT(x) checkRtError(x, #x, __FILE__, __LINE__);
+
+static inline void
+checkDrvError(CUresult res, const char *tok, const char *file, unsigned line) {
+    if (res != CUDA_SUCCESS) {
+        const char *errStr = NULL;
+        (void)cuGetErrorString(res, &errStr);
+        std::cerr << file << ':' << line << ' ' << tok
+                  << "failed (" << (unsigned)res << "): " << errStr << std::endl;
+        abort();
+    }
+}
+
+#define CHECK_DRV(x) checkDrvError(x, #x, __FILE__, __LINE__);
+
+
+// kvCacheRegion class, to warp CUdeviceptr, used to kv-cache tensor
+// record the reserved virtual address size and allocated physical memory size.
+// TODO: we may avoid expose this class externally in the future. 
+class kvCacheRegion : public torch::CustomClassHolder{
+private:
+  // the starting address of this region
+  char * dptr;
+
+  // the size of a kv cache block in bytes
+  uint64_t cache_block_size; 
+
+  // the size of a physical block, which is the multiple of cache_block_size and is aligned with page_size
+  uint64_t physical_block_size; 
+
+  // The actual size that has been mapped successfully
+  uint64_t mapped_size; 
+
+  // virtual address of the next page that needs to be mapped. 
+  // Typically, (nextUnmappedAddr - dptr)/page_size == total_pagees 
+  char * nextUnmappedAddr; 
+
+public:
+
+  kvCacheRegion(int64_t cache_block_size, int64_t physical_block_size, CUdeviceptr ptr);
+
+  ~kvCacheRegion();
+
+  // get the number of physical pages
+  CUdeviceptr getStartPtr(void); 
+  
+  void updateBlocks(uint64_t blocks);
+  void freeAllPhyMemory(void);
+};
+
+
+// kvCacheAllocator class, used for memory allocation of kv-cachemanager, memory allocation is based on page granularity,
+class kvCacheAllocator : public torch::CustomClassHolder{
+private:
+  CUcontext origContext;   
+
+  int64_t region_size; 
+  int64_t cache_block_size;
+  int64_t physical_block_size; 
+  uint64_t page_size;
+  CUdevice device;
+  std::mutex mutex;
+
+  cudaStream_t stream;
+  
+  // the hashtable to record the relationship between regions and ptrs
+  std::unordered_map<uint64_t, kvCacheRegion*> active_regions_map;
+
+  // Internal functions
+  static void *memoryManagerThread(void * arg); 
+  void _releaseRegion(int64_t region_id);
+
+  bool manager_running;
+  bool immediate_allocate; 
+  
+  pthread_t thread_id;
+  pthread_mutex_t mutex_manager;
+  pthread_cond_t  cond_manager; 
+  std::vector<std::vector<int64_t>> update_blocks; 
+  std::vector<std::vector<int64_t>> swap_out_caches; 
+  std::vector<std::vector<int64_t>> swap_in_caches; 
+
+public:
+
+  // The default contructor. Otherwise, torch bindings will complain it. 
+  kvCacheAllocator(int64_t max_gpu_memory_size, int64_t cache_block_size, int64_t region_cache_size);
+
+
+  ~kvCacheAllocator() = default;
+
+   // get the granularity of the physical memory allocation
+  int64_t getPageSize(void);
+
+  // Reserve the virtual address space for a region that is related to one request
+  // In particular, the regionSize == 2 * max_seq_length * layers_num * heads_num * head_size * dtype_size
+  // "2" here is to allocate Key and Value cache together, which helps to reduce the fragmentation 
+  int64_t reserveRegion(int64_t region_id);
+  std::vector<int64_t> allocCPUCaches(int64_t num_caches, int64_t cache_size);
+  void releaseRegions(std::vector<int64_t> regions);
+  
+  void updateBlocks(std::vector<std::vector<int64_t>> reqs_blocks);
+
+  void updateCacheBlocks(bool immediate_allocate,  std::vector<std::vector<int64_t>> to_update_blocks, 
+                      std::vector<std::vector<int64_t>> to_swap_out, std::vector<std::vector<int64_t>> to_swap_in);
+
+  void swapOutCache(std::vector<std::vector<int64_t>> swap_caches, cudaStream_t stream); 
+  void swapInCache(std::vector<std::vector<int64_t>> swap_caches, cudaStream_t stream); 
+
+};
+
diff -Nur orig/csrc/dattn/torch_bindings.cpp dattn/csrc/dattn/torch_bindings.cpp
--- orig/csrc/dattn/torch_bindings.cpp	1970-01-01 00:00:00.000000000 +0000
+++ dattn/csrc/dattn/torch_bindings.cpp	2024-12-30 22:27:03.170886939 +0000
@@ -0,0 +1,21 @@
+/*
+ Copyright (c) ByteDance Inc.
+ Authors: 
+  - Tongping Liu (tongping.liu@bytedance.com)
+  - https://github.com/vllm-project/vllm/pull/6102/commits
+ */ 
+#include "core/registration.h"
+#include "dattn.h"
+
+
+TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, m) {
+  // kvCacheAllocator class bind
+  m.class_<kvCacheAllocator>("kvCacheAllocator")
+    .def(torch::init<int64_t, int64_t, int64_t>())
+    .def("reserveRegion", &kvCacheAllocator::reserveRegion)
+    .def("allocCPUCaches", &kvCacheAllocator::allocCPUCaches)
+    .def("releaseRegions", &kvCacheAllocator::releaseRegions)
+    .def("updateCacheBlocks", &kvCacheAllocator::updateCacheBlocks);
+}
+
+REGISTER_EXTENSION(TORCH_EXTENSION_NAME)
diff -Nur orig/csrc/ops.h dattn/csrc/ops.h
--- orig/csrc/ops.h	2024-12-30 22:26:31.473992447 +0000
+++ dattn/csrc/ops.h	2024-12-30 22:27:03.170886939 +0000
@@ -1,7 +1,9 @@
 #pragma once
 
+#include <vector>
 #include <optional>
 #include <torch/library.h>
+#include <torch/nn/functional.h>
 
 #include "core/scalar_type.hpp"
 
@@ -26,6 +28,30 @@
     const int64_t blocksparse_vert_stride, const int64_t blocksparse_block_size,
     const int64_t blocksparse_head_sliding_step);
 
+void dattention(
+    torch::Tensor& output,
+    torch::Tensor& exp_sums, 
+    torch::Tensor& max_logits,
+    torch::Tensor& tmp_out, 
+    torch::Tensor& query,
+    bool use_reduce,  
+    int64_t layer_idx,
+    int64_t num_layers, 
+    int64_t block_size,
+    int64_t max_seq_len, 
+    torch::Tensor & seq_lens,
+    torch::Tensor & cache_row_mapping, 
+    torch::Tensor & cache_col_mapping,  
+    const std::string& kv_cache_dtype,
+    int64_t num_kv_heads,
+    double  scale,
+    const c10::optional<torch::Tensor>&  alibi_slopes,
+    double k_scale, double v_scale, 
+    const int64_t tp_rank, const int64_t blocksparse_local_blocks,
+    const int64_t blocksparse_vert_stride, const int64_t blocksparse_block_size,
+    const int64_t blocksparse_head_sliding_step);
+
+
 void rms_norm(torch::Tensor& out, torch::Tensor& input, torch::Tensor& weight,
               double epsilon);
 
diff -Nur orig/csrc/torch_bindings.cpp dattn/csrc/torch_bindings.cpp
--- orig/csrc/torch_bindings.cpp	2024-12-30 22:26:31.473992447 +0000
+++ dattn/csrc/torch_bindings.cpp	2024-12-30 22:27:03.170886939 +0000
@@ -47,6 +47,19 @@
       "    int blocksparse_head_sliding_step) -> ()");
   ops.impl("paged_attention_v2", torch::kCUDA, &paged_attention_v2);
 
+  ops.def(
+      "dattention("
+      "    Tensor! output, Tensor exp_sums, Tensor max_logits,"
+      "    Tensor tmp_out, Tensor query, bool use_reduce,"
+      "    int layer_idx, int num_layers, int block_size, int max_seq_len,"
+      "    Tensor seq_lens, Tensor row_mapping, Tensor col_mapping,"
+      "    str kv_cache_dtype, int num_kv_heads, float scale,"
+      "    Tensor? alibi_slopes, float kscale, float v_scale,"
+      "    int tp_rank, int blocksparse_local_blocks,"
+      "    int blocksparse_vert_stride, int blocksparse_block_size,"
+      "    int blocksparse_head_sliding_step) -> ()");
+
+  ops.impl("dattention", torch::kCUDA, &dattention);
   // Activation ops
   // Activation function used in SwiGLU.
   ops.def("silu_and_mul(Tensor! out, Tensor input) -> ()");
@@ -347,6 +360,7 @@
       "Tensor!? azp) -> ()");
   ops.impl("dynamic_scaled_int8_quant", torch::kCUDA,
            &dynamic_scaled_int8_quant);
+
 }
 
 TORCH_LIBRARY_EXPAND(CONCAT(TORCH_EXTENSION_NAME, _cache_ops), cache_ops) {
@@ -356,6 +370,17 @@
       "swap_blocks(Tensor src, Tensor! dst, Tensor block_mapping) -> ()");
   cache_ops.impl("swap_blocks", torch::kCUDA, &swap_blocks);
 
+  // Swap in the cache blocks from src to dst.
+  //cache_ops.def(
+  //    "swap_in_cache(Tensor src_to_dests) -> ()");
+  //cache_ops.impl("swap_in_cache", torch::DispatchKey::CompositeExplicitAutograd, &swap_in_cache);
+
+  // Swap out cache blocks from src to dst.
+  //cache_ops.def(
+  //    "swap_out_cache(Tensor src_to_dests) -> ()");
+  //cache_ops.impl("swap_out_cache", torch::DispatchKey::CompositeExplicitAutograd, &swap_out_cache);
+  //cache_ops.impl("swap_out_cache", torch::kCUDA, &swap_out_cache);
+
   // Copy the cache blocks from src to dst.
   cache_ops.def(
       "copy_blocks(Tensor(a!)[] key_caches, Tensor[](b!) value_caches, "
@@ -382,6 +407,17 @@
   cache_ops.impl("reshape_and_cache_flash", torch::kCUDA,
                  &reshape_and_cache_flash);
 
+  // new add for dAttention
+  cache_ops.def(
+      "reshape_and_cache_dattn(Tensor key, Tensor value,"
+      "                        int layer_idx,"
+      "                        int num_layers,"
+      "                        int block_size,"
+      "                        Tensor cache_row_mapping,"
+      "                        Tensor cache_col_mapping,"
+      "                        str kv_cache_dtype) -> ()");
+  cache_ops.impl("reshape_and_cache_dattn", torch::kCUDA, &reshape_and_cache_dattn); 
+
   // Convert the key and value cache to fp8 data type.
   cache_ops.def(
       "convert_fp8(Tensor! dst_cache, Tensor src_cache, float scale, "
diff -Nur orig/setup.py dattn/setup.py
--- orig/setup.py	2024-12-30 22:26:31.413992736 +0000
+++ dattn/setup.py	2024-12-30 22:27:03.089887329 +0000
@@ -68,7 +68,7 @@
         "so vLLM may not be able to run correctly", sys.platform)
     VLLM_TARGET_DEVICE = "empty"
 
-MAIN_CUDA_VERSION = "12.1"
+MAIN_CUDA_VERSION = "12.4"
 
 
 def is_sccache_available() -> bool:
@@ -152,10 +152,21 @@
         default_cfg = "Debug" if self.debug else "RelWithDebInfo"
         cfg = envs.CMAKE_BUILD_TYPE or default_cfg
 
+        
         # where .so files will be written, should be the same for all extensions
         # that use the same CMakeLists.txt.
         outdir = os.path.abspath(
             os.path.dirname(self.get_ext_fullpath(ext.name)))
+      
+        build_dir = os.path.abspath('build')
+        if not os.path.exists(build_dir):
+            os.makedirs(build_dir) 
+        self.build_temp = build_dir
+        #if not self.build_temp:
+            # Define the path you want to use as the temporary build directory
+            #self.build_temp = os.path.join(os.path.abspath("build"), "temp")
+
+        logger.info("NOOOOOOOOO: self.build_temp %s, outdir %s", self.build_temp, outdir)
 
         cmake_args = [
             '-DCMAKE_BUILD_TYPE={}'.format(cfg),
@@ -221,7 +232,7 @@
 
         # Create build directory if it does not exist.
         if not os.path.exists(self.build_temp):
-            os.makedirs(self.build_temp)
+            os.makedirs(self.build_temp, exist_ok=True)
 
         targets = []
         # Build all the extensions
@@ -456,18 +467,27 @@
     return requirements
 
 
+if "TARGET_MODULES" not in os.environ or not os.environ["TARGET_MODULES"]:
+    default_modules = "_core_C,_dattn_C,_C"
+    target_modules=default_modules.split(",")
+else:
+    target_modules = os.getenv("TARGET_MODULES", "").split(",")
+
 ext_modules = []
 
-if _build_core_ext():
+if _build_core_ext() and  "_core_C" in target_modules:
     ext_modules.append(CMakeExtension(name="vllm._core_C"))
 
-if _is_cuda() or _is_hip():
-    ext_modules.append(CMakeExtension(name="vllm._moe_C"))
+if _is_cuda() and  "_dattn_C" in target_modules:
+    ext_modules.append(CMakeExtension(name="vllm._dattn_C"))
+
+#if _is_cuda() or _is_hip():
+#    ext_modules.append(CMakeExtension(name="vllm._moe_C"))
 
 if _is_hip():
     ext_modules.append(CMakeExtension(name="vllm._rocm_C"))
 
-if _build_custom_ops():
+if _build_custom_ops() and  "_C" in target_modules:
     ext_modules.append(CMakeExtension(name="vllm._C"))
 
 package_data = {
@@ -506,7 +526,7 @@
     packages=find_packages(exclude=("benchmarks", "csrc", "docs", "examples",
                                     "tests*")),
     python_requires=">=3.8",
-    install_requires=get_requirements(),
+   # install_requires=get_requirements(),
     ext_modules=ext_modules,
     extras_require={
         "tensorizer": ["tensorizer>=2.9.0"],
Binary files orig/vllm/_C.abi3.so and dattn/vllm/_C.abi3.so differ
Binary files orig/vllm/_core_C.abi3.so and dattn/vllm/_core_C.abi3.so differ
diff -Nur orig/vllm/_custom_ops.py dattn/vllm/_custom_ops.py
--- orig/vllm/_custom_ops.py	2024-12-30 22:26:31.413992736 +0000
+++ dattn/vllm/_custom_ops.py	2024-12-30 22:27:03.214886727 +0000
@@ -97,7 +97,6 @@
         blocksparse_vert_stride, blocksparse_block_size,
         blocksparse_head_sliding_step)
 
-
 def paged_attention_v2(
     out: torch.Tensor,
     exp_sum: torch.Tensor,
@@ -129,6 +128,60 @@
         blocksparse_local_blocks, blocksparse_vert_stride,
         blocksparse_block_size, blocksparse_head_sliding_step)
 
+# page attention ops for dAttention
+def dattention(
+    output: torch.Tensor,
+    exp_sum: torch.Tensor,
+    max_logits: torch.Tensor,
+    tmp_out: torch.Tensor,
+    query: torch.Tensor,
+    use_reduce: bool, 
+    layer_idx: int,
+    num_layers: int,
+    block_size: int,
+    max_seq_len: int, 
+    seq_lens: torch.Tensor,
+    cache_row_mapping: torch.Tensor, 
+    cache_col_mapping: torch.Tensor,  
+    kv_cache_dtype: str,
+    num_kv_heads: int,
+    scale: float,
+    alibi_slopes: Optional[torch.Tensor],
+    k_scale: float,
+    v_scale: float,
+    tp_rank: int = 0,
+    blocksparse_local_blocks: int = 0,
+    blocksparse_vert_stride: int = 0,
+    blocksparse_block_size: int = 64,
+    blocksparse_head_sliding_step: int = 0, 
+) -> None:
+    #print(f"before calling torch.ops._C.dattention, scale:{scale}, kv_scale:{kv_scale}\n")
+    torch.ops._C.dattention(
+            output,
+            exp_sum, 
+            max_logits, 
+            tmp_out,
+            query,
+            use_reduce, 
+            layer_idx,
+            num_layers,
+            block_size,
+            max_seq_len, 
+            seq_lens,
+            cache_row_mapping, 
+            cache_col_mapping,  
+            kv_cache_dtype,
+            num_kv_heads,
+            scale,
+            alibi_slopes,
+            k_scale,
+            v_scale, 
+            tp_rank, 
+            blocksparse_local_blocks,
+            blocksparse_vert_stride, 
+            blocksparse_block_size,
+            blocksparse_head_sliding_step,   
+        )
 
 def paged_attention_rocm(
     out: torch.Tensor,
@@ -842,6 +895,24 @@
                                                    v_scale)
 
 
+# new add for dAttention
+def reshape_and_cache_dattn(
+    key: torch.Tensor,
+    value: torch.Tensor,
+    layer_idx: int,
+    num_layers: int, 
+    block_size: int,
+    cache_row_mapping: torch.Tensor,
+    cache_col_mapping: torch.Tensor,
+    kv_cache_dtype: str,
+) -> None:
+    torch.ops._C_cache_ops.reshape_and_cache_dattn(key, value, layer_idx, 
+                                                num_layers,
+                                                block_size,
+                                                cache_row_mapping,
+                                                cache_col_mapping, kv_cache_dtype)
+    
+
 def copy_blocks(key_caches: List[torch.Tensor],
                 value_caches: List[torch.Tensor],
                 block_mapping: torch.Tensor) -> None:
Binary files orig/vllm/_dattn_C.abi3.so and dattn/vllm/_dattn_C.abi3.so differ
diff -Nur orig/vllm/_dattn_ops.py dattn/vllm/_dattn_ops.py
--- orig/vllm/_dattn_ops.py	1970-01-01 00:00:00.000000000 +0000
+++ dattn/vllm/_dattn_ops.py	2024-12-30 22:27:03.214886727 +0000
@@ -0,0 +1,88 @@
+'''
+ Copyright (c) ByteDance Inc.
+ Authors: 
+  - Tongping Liu (tongping.liu@bytedance.com)
+  - https://github.com/vllm-project/vllm/pull/6102/commits
+'''
+
+import torch
+from vllm.logger import init_logger
+from typing import List, Optional, Tuple, Type
+import sys
+
+logger = init_logger(__name__)
+
+try:
+    import vllm._dattn_C # noqa: F401
+except ImportError as e:
+    logger.warning("Import dattn error msg: %s", e.msg)
+
+"""
+# It seems that there is no need for this function, since we will utilize the 
+# same 
+# cache device ptr, used for kv cache tensor
+class kvCacheRegion:
+    def __init__(self):
+        self._ptr = torch.classes._dattn_C.kvCacheRegion()
+    
+    @property
+    def reserved_page_num(self):
+        return self._ptr.revervedPageNum
+    
+    @reserved_page_num.setter
+    def reserved_page_num(self, value:int):
+        self._ptr.reservedPageNum = value
+    
+    @property
+    def allocated_page_num(self):
+        return self._ptr.allocatedPageNum
+    
+    @allocated_page_num.setter
+    def allocated_page_num(self, value:int):
+        self._ptr.allocatedPageNum = value
+"""
+
+
+# cache allocator based dAttention, used to manage kv cache tensor
+class kvCacheAllocator:
+    def __init__(self, 
+                 max_gpu_memory_size, 
+                 block_bytes_size,
+                 cache_space_per_req,  
+        ):
+        self.block_size = block_bytes_size
+        self._allocator = torch.classes._dattn_C.kvCacheAllocator(max_gpu_memory_size,
+                                                                  block_bytes_size,
+                                                                  cache_space_per_req 
+                                                                  )
+
+    
+    #def reserve_cache_ptr(self, ptr:CacheDevicePtr, page_num:int = 1):
+    def reserve_cache_region(self, req_id:int = 0):
+        #print(f"NOOW, in reserve_cache_region, with req_id:{req_id}")
+        ptr = self._allocator.reserveRegion(req_id)
+        #print(f"NOOW, in reserve_cache_region, with req_id:{req_id}, ptr:{ptr}")
+        # TODO: wrap the ptr to a tensor
+        #return wrapDptr2Tensor()
+        return ptr 
+
+    def alloc_cpu_caches(self, cache_num:int, cache_space_per_req: int):
+        caches = self._allocator.allocCPUCaches(cache_num, cache_space_per_req)
+        return caches
+
+    #def alloc_cache_ptr(self, ptr:CacheDevicePtr, page_num:int = 1, offset:int = 0):    
+    #def free_cache_ptr(self, ptr:CacheDevicePtr):
+    #def release_cache_ptr(self, ptr:CacheDevicePtr, page_num: int = 0, offset: int = 0):
+   
+    def release_cache_regions(self, free_caches: List[int]):
+        self._allocator.releaseRegions(free_caches)
+        return 
+
+    def update_cache_blocks(self, 
+                            immediate_allocate: bool, 
+                            to_update_blocks:List[List[int]], 
+                            to_swap_out: List[List[int]], 
+                            to_swap_in: List[List[int]]):
+        #print(f"before invoking updateCacheBlocks!!!!", file=sys.stderr)
+        return self._allocator.updateCacheBlocks(immediate_allocate, to_update_blocks, to_swap_out, to_swap_in)
+     
\ No newline at end of file
Binary files orig/vllm/_moe_C.abi3.so and dattn/vllm/_moe_C.abi3.so differ
diff -Nur orig/vllm/attention/backends/utils.py dattn/vllm/attention/backends/utils.py
--- orig/vllm/attention/backends/utils.py	2024-12-30 22:26:31.415992726 +0000
+++ dattn/vllm/attention/backends/utils.py	2024-12-30 22:27:03.187886857 +0000
@@ -143,7 +143,7 @@
 
     def _add_seq_group(
             self, inter_data: "ModelInputForGPUBuilder.InterDataForSeqGroup",
-            chunked_prefill_enabled: bool):
+            chunked_prefill_enabled: bool, use_dattn: bool):
         is_prompt = inter_data.is_prompt
         block_tables = inter_data.block_tables
         computed_block_nums = inter_data.computed_block_nums
@@ -171,24 +171,26 @@
             # only allowing multiple of block_size chunk size.
             # NOTE: This only works for oooooooxxx style attention.
             block_table = []
-            if inter_data.prefix_cache_hit:
-                block_table = computed_block_nums
-            elif ((chunked_prefill_enabled or not is_prompt)
-                  and block_tables is not None):
-                block_table = block_tables[seq_id][-curr_sliding_window_block:]
-            self.block_tables.append(block_table)
-
-            # Compute slot mapping.
-            is_profile_run = is_block_tables_empty(block_tables)
-            start_idx = compute_slot_mapping_start_idx(
-                is_prompt, query_len, context_len, self.sliding_window,
-                self.use_v2_block_manager)
-            compute_slot_mapping(is_profile_run, self.slot_mapping, seq_id,
+            if not use_dattn:
+                if inter_data.prefix_cache_hit:
+                    block_table = computed_block_nums
+                elif ((chunked_prefill_enabled or not is_prompt)
+                    and block_tables is not None):
+                    block_table = block_tables[seq_id][-curr_sliding_window_block:]
+                self.block_tables.append(block_table)
+                
+
+                # Compute slot mapping.
+                is_profile_run = is_block_tables_empty(block_tables)
+                start_idx = compute_slot_mapping_start_idx(
+                    is_prompt, query_len, context_len, self.sliding_window,
+                    self.use_v2_block_manager)
+                compute_slot_mapping(is_profile_run, self.slot_mapping, seq_id,
                                  seq_len, context_len, start_idx,
                                  self.block_size, inter_data.block_tables)
 
     def build(self, seq_lens: List[int], query_lens: List[int],
-              cuda_graph_pad_size: int, batch_size: int):
+              cuda_graph_pad_size: int, batch_size: int, use_dattn: bool):
         """Build attention metadata with on-device tensors.
 
         Args:
@@ -200,17 +202,18 @@
         """
         for inter_data in self.input_builder.inter_data_list:
             self._add_seq_group(inter_data,
-                                self.input_builder.chunked_prefill_enabled)
+                                self.input_builder.chunked_prefill_enabled, use_dattn)
 
         device = self.runner.device
         use_captured_graph = cuda_graph_pad_size != -1
-
+        
         max_query_len = max(query_lens)
         max_prefill_seq_len = max(self.prefill_seq_lens, default=0)
         max_decode_seq_len = max(self.curr_seq_lens, default=0)
         num_decode_tokens = self.num_decode_tokens
 
-        if use_captured_graph:
+        if not use_dattn and use_captured_graph:
+            print(f"IN use_captured_graph!!!")
             self.slot_mapping.extend([PAD_SLOT_ID] * cuda_graph_pad_size)
             self.block_tables.extend([] * cuda_graph_pad_size)
             num_decode_tokens = batch_size
@@ -223,13 +226,20 @@
                     input_block_tables[i, :len(block_table)] = block_table
             block_tables = torch.from_numpy(input_block_tables).to(
                 device, non_blocking=True)
-        else:
+            slot_mapping_tensor = async_tensor_h2d(self.slot_mapping, torch.long,
+                                               device, self.runner.pin_memory)
+        elif not use_dattn:
             block_tables = make_tensor_with_pad(
                 self.block_tables,
                 pad=0,
                 dtype=torch.int,
                 device=device,
             )
+            slot_mapping_tensor = async_tensor_h2d(self.slot_mapping, torch.long,
+                                               device, self.runner.pin_memory)
+        else:
+            slot_mapping_tensor = None
+            block_tables = None
         assert max_query_len > 0, "query_lens: {}".format(query_lens)
 
         assert device is not None
@@ -239,8 +249,7 @@
                                            self.runner.pin_memory)
         query_lens_tensor = async_tensor_h2d(query_lens, torch.long, device,
                                              self.runner.pin_memory)
-        slot_mapping_tensor = async_tensor_h2d(self.slot_mapping, torch.long,
-                                               device, self.runner.pin_memory)
+        
         query_start_loc = torch.zeros(query_lens_tensor.shape[0] + 1,
                                       dtype=torch.int32,
                                       device=device)
@@ -358,8 +367,9 @@
             is_encoder_decoder_model: bool = False) -> None:
         input_buffers["seq_lens_tensor"].copy_(
             attn_metadata.decode_metadata.seq_lens_tensor, non_blocking=True)
-        input_buffers["block_tables"].copy_(
-            attn_metadata.decode_metadata.block_tables, non_blocking=True)
+        if not attn_metadata.use_dattn:
+            input_buffers["block_tables"].copy_(
+                attn_metadata.decode_metadata.block_tables, non_blocking=True)
         if is_encoder_decoder_model:
             # The encoder decoder model works only with XFormers backend.
             # Assert the same.
diff -Nur orig/vllm/attention/backends/xformers.py dattn/vllm/attention/backends/xformers.py
--- orig/vllm/attention/backends/xformers.py	2024-12-30 22:26:31.415992726 +0000
+++ dattn/vllm/attention/backends/xformers.py	2024-12-30 22:27:03.187886857 +0000
@@ -16,9 +16,10 @@
 from vllm.attention.ops.paged_attn import (PagedAttention,
                                            PagedAttentionMetadata)
 from vllm.logger import init_logger
-
+import time
 logger = init_logger(__name__)
-
+import sys
+import os
 
 class XFormersBackend(AttentionBackend):
 
@@ -144,6 +145,14 @@
     cross_slot_mapping: Optional[torch.Tensor] = None
     cross_block_tables: Optional[torch.Tensor] = None
 
+    # Added for dAttention
+    use_dattn: bool = False
+    num_layers: int = 0
+    block_size: int = 0
+    cache_batch_idx: Optional[torch.Tensor] = None # (batch_size, ) the index of batch in cache
+    cache_row_mapping: Optional[torch.Tensor] = None  # (num_tokens,)  record key/value write to which seq row in cache
+    cache_col_mapping: Optional[torch.Tensor] = None  # (num_tokens,)  record key/value write to which token col in cache
+
     def __post_init__(self):
         # Set during the execution of the first attention op.
         # It is a list because it is needed to set per prompt
@@ -192,16 +201,21 @@
         # Compute some attn_metadata fields which default to None
         query_start_loc = (None if self.query_start_loc is None else
                            self.query_start_loc[:self.num_prefills + 1])
-        slot_mapping = (None if self.slot_mapping is None else
-                        self.slot_mapping[:self.num_prefill_tokens])
         seq_lens = (None if self.seq_lens is None else
                     self.seq_lens[:self.num_prefills])
         seq_lens_tensor = (None if self.seq_lens_tensor is None else
                            self.seq_lens_tensor[:self.num_prefills])
         context_lens_tensor = (None if self.context_lens_tensor is None else
                                self.context_lens_tensor[:self.num_prefills])
-        block_tables = (None if self.block_tables is None else
-                        self.block_tables[:self.num_prefills])
+
+        #print(f"self.use_dattn:{self.use_dattn}, cache_batch_idx:{self.cache_batch_idx}", file=sys.stderr)
+        if not self.use_dattn:
+            assert self.block_tables is not None
+            block_tables=self.block_tables[:self.num_prefills]
+            slot_mapping=self.slot_mapping[:self.num_prefill_tokens]
+        else: 
+            block_tables = None
+            slot_mapping = None 
 
         # Construct & cache prefill-phase attention metadata structure
         self._cached_prefill_metadata = XFormersMetadata(
@@ -218,6 +232,13 @@
             context_lens_tensor=context_lens_tensor,
             block_tables=block_tables,
             use_cuda_graph=False,
+            # Support dattn
+            use_dattn=self.use_dattn,
+            num_layers=self.num_layers,
+            block_size=self.block_size,
+            cache_batch_idx=self.cache_batch_idx,
+            cache_row_mapping=self.cache_row_mapping,
+            cache_col_mapping=self.cache_col_mapping,            
             # Begin encoder & cross attn fields below...
             encoder_seq_lens=self.encoder_seq_lens,
             encoder_seq_lens_tensor=self.encoder_seq_lens_tensor,
@@ -239,12 +260,16 @@
                 or (self.encoder_seq_lens_tensor is not None))
 
         # Compute some attn_metadata fields which default to None
-        slot_mapping = (None if self.slot_mapping is None else
-                        self.slot_mapping[self.num_prefill_tokens:])
         seq_lens_tensor = (None if self.seq_lens_tensor is None else
                            self.seq_lens_tensor[self.num_prefills:])
-        block_tables = (None if self.block_tables is None else
-                        self.block_tables[self.num_prefills:])
+
+        if not self.use_dattn:
+            assert self.block_tables is not None
+            block_tables=self.block_tables[:self.num_prefills]
+            slot_mapping=self.slot_mapping[:self.num_prefill_tokens]
+        else: 
+            block_tables = None
+            slot_mapping = None 
 
         # Construct & cache decode-phase attention metadata structure
         self._cached_decode_metadata = XFormersMetadata(
@@ -257,6 +282,13 @@
             max_decode_seq_len=self.max_decode_seq_len,
             block_tables=block_tables,
             use_cuda_graph=self.use_cuda_graph,
+            # Support dattn
+            use_dattn=self.use_dattn,
+            num_layers=self.num_layers,
+            block_size=self.block_size, 
+            cache_batch_idx=self.cache_batch_idx,
+            cache_row_mapping=self.cache_row_mapping,
+            cache_col_mapping=self.seq_lens_tensor[self.num_prefills:],   
             # Begin encoder & cross attn fields below...
             encoder_seq_lens=self.encoder_seq_lens,
             encoder_seq_lens_tensor=self.encoder_seq_lens_tensor,
@@ -433,7 +465,19 @@
 
         assert self.num_heads % self.num_kv_heads == 0
         self.num_queries_per_kv = self.num_heads // self.num_kv_heads
+        self.profile = os.getenv("PROFILE", "False") == "True"
+        self.profile = False
+         
+        if self.profile == True:
+            self.step = 0
+            self.cache_time = 0.0
+            self.attent_time = 0.0
+            self.schedule_time = 0.0
+            self.stop_time = 0.0
+            self.start = torch.cuda.Event(enable_timing=True)
+            self.stop = torch.cuda.Event(enable_timing=True)
 
+        #print(f"XFormersImplXFormersImplXFormersImplXFormersImplXFormersImpl")
         suppored_head_sizes = PagedAttention.get_supported_head_sizes()
         if head_size not in suppored_head_sizes:
             raise ValueError(
@@ -510,6 +554,7 @@
                                  "requires setting cross-attention "
                                  "metadata attributes.")
 
+
         query = query.view(-1, self.num_heads, self.head_size)
         if key is not None:
             assert value is not None
@@ -518,41 +563,86 @@
         else:
             assert value is None
 
+        if self.profile == True:
+            self.step += 1
+            self.start.record()
+            start_time = time.time()
+            if self.stop_time != 0:
+                self.schedule_time += (start_time - self.stop_time)
+                self.stop_time = 0
+
         # Self-attention vs. cross-attention will impact
         # which KV cache memory-mapping & which
         # seqlen datastructures we utilize
 
         if (attn_type != AttentionType.ENCODER and kv_cache is not None):
-            # KV-cache during decoder-self- or
-            # encoder-decoder-cross-attention, but not
-            # during encoder attention.
-            #
-            # Even if there are no new key/value pairs to cache,
-            # we still need to break out key_cache and value_cache
-            # i.e. for later use by paged attention
-            key_cache, value_cache = PagedAttention.split_kv_cache(
-                kv_cache, self.num_kv_heads, self.head_size)
-
-            if (key is not None) and (value is not None):
-
-                if attn_type == AttentionType.ENCODER_DECODER:
-                    # Update cross-attention KV cache (prefill-only)
-                    # During cross-attention decode, key & value will be None,
-                    # preventing this IF-statement branch from running
-                    updated_slot_mapping = attn_metadata.cross_slot_mapping
-                else:
-                    # Update self-attention KV cache (prefill/decode)
-                    updated_slot_mapping = attn_metadata.slot_mapping
-
-                # Reshape the input keys and values and store them in the cache.
-                # If kv_cache is not provided, the new key and value tensors are
-                # not cached. This happens during the initial memory
-                # profiling run.
-                PagedAttention.write_to_paged_cache(key, value, key_cache,
+            
+            if attn_metadata.use_dattn != True:
+                # print(f"attn_metadata.use_dattn is NOT TRUE, kv_cache.shape:{kv_cache.shape}")
+                # KV-cache during decoder-self- or
+                # encoder-decoder-cross-attention, but not
+                # during encoder attention.
+                #
+                # Even if there are no new key/value pairs to cache,
+                # we still need to break out key_cache and value_cache
+                # i.e. for later use by paged attention
+                key_cache, value_cache = PagedAttention.split_kv_cache(
+                    kv_cache, self.num_kv_heads, self.head_size)
+
+                if (key is not None) and (value is not None):
+
+                    if attn_type == AttentionType.ENCODER_DECODER:
+                        # Update cross-attention KV cache (prefill-only)
+                        # During cross-attention decode, key & value will be None,
+                        # preventing this IF-statement branch from running
+                        updated_slot_mapping = attn_metadata.cross_slot_mapping
+                    else:
+                        # Update self-attention KV cache (prefill/decode)
+                        updated_slot_mapping = attn_metadata.slot_mapping
+
+                    #print(f"updated_slot_mapping:{updated_slot_mapping.shape}, key:{key.shape}, value:{value.shape}")
+                    # Reshape the input keys and values and store them in the cache.
+                    # If kv_cache is not provided, the new key and value tensors are
+                    # not cached. This happens during the initial memory
+                    # profiling run.
+                    PagedAttention.write_to_paged_cache(key, value, key_cache,
                                                     value_cache,
                                                     updated_slot_mapping,
                                                     self.kv_cache_dtype,
                                                     k_scale, v_scale)
+            else: # use_dattn is True
+                # Reshape the input keys and values and store them in the cache.
+                # If kv_cache is not provided, the new key and value tensors are
+                # not cached. This happens during the initial memory profiling run.
+                layer_idx = kv_cache.item()
+
+                #print(f"layer_idx is {layer_idx}")
+                #torch.set_printoptions(precision=2, sci_mode=False)
+                #print(f"key.shape:{key.shape}, type:{key.dtype}, key.elements:{key.numel()}, key.size:{key.storage().nbytes() } stride:{key.stride(0)}") 
+                #print(f"value.shape:{value.shape}, type:{key.dtype} \n{value[:3,:1,]}") 
+                #if layer_idx > 50:
+                #    print(f"key.shape:{key.shape}, type:{key.dtype} \n", file=sys.stderr) 
+                #    print(f"value.shape:{value.shape}, type:{key.dtype} \n", file=sys.stderr) 
+                #    print(f"layers:{attn_metadata.num_layers}, block_size:{attn_metadata.block_size}\n", file=sys.stderr)
+                #    print(f"NNNNNN attn_metadata.cache_row_mapping:{attn_metadata.cache_row_mapping}\n", file=sys.stderr)
+                #    if attn_metadata.cache_row_mapping.shape == torch.Size([1]):
+                #        print(f"attn_metadata.cache_row_mapping:{hex(attn_metadata.cache_row_mapping)}, col_mapping:{hex(attn_metadata.cache_col_mapping)}", file=sys.stderr)
+                #        print(f"attn_metadata.cache_row_mapping at 0x{attn_metadata.cache_row_mapping.data_ptr():X}, col_mapping at 0x{attn_metadata.cache_col_mapping.data_ptr():X}", file=sys.stderr)
+                
+                #print(f"attn_metadata.cache_col_mapping:{attn_metadata.cache_col_mapping}", file=sys.stderr)
+                PagedAttention.write_to_paged_cache_dattn(key, value, layer_idx,
+                                                          attn_metadata.num_layers,
+                                                          attn_metadata.block_size,  
+                                                          attn_metadata.cache_row_mapping, 
+                                                          attn_metadata.cache_col_mapping,
+                                                          self.kv_cache_dtype)  
+
+            if self.profile == True:
+                self.stop.record()
+                torch.cuda.synchronize()
+                self.cache_time += self.start.elapsed_time(self.stop)
+                #self.cache_time += time.time() - start_time
+              
 
         if attn_type != AttentionType.ENCODER:
             # Decoder self-attention supports chunked prefill.
@@ -585,10 +675,12 @@
 
         assert query.shape[0] == num_prefill_tokens
         assert decode_query.shape[0] == num_decode_tokens
+        layer_idx = 0
 
+        #print(f"num_prefill_tokens:{num_prefill_tokens}, num_decode_tokens:{num_decode_tokens}")
         if prefill_meta := attn_metadata.prefill_metadata:
             # Prompt run.
-            if kv_cache is None or prefill_meta.block_tables.numel() == 0:
+            if kv_cache is None or prefill_meta.use_dattn or prefill_meta.block_tables.numel() == 0:
                 # normal attention.
                 # block tables are empty if the prompt does not have a cached
                 # prefix.
@@ -626,30 +718,81 @@
                 output[:num_prefill_tokens] = out
 
         if decode_meta := attn_metadata.decode_metadata:
-
-            (
+            if self.profile == True:
+                start_time = time.time()
+                self.start.record()
+                        
+            if not attn_metadata.use_dattn: 
+                (
                 seq_lens_arg,
                 max_seq_len_arg,
                 block_tables_arg,
-            ) = _get_seq_len_block_table_args(decode_meta, False, attn_type)
+                ) = _get_seq_len_block_table_args(decode_meta, False, attn_type)
 
-            output[num_prefill_tokens:] = PagedAttention.forward_decode(
-                decode_query,
-                key_cache,
-                value_cache,
-                block_tables_arg,
-                seq_lens_arg,
-                max_seq_len_arg,
-                self.kv_cache_dtype,
-                self.num_kv_heads,
-                self.scale,
-                self.alibi_slopes,
-                k_scale,
-                v_scale,
-            )
+                #print(f"max_seq_len_arg:{max_seq_len_arg}, max_seq_len:{decode_meta.max_decode_seq_len}", file=sys.stderr)
+                output[num_prefill_tokens:] = PagedAttention.forward_decode(
+                    decode_query,
+                    key_cache,
+                    value_cache,
+                    block_tables_arg,
+                    seq_lens_arg,
+                    max_seq_len_arg,
+                    self.kv_cache_dtype,
+                    self.num_kv_heads,
+                    self.scale,
+                    self.alibi_slopes,
+                    k_scale,
+                    v_scale,
+                )
+            else: 
+                assert attn_metadata.use_dattn == True
+                layer_idx = kv_cache.item()
+
+                #print(f"max_seq_len_arg:{decode_meta.max_decode_seq_len}", file=sys.stderr)
+                #print(f"decoding: layer_idx:{layer_idx} after printing\n", file=sys.stderr)
+                #print(f"decoding: layer_idx:{layer_idx}, decode_meta.num_layers:{decode_meta.num_layers}, decode_meta.block_size:{decode_meta.block_size}, decode_meta.cache_row_mapping:{decode_meta.cache_row_mapping.shape}, decode_meta.cache_col_mapping:{decode_meta.cache_col_mapping}")
+                #print(f"decoding: layer_idx:{layer_idx}, decode_meta.num_layers:{decode_meta.num_layers}", file=sys.stderr)
+                #print(f"decoding: layer_idx:{layer_idx}, cache_row_mapping:{decode_meta.cache_row_mapping}, cache_col_mapping:{decode_meta.cache_col_mapping}", file=sys.stderr)
+                #print(f"decode_meta.seq_lens_tensor:{decode_meta.seq_lens_tensor}, decode_meta.max_decode_seq_len:{decode_meta.max_decode_seq_len}", file=sys.stderr) 
+                output[num_prefill_tokens:] = PagedAttention.forward_decode_dattn(
+                    decode_query,
+                    layer_idx,
+                    decode_meta.num_layers,
+                    decode_meta.block_size,
+                    decode_meta.max_decode_seq_len, 
+                    decode_meta.seq_lens_tensor, 
+                    decode_meta.cache_row_mapping, 
+                    decode_meta.cache_col_mapping,
+                    self.kv_cache_dtype,
+                    self.num_kv_heads,
+                    self.scale,
+                    self.alibi_slopes,
+                    k_scale,
+                    v_scale,
+                )
+
+            if self.profile == True:
+                self.stop.record()
+                torch.cuda.synchronize()
+                self.attent_time += self.start.elapsed_time(self.stop)
+                #self.attent_time += time.time() - start_time
+            
+        if self.profile == True:
+            #print(f"STEP:{self.step}, lay_index:{layer_idx}, cache time:{self.cache_time}, attent time: {self.attent_time}, loop time: {self.schedule_time}", file=sys.stderr) 
+            if self.step % 512 == 0:
+                if layer_idx % 16 == 1: 
+                    #print(f"STEP:{self.step}, lay_index:{layer_idx}, cache time:{self.cache_time}, attent time: {self.attent_time}, loop time: {self.schedule_time}. Portion: 1-{self.attent_time/self.cache_time}-{self.schedule_time/(self.cache_time*32)}", file=sys.stderr)
+                    print(f"STEP:{self.step}, lay_index:{layer_idx}, cache time:{self.cache_time}, attent time: {self.attent_time}, loop time: {self.schedule_time}, portion: 1-{self.attent_time/self.cache_time}", file=sys.stderr)
+                print(f"STEP:{self.step}, lay_index:{layer_idx}, cache time:{self.cache_time}, attent time: {self.attent_time}, loop time: {self.schedule_time}. Portion: 1-{self.attent_time/self.cache_time}-{self.schedule_time/(self.cache_time*32)}", file=sys.stderr)
+                self.cache_time = 0
+                self.attent_time = 0
+                self.schedule_time = 0
+            self.stop_time = time.time()
 
         # Reshape the output tensor.
-        return output.view(-1, self.num_heads * self.head_size)
+        result = output.view(-1, self.num_heads * self.head_size)
+     
+        return result
 
     def _run_memory_efficient_xformers_forward(
         self,
diff -Nur orig/vllm/attention/ops/paged_attn.py dattn/vllm/attention/ops/paged_attn.py
--- orig/vllm/attention/ops/paged_attn.py	2024-12-30 22:26:31.415992726 +0000
+++ dattn/vllm/attention/ops/paged_attn.py	2024-12-30 22:27:03.188886852 +0000
@@ -84,6 +84,28 @@
         )
 
     @staticmethod
+    def write_to_paged_cache_dattn(
+        key: torch.Tensor,
+        value: torch.Tensor,
+        layer_idx: int,
+        num_layers: int, 
+        block_size: int, 
+        cache_row_mapping: torch.Tensor, 
+        cache_col_mapping: torch.Tensor,  
+        kv_cache_dtype: str,
+    ) -> None:
+        ops.reshape_and_cache_dattn(
+            key,
+            value,
+            layer_idx,
+            num_layers, 
+            block_size, 
+            cache_row_mapping, 
+            cache_col_mapping, 
+            kv_cache_dtype
+        )
+
+    @staticmethod
     def forward_decode(
         query: torch.Tensor,
         key_cache: torch.Tensor,
@@ -126,6 +148,9 @@
         use_v1 = (max_seq_len <= 8192
                   and (max_num_partitions == 1 or num_seqs * num_heads > 512))
 
+        #if use_v1 == True and max_num_partitions > 1:
+        #    print(f"DDDD: use_v1:{use_v1}, max_seq_len:{max_seq_len}, max_num_partitions:{max_num_partitions}, num_seqs:{num_seqs} num_seqs * num_heads:{num_seqs * num_heads} ")
+
         if use_v1:
             # Run PagedAttention V1.
             ops.paged_attention_v1(
@@ -190,6 +215,75 @@
         return output
 
     @staticmethod
+    
+    def forward_decode_dattn(
+        query: torch.Tensor,
+        layer_idx: int,
+        num_layers: int,
+        block_size: int,
+        max_seq_len: int, 
+        seq_lens: torch.Tensor,
+        cache_row_mapping: torch.Tensor, 
+        cache_col_mapping: torch.Tensor,  
+        kv_cache_dtype: str,
+        num_kv_heads: int,
+        scale: float,
+        alibi_slopes: Optional[torch.Tensor],
+        k_scale: float,
+        v_scale: float,
+    ) -> torch.Tensor:
+        output = torch.empty_like(query)
+        num_seqs, num_heads, head_size = query.shape
+        max_num_partitions = ((max_seq_len + _PARTITION_SIZE - 1) //
+                              _PARTITION_SIZE)
+        
+        no_reduce = ((max_seq_len <= 8192) and (max_num_partitions == 1 or num_seqs * num_heads > 512))
+        #no_reduce = ((max_seq_len <= 8192) and (max_num_partitions == 1))
+        use_reduce = not no_reduce
+        if use_reduce:
+            #print(f"using reduce now!!!", file=sys.stderr) 
+            tmp_output = torch.empty(
+                size=(num_seqs, num_heads, max_num_partitions, head_size),
+                dtype=output.dtype,
+                device=output.device,
+            )
+            exp_sums = torch.empty(
+                size=(num_seqs, num_heads, max_num_partitions),
+                dtype=torch.float32,
+                device=output.device,
+            )
+            max_logits = torch.empty_like(exp_sums)
+            #import sys
+            #print(f"forward_decode_dattn on long sequence: Query shape: {output.shape}", file=sys.stderr)
+        else:
+            tmp_output = None
+            exp_sums = None
+            max_logits = None
+        ops.dattention(
+            output,
+            exp_sums,
+            max_logits,
+            tmp_output,
+            query,
+            use_reduce, 
+            layer_idx,
+            num_layers,
+            block_size,
+            max_seq_len, 
+            seq_lens,
+            cache_row_mapping, 
+            cache_col_mapping,  
+            kv_cache_dtype,
+            num_kv_heads,
+            scale,
+            alibi_slopes,
+            k_scale,
+            v_scale,     
+        ) 
+
+        return output
+    
+    @staticmethod
     def forward_prefix(
         query: torch.Tensor,
         key: torch.Tensor,
@@ -251,3 +345,4 @@
         key_caches = [kv_cache[0] for kv_cache in kv_caches]
         value_caches = [kv_cache[1] for kv_cache in kv_caches]
         ops.copy_blocks(key_caches, value_caches, src_to_dists)
+
diff -Nur orig/vllm/commit_id.py dattn/vllm/commit_id.py
--- orig/vllm/commit_id.py	1970-01-01 00:00:00.000000000 +0000
+++ dattn/vllm/commit_id.py	2024-12-30 22:27:03.191886838 +0000
@@ -0,0 +1 @@
+__commit__ = "c631cf3108836d5f77ec5be533f5a505557f45f3"
diff -Nur orig/vllm/config.py dattn/vllm/config.py
--- orig/vllm/config.py	2024-12-30 22:26:31.413992736 +0000
+++ dattn/vllm/config.py	2024-12-30 22:27:03.186886862 +0000
@@ -30,6 +30,7 @@
 
 logger = init_logger(__name__)
 
+_MB = 1 << 20
 _EMBEDDING_MODEL_MAX_NUM_BATCHED_TOKENS = 32768
 _MULTIMODAL_MODEL_MAX_NUM_BATCHED_TOKENS = 4096
 
@@ -589,7 +590,10 @@
         num_gpu_blocks_override: Optional[int] = None,
         sliding_window: Optional[int] = None,
         enable_prefix_caching: bool = False,
+        use_dattn: bool = False,
         cpu_offload_gb: float = 0,
+        vmm_frequency: int = 2, 
+        num_cpu_caches: int = 80
     ) -> None:
         self.block_size = block_size
         self.gpu_memory_utilization = gpu_memory_utilization
@@ -607,6 +611,11 @@
         self.num_gpu_blocks = None
         self.num_cpu_blocks = None
 
+        # Initialization for dattn
+        self.use_dattn = use_dattn
+        self.num_cpu_caches = num_cpu_caches 
+        self.vmm_frequency = vmm_frequency
+
     def metrics_info(self):
         # convert cache_config to dict(key: str, value: str) for prometheus
         # metrics info
diff -Nur orig/vllm/core/block_manager_dattn.py dattn/vllm/core/block_manager_dattn.py
--- orig/vllm/core/block_manager_dattn.py	1970-01-01 00:00:00.000000000 +0000
+++ dattn/vllm/core/block_manager_dattn.py	2024-12-30 22:27:03.189886848 +0000
@@ -0,0 +1,538 @@
+'''
+ Copyright (c) ByteDance Inc.
+ Authors: 
+  - Tongping Liu (tongping.liu@bytedance.com)
+
+  This file will manage blocks and cache ids for both CPU and GPU memory. 
+  However, the address related to each cache id will be tracked and managed by CacheEngineDattn
+
+  Adopted from https://github.com/vllm-project/vllm/pull/6102/commits
+'''
+from collections import deque
+from typing import Dict, List, Optional, Tuple
+
+from vllm.core.block.utils import check_no_caching_or_swa_for_blockmgr_encdec
+from vllm.core.evictor_v1 import EvictionPolicy, Evictor, make_evictor
+from vllm.core.interfaces import AllocStatus, BlockSpaceManager
+from vllm.logger import init_logger
+from vllm.sequence import Sequence, SequenceGroup, SequenceStatus
+from vllm.utils import Device, Counter
+from collections import deque
+from dataclasses import dataclass, field
+import sys
+import torch
+
+logger = init_logger(__name__)
+
+class CacheAllocator:
+    def __init__(self, name: str, num_caches: int):
+        self.num_caches = num_caches
+        self.type = name 
+        # kv_caches: tracking the available cache ids
+        self.kv_caches = deque(range(num_caches))
+
+    def allocate(self) -> int:
+        assert len(self.kv_caches) > 0, f"Please set self.num_caches to a bigger value"
+        cache_id = self.kv_caches.popleft() 
+        #    print(f"ERROR: self.kv_caches is 000000000 NOW", file=sys.stderr)
+        #elif self.type == "cpu":
+        #    print(f"ERROR checking: allocated a cpu cache:{cache_id}, remaining cache:{len(self.kv_caches)}", file=sys.stderr) 
+        return cache_id
+
+    def free(self, cache_id: int):
+        self.kv_caches.appendleft(cache_id)
+
+        #assert cache_id == self.kv_caches[0]
+        #print(f"after free-{cache_id} of {self.type}, the left item:{self.kv_caches[0]} ", file=sys.stderr)
+        #self.kv_caches.append(cache_id)
+
+    def get_free_caches(self):
+        return len(self.kv_caches)
+
+class SwappedCPUCache:
+    def __init__(self, cache_id, blocks):
+        self.cache_id = cache_id
+        self.blocks = blocks
+
+class BlockSpaceManagerDAttn(BlockSpaceManager):
+    """Manages the mapping between logical and physical token blocks."""
+
+    def __init__(
+        self,
+        block_size: int,
+        num_gpu_blocks: int,
+        num_cpu_blocks: int,
+        watermark: float = 0.03,
+        sliding_window: Optional[int] = None, # Not supported
+        enable_caching: bool = False, # Not supported
+        vmm_frequency: int = 8, 
+        num_caches: int = 0,
+        num_cpu_caches: int = 40,
+    ) -> None:
+        self.block_size = block_size
+        self.num_total_gpu_blocks = num_gpu_blocks
+        self.num_total_cpu_blocks = num_cpu_blocks
+
+        # For every 16 steps, we will perform vmm updates by invoking update_cache_blocks
+        self.vmm_frequency = vmm_frequency
+        self.vmm_frequency_mask = vmm_frequency - 1 
+        
+        # Tracking the number of gpu_blocks (including self.cached_free_gpu_blocks) 
+        self.num_free_gpu_blocks = num_gpu_blocks
+        self.num_free_cpu_blocks = num_cpu_blocks
+
+        num_gpu_caches = num_caches
+        num_cpu_caches = num_cpu_caches 
+
+        #print(f"self.num_free_cpu_blocks-{self.num_free_cpu_blocks}, num_cpu_caches:{num_cpu_caches}", file=sys.stderr)
+        # use to alloc cache buffer id for seq
+        self.gpu_allocator = CacheAllocator("cuda", num_gpu_caches)
+        self.cpu_allocator = CacheAllocator("cpu", num_cpu_caches)
+
+        # Watermark indicates that the least amount of blocks should be free. 
+        assert watermark >= 0.0
+        self.watermark_blocks = 1
+        #int(watermark * num_gpu_blocks)
+        
+        # Mapping from cache_id to the number of allocated blocks.
+        # The information is more persitent across different steps
+        self.allocated_gpu_blocks: Dict[int, int] = {} 
+        # Pre-allocate one block for the first cache,  to support graph capture
+        self.allocated_gpu_blocks[0] = 1 
+
+        # Temporary buffer for each step. self.step() will collect these information and freed all 
+        # caches of to_free_gpu_caches
+        self.to_allocate_blocks: Dict[int, int] = {} 
+        self.to_free_blocks: Dict[int, int] = {} 
+
+        # Useful when admitting new requests or swapping in some requests. 
+        # Then we prefer those requests that just exit.
+        # Making cached_free_gpu_blocks a part of num_free_gpu_blocks
+        self.cached_free_gpu_blocks: int = 0
+
+        # to_free_gpu_caches keeps the requests that are freed in the current step
+        self.to_free_gpu_caches: Dict[int, int] = {}
+        self.immediate_allocate = False
+
+        # Maintain the mapping between seq.req_id and SwappedCPUCache (cache_id, blocks)
+        self.swapped_out_caches: Dict[int, SwappedCPUCache] = {}
+
+        # number of active requests (which will be used to improve the scheduler)
+        self.total_active_reqs = 0
+
+        # Track the step information, used for periodical memory management
+        self.step_index = 0
+    
+    def _predict_n_blocks(self, tokens: int) -> int:
+        if tokens == 0:
+            return 0
+        
+        return (tokens + self.vmm_frequency + self.block_size - 1) // self.block_size 
+
+    def _get_n_blocks(self, tokens: int) -> int:
+        return (tokens + self.block_size - 1) // self.block_size 
+
+    def _check_availability(self, need_blocks) -> AllocStatus:
+        # Ensure that one request should not use more than 90% or 99% of memory
+        # This can avoid frequent cache eviction 
+        if (self.num_total_gpu_blocks - need_blocks < self.watermark_blocks):
+            return AllocStatus.NEVER
+        
+        #if self.num_free_gpu_blocks - need_blocks >= self.watermark_blocks:
+        if self.num_free_gpu_blocks > need_blocks:
+            # Make sure that we are not holding more than schedule_config.max_num_seqs
+            if self.gpu_allocator.get_free_caches() > 0 or len(self.to_free_gpu_caches) > 0:
+                return AllocStatus.OK
+            else:
+                return AllocStatus.LATER
+        else:
+            return AllocStatus.LATER
+
+    # This function is invoked only in the prefill phase
+    def can_allocate(self, seq_group: SequenceGroup) -> AllocStatus:
+        if (self.step_index & self.vmm_frequency_mask):
+            return AllocStatus.LATER
+    
+        # FIXME(woosuk): Here we assume that all sequences in the group share
+        # the same prompt. This may not be true for preempted sequences.
+        check_no_caching_or_swa_for_blockmgr_encdec(self, seq_group)
+
+        # get_seqs will collect a list of sequence with status equalling to SequenceStatus.WAITING
+        # then we will get the first sequence in this group 
+        seq = seq_group.get_seqs(status=SequenceStatus.WAITING)[0]
+
+        self_num_required_blocks = self._predict_n_blocks(tokens=seq.get_len())
+        cross_seq = seq_group.get_encoder_seq()
+        cross_num_required_blocks = 0 
+        if cross_seq:
+            cross_num_required_blocks = self._predict_n_blocks(tokens = cross_seq.get_len())
+
+        num_required_blocks = self_num_required_blocks + \
+                              cross_num_required_blocks
+
+        return self._check_availability(num_required_blocks)
+
+
+    # This function is only invoked by _allocate_and_set_running (invoked by _schedule_prefills)
+    # Allocate a GPU cache when admitting a new request in prefill phase.
+    def allocate(self, seq_group: SequenceGroup) -> None:
+        # Allocate decoder sequences
+        #
+        # NOTE: Here we assume that all sequences in the group have the same
+        # decoder prompt.
+        seq = seq_group.get_seqs(status=SequenceStatus.WAITING)[0]
+        
+        need_blocks = self._predict_n_blocks(tokens=seq.get_len())
+
+        self.immediate_allocate = True 
+        print(f"NNOOOOOOWWW allocate sequence-{seq.seq_id} at step_index-{self.step_index}, need_blocks:{need_blocks}, tokens:{seq.get_len()}", file=sys.stderr) 
+        cache_id = self._allocate_gpu_cache(need_blocks)
+        
+        seq.cache_id = cache_id
+        seq.data.cache_id = cache_id
+
+    #  Allocate a new GPU cache, when the available GPU blocks are sufficient
+    def _allocate_gpu_cache(self, need_blocks: int) -> Tuple[int, int]:
+        cache_id = -1
+        to_allocate = True
+        
+        # update total_active_reqs and num_free_gpu_blocks
+        self.total_active_reqs +=1
+        self.num_free_gpu_blocks -= need_blocks
+
+        allocated_block_num = 0
+        # Prefer to reuse the to_free_gpu_caches at first, as some pages have been allocated already. 
+        if self.cached_free_gpu_blocks > 0:
+            # Make it block_diff a big number for the better comparison
+            block_diff = need_blocks*100
+
+            # Find one kv_cache with the smallest difference on the number of blocks
+            # The found cache can have more or less available blocks.   
+            for id, num_blocks in self.to_free_gpu_caches.items():
+                diff = abs(num_blocks - need_blocks)
+                
+                # kv_cache : cache_id, blocks 
+                if diff < block_diff:
+                    cache_id = id
+                    block_diff = diff
+
+                    allocated_block_num = num_blocks
+
+                    # No need to check anymore if we already found a perfect one
+                    if diff == 0:
+                        break 
+
+            # Remove this item from the to_free_gpu_caches
+            del self.to_free_gpu_caches[cache_id]
+            self.cached_free_gpu_blocks -= allocated_block_num
+            
+            print(f"reuse cache-{cache_id}: allocated_blocks-{allocated_block_num}, need_blocks:{need_blocks}, self.num_free_gpu_blocks:{self.num_free_gpu_blocks}", file=sys.stderr)
+        else:
+            # Check whether the can_allocate or can_swap_in has a bug
+            if self.num_free_gpu_blocks < 0: 
+                print(f"Error: self.num_free_gpu_blocks:{self.num_free_gpu_blocks}, need_blocks:{need_blocks}", file=sys.stderr)
+            assert self.num_free_gpu_blocks >= 0
+
+            cache_id = self.gpu_allocator.allocate()
+
+        self.allocated_gpu_blocks[cache_id] = need_blocks 
+
+        # We need to adjust the number of blocks for this cache id
+        # Here, we specifically differentiate to_allocate and to_free so that 
+        # we could place to_free before to_allocate in the step() function
+        if allocated_block_num < need_blocks:
+            self.to_allocate_blocks[cache_id] = need_blocks
+        elif allocated_block_num > need_blocks: 
+            self.to_free_blocks[cache_id] = need_blocks
+
+        return cache_id
+
+    # Invoked by _schedule_running in running phase.  
+    def can_append_slots(self,
+                         seq_group: SequenceGroup,
+                         num_lookahead_slots: int = 0) -> bool:
+        
+        # Only check periodically in asynchronous memory management, not each step
+        if (self.step_index & self.vmm_frequency_mask):
+            return True
+
+        # Do not evict a request that have at least 16 slots to extend (at least we could do it next step)
+        cache_blocks, tokens = self._get_blocks_tokens(seq_group)
+        if self._predict_n_blocks(tokens) <= cache_blocks:
+            return True
+
+        # Simple heuristic: at least one free block for each request.
+        # Since we will perform the actual allocation in the next epoch (16 steps), where 
+        # each request can allocate one block successfully, then there
+        # is no need to preempt. Note that self.cache_free_gpu_blocks 
+        # should be included as they will be allocated first in the next epoch 
+        return self.num_free_gpu_blocks >= 1
+        
+    # FIXME: there is no handling on num_lookahead_slots, which should be handled.  
+    def append_slots(
+        self,
+        seq: Sequence,
+        num_lookahead_slots: int = 0,
+    ) -> List[Tuple[int, int]]:
+
+        # We only need to check periodically, not each step
+        if (self.step_index & self.vmm_frequency_mask):
+            return []
+
+        """Allocate a physical token/slot for a new token."""
+        cache_id = seq.cache_id
+
+        # If the sequence is allocated, its cache_id must >= 0.
+        assert cache_id >= 0
+
+        allocated_block_num = self.allocated_gpu_blocks[cache_id]
+        logical_blocks_num = self._predict_n_blocks(seq.get_len())
+
+        # If we need to allocate a new physical block
+        if allocated_block_num < logical_blocks_num:
+            if allocated_block_num != logical_blocks_num - 1: 
+                print(f"append_slots cache_id:{cache_id}, logical_blocks_num:{logical_blocks_num} - allocated_block_num:{allocated_block_num}, real tokens:{seq.get_len()}", file=sys.stderr) 
+
+            # Currently this code only supports adding one physical block in the decoding phase
+            assert allocated_block_num == logical_blocks_num - 1
+
+            self.num_free_gpu_blocks -= 1
+            self.allocated_gpu_blocks[cache_id] = logical_blocks_num
+            self.to_allocate_blocks[cache_id] = logical_blocks_num 
+        # related to the current scheduling phase.            
+        return []
+
+    # Collect the number of physical blocks used by this sequence group 
+    def _get_blocks_tokens(
+            self, seq_group: SequenceGroup):
+        
+        cache_blocks = 0
+        tokens = 0
+        for seq in seq_group.get_seqs():
+            if seq.is_finished():
+                continue
+
+            cache_id = seq.cache_id
+            cache_blocks += self.allocated_gpu_blocks[cache_id]
+            tokens += seq.get_len()
+        
+        return cache_blocks, tokens
+
+    def fork(self, parent_seq: Sequence, child_seq: Sequence) -> None:
+        raise NotImplementedError("Forking is not supported in BlockSpaceManagerDAttn now.")
+
+    # This is to swap_in an pre-existing block, which is slightly different 
+    # from can_allocate(). 
+    def can_swap_in(self, seq_group: SequenceGroup,
+                    num_lookahead_slots: int) -> AllocStatus:
+        
+        if (self.step_index & self.vmm_frequency_mask):
+            return AllocStatus.LATER
+
+        need_blocks = num_lookahead_slots
+        req_id = None
+        for seq in seq_group.get_seqs(status=SequenceStatus.SWAPPED):
+            if seq.is_finished():
+                continue
+            
+            req_id = seq.seq_id
+            need_blocks += self.swapped_out_caches[req_id].blocks
+
+        # Make sure that the number of free blocks at least one block more
+        #need_blocks += 1
+        
+        result = self._check_availability(need_blocks) 
+
+        return result
+
+    # A fucntion is invoked to figure out the blocks that need to be allocated. 
+    def swap_in(self, seq_group: SequenceGroup) -> List[Tuple[int, int]]:
+        to_swap_in_caches = []
+
+        #print(f"SWAP IN NOW with sequence-{seq_group.request_id}, number-{seq_group.num_seqs(status=SequenceStatus.SWAPPED)} at step-{self.step_index}", file=sys.stderr)
+        for seq in seq_group.get_seqs(status=SequenceStatus.SWAPPED):
+            cpu_cache = self.swapped_out_caches[seq.seq_id] 
+
+            need_blocks = cpu_cache.blocks
+            cpu_cache_id = cpu_cache.cache_id
+
+            # Free cpu cache id and update the counter
+            self.cpu_allocator.free(cpu_cache_id)
+            self.num_free_cpu_blocks += need_blocks  
+            
+            # Allocate a gpu cache id, based on the need_blocks. 
+            # Note that we specifically request one more block in order to accomodate vmm_frequency's memory management
+            gpu_cache_id = self._allocate_gpu_cache(need_blocks + 1)
+
+            seq.cache_id = gpu_cache_id
+            seq.data.cache_id = gpu_cache_id
+
+            # NOTE: we may not need the allocation, if gpu_cache_id 
+            #print(f"SWAPIN seq_id:{seq.seq_id} with tokens:{seq.get_len()}, need_blocks:{need_blocks}, allocated_blocks:{self.allocated_gpu_blocks[gpu_cache_id]}, free_gpu_blocks:{self.num_free_gpu_blocks}", file=sys.stderr)
+            to_swap_in_caches.append([cpu_cache_id, gpu_cache_id, need_blocks])
+            
+            # Delete this entry
+            del self.swapped_out_caches[seq.seq_id]
+
+        return to_swap_in_caches
+
+    def can_swap_out(self, seq_group: SequenceGroup) -> bool:
+        cache_blocks, tokens = self._get_blocks_tokens(seq_group)
+    
+        return cache_blocks <= self.num_free_cpu_blocks
+
+    def swap_out(self, seq_group: SequenceGroup) -> List[Tuple[int, int]]:
+    
+        to_swap_out_caches = []
+
+        for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):
+            # Find the cache id and gpu_blocks        
+            gpu_cache_id = seq.cache_id
+
+            # Since this cache may have more blocks than its necessity, we only record the 
+            # real_gpu_blocks here in order to reduce the overhead involved in copy in swapping
+            need_blocks = self._get_n_blocks(seq.get_len())
+
+            #print(f"SWAPOUT request-{seq.seq_id} with blocks-{real_gpu_blocks},  free GPU blocks:{self.num_free_gpu_blocks} at step-{self.step_index}", file=sys.stderr)
+
+            # Free the cache related to gpu_cache_id
+            self._free_cache(cache_id=gpu_cache_id)
+
+            # Allocate the cpu cache id
+            cpu_cache_id = self.cpu_allocator.allocate()
+            cpu_cache = SwappedCPUCache(cpu_cache_id, need_blocks) 
+            self.swapped_out_caches[seq.seq_id] = cpu_cache
+
+            # After the swapped out, num_free_cpu_blocks should be decremented 
+            self.num_free_cpu_blocks -= need_blocks
+            
+            #print(f"SWAPOUT request-{seq.seq_id} with blocks-{real_gpu_blocks},  free GPU blocks:{self.num_free_gpu_blocks} at step-{self.step_index}", file=sys.stderr)
+            
+            to_swap_out_caches.append([gpu_cache_id, cpu_cache_id, need_blocks]) 
+
+        #print(f"to_swap_out_caches:{to_swap_out_caches}", file=sys.stderr)
+        return to_swap_out_caches
+
+    def _free_cache(self, cache_id: int) -> None:
+        # Check whether cache_id is in the list
+        if cache_id in self.to_free_gpu_caches:
+            # Already freed yet, no need to do anything.
+            return
+
+        # Get blocks of this cache
+        free_blocks = self.allocated_gpu_blocks[cache_id]
+        #    print(f"FREE gpu cache_id:{cache_id}, free_blocks:{free_blocks}, step:{self.step_index}", file=sys.stderr)
+       
+        # Note that we update self.total_active_reqs here, as free_cache() is invoked twice for every request
+        self.total_active_reqs -=1
+        self.allocated_gpu_blocks[cache_id] = 0
+
+        self.to_free_gpu_caches[cache_id] = free_blocks
+        self.cached_free_gpu_blocks += free_blocks
+        self.num_free_gpu_blocks += free_blocks
+
+    """
+    Free a sequence. We will append the seq to to_free_gpu_caches. 
+    Initially, we did this inside the memory management library. Maybe we should do it here as well. 
+    """
+    def free(self, seq: Sequence) -> None:
+        #print(f"free sequence:{seq.seq_id}, cache_id:{seq.cache_id}, data_cache_id:{seq.data.cache_id}", file=sys.stderr)
+        self._free_cache(cache_id=seq.cache_id)
+        
+        #print(f"After free, self.total_active_reqs: {self.total_active_reqs}", file=sys.stderr)
+
+    def reset(self) -> None:
+        # Free decoder block tables
+        self.allocated_gpu_blocks.clear()
+        self.num_free_gpu_blocks = self.num_total_gpu_blocks
+        self.num_free_cpu_blocks = self.num_total_cpu_blocks
+        
+        self.to_free_gpu_caches = {}
+        self.to_allocate_blocks = {}
+
+    # A dummy function that will be never invoked
+    def get_block_table(self, seq: Sequence) -> List[int]:
+        # logger.warning("block table is not used in BlockSpaceManagerDAttn now.")
+        return []
+
+    def get_num_free_gpu_blocks(self) -> int:
+        return self.num_free_gpu_blocks
+
+    def get_num_free_cpu_blocks(self) -> int:
+        return self.num_free_cpu_blocks
+
+    def access_all_blocks_in_seq(
+        self,
+        seq: Sequence,
+        access_time: float,
+    ) -> None:
+        # logger.warning("Access all blocks in seq is not supported in BlockSpaceManagerDAttn now.")
+        pass
+
+    def get_common_computed_block_ids(self,
+                                      seq_group: SequenceGroup) -> List[int]:
+        # logger.warning("Common computed block ids is not supported in BlockSpaceManagerDAttn now.")
+        return None  # type: ignore
+
+    def mark_blocks_as_computed(self, seq_group: SequenceGroup, token_chunk_size: int) -> None:
+        # logger.warning("Mark blocks as computed is not supported in BlockSpaceManagerDAttn now.")
+        pass
+
+    # In the end of each step's scheduling, this function is invoked to 
+    # collect the information of allocation and deallocation  
+    def step(self) -> Tuple[Dict[int, int], List[int], bool]:
+        to_update_blocks = {}
+
+        immediate_allocate = self.immediate_allocate
+        self.immediate_allocate = False
+
+        #print(f"in the end step-{self.step_index} with requests:{self.total_active_reqs}, allocate_blocks:{len(self.to_allocate_blocks)} now!", file=sys.stderr) 
+        # We will perform virtual memory management once for every self.vmm_frequency 
+        if ((self.step_index & self.vmm_frequency_mask) != 0) and (immediate_allocate != True):
+            # No need to invoke virtual memory management
+            #print(f"step-{self.step_index} no need to do memory management", file=sys.stderr) 
+            self.step_index += 1
+            return to_update_blocks, immediate_allocate
+
+        # In the following, we place to_free_blocks in the header of to_update_blocks, which 
+        # ensures that allocation can be performed without any issue. 
+    
+        to_free_blocks = 0
+        # First, place all to_free_gpu_caches at first. 
+        for cache_id, num_blocks in self.to_free_gpu_caches.items():
+            #print(f"step-{self.step_index} free cache_id:{cache_id}, num_blocks:{num_blocks}", file=sys.stderr)
+            # Freeing all blocks of this cache
+            to_update_blocks[cache_id] = 0
+            self.gpu_allocator.free(cache_id)
+            to_free_blocks += num_blocks 
+
+        # Second, place all to_free_blocks (caused by reusing a freed cache)
+        for cache_id, num_blocks in self.to_free_blocks.items():
+            #print(f"step-{self.step_index} tofree cache_id:{cache_id}, num_blocks:{num_blocks}", file=sys.stderr)
+            to_update_blocks[cache_id] = num_blocks
+
+        # Third, place the caches that need to increase their blocks
+        for cache_id, num_blocks in self.to_allocate_blocks.items():
+            #print(f"step-{self.step_index} toallocate cache_id:{cache_id}, num_blocks:{num_blocks}", file=sys.stderr)
+            to_update_blocks[cache_id] = num_blocks
+
+        
+
+        #if len(to_allocate_blocks) > 0 or len(to_free_gpu_caches) > 0:         
+        #if self.step_index >= 1600:
+        #print(f"step-{self.step_index}, to_allocate_blocks:{len(to_update_blocks)}, freeing ({to_free_blocks} blocks), self.num_free_gpu_blocks:{self.num_free_gpu_blocks}, requests:{self.total_active_reqs}, swapped requests:{len(self.swapped_out_caches)}", file=sys.stderr)
+
+        # step() is invoked once after _schedule() inside Scheduler::schedule(). It is invoked once for every decode or prefill
+        self.to_free_gpu_caches.clear()
+        self.to_free_blocks.clear()
+        self.to_allocate_blocks.clear()
+        self.cached_free_gpu_blocks = 0
+
+        # Only update the step index for decoding steps
+        if immediate_allocate == False:
+            self.step_index += 1  
+
+        return to_update_blocks, immediate_allocate
+
+    def get_prefix_cache_hit_rate(self, device: Device) -> float:
+        return 0
\ No newline at end of file
diff -Nur orig/vllm/core/block_manager_v1.py dattn/vllm/core/block_manager_v1.py
--- orig/vllm/core/block_manager_v1.py	2024-12-30 22:26:31.416992721 +0000
+++ dattn/vllm/core/block_manager_v1.py	2024-12-30 22:27:03.189886848 +0000
@@ -277,6 +277,8 @@
         # Note that each SequenceGroup has a unique
         # request ID
         self.cross_block_tables: Dict[str, BlockTable] = {}
+        self.num_swap_out = 0
+        self.num_free = 0
 
     def _get_seq_num_required_blocks(self, seq: Optional[Sequence]) -> int:
         return 0 if seq is None else seq.n_blocks
@@ -529,18 +531,25 @@
         assert (num_lookahead_slots == 0
                 ), "BlockSpaceManagerV1 does not support lookahead allocation"
 
+
         blocks = self._get_physical_blocks(seq_group)
         num_swapped_seqs = seq_group.num_seqs(status=SequenceStatus.SWAPPED)
         if seq_group.is_encoder_decoder():
             num_swapped_seqs += 1
+        
+
         num_free_blocks = self.gpu_allocator.get_num_free_blocks()
+        
         # NOTE: Conservatively, we assume that every sequence will allocate
         # at least one free block right after the swap-in.
         # NOTE: This should match the logic in can_append_slot().
         num_required_blocks = len(blocks) + num_swapped_seqs
+
         if self.gpu_allocator.get_num_total_blocks() < num_required_blocks:
             return AllocStatus.NEVER
         elif num_free_blocks - num_required_blocks >= self.watermark_blocks:
+        #elif num_free_blocks > num_required_blocks:
+            #print(f"seq-{seq_group.request_id}, blocks:{len(blocks)}, num_free_blocks:{num_free_blocks}, num_swapped_seqs:{num_swapped_seqs}, num_required_blocks:{num_required_blocks}, self.watermark_blocks:{self.watermark_blocks}")
             return AllocStatus.OK
         else:
             return AllocStatus.LATER
@@ -596,6 +605,7 @@
     def swap_out(self, seq_group: SequenceGroup) -> List[Tuple[int, int]]:
         request_id = seq_group.request_id
 
+        self.num_swap_out += 1
         # GPU block -> CPU block.
         # dict is efficient in lookup `if gpu_block in mapping`
         mapping: Dict[PhysicalTokenBlock, PhysicalTokenBlock] = {}
@@ -634,6 +644,11 @@
         if seq.seq_id not in self.block_tables:
             # Already freed or haven't been scheduled yet.
             return
+
+        #print(f"Free seq-{seq.seq_id}")
+        if self.num_swap_out > 0:
+            self.num_free += 1
+            
         block_table = self.block_tables[seq.seq_id]
         self._free_block_table(block_table)
         del self.block_tables[seq.seq_id]
diff -Nur orig/vllm/core/interfaces.py dattn/vllm/core/interfaces.py
--- orig/vllm/core/interfaces.py	2024-12-30 22:26:31.416992721 +0000
+++ dattn/vllm/core/interfaces.py	2024-12-30 22:27:03.188886852 +0000
@@ -1,3 +1,4 @@
+
 import enum
 from abc import ABC, abstractmethod
 from typing import List
@@ -41,6 +42,11 @@
                 EmbeddingModelBlockSpaceManager)
             return EmbeddingModelBlockSpaceManager
 
+        if version == "dattn":        # new add for dattn
+            from vllm.core.block_manager_dattn import BlockSpaceManagerDAttn
+            return BlockSpaceManagerDAttn
+
+        
         raise ValueError(f"Unknown version {version=}")
 
     @abstractmethod
diff -Nur orig/vllm/core/scheduler.py dattn/vllm/core/scheduler.py
--- orig/vllm/core/scheduler.py	2024-12-30 22:26:31.416992721 +0000
+++ dattn/vllm/core/scheduler.py	2024-12-30 22:27:03.189886848 +0000
@@ -1,4 +1,5 @@
 import enum
+import sys
 import os
 import random
 import time
@@ -64,6 +65,8 @@
         return (self.num_batched_tokens + num_new_tokens <= self.token_budget
                 and self.num_curr_seqs + num_new_seqs <= self.max_num_seqs)
 
+        #print(f"can_schedule: self.num_batched_tokens-{self.num_batched_tokens}, num_new_tokens:{num_new_tokens}, self.token_budget:{self.token_budget},self.max_num_seqs:{self.max_num_seqs},self.num_curr_seqs:{self.num_curr_seqs}, num_new_seqs:{num_new_seqs} ")
+
     def remaining_token_budget(self):
         return self.token_budget - self.num_batched_tokens
 
@@ -111,6 +114,7 @@
     token_chunk_size: int
 
 
+
 @dataclass
 class SchedulerOutputs:
     """The scheduling decision made from a scheduler."""
@@ -134,9 +138,14 @@
     running_queue_size: int
     preempted: int
 
+    # dattn support
+    to_update_blocks: Dict[int, int] = field(default_factory=dict)
+    immediate_allocate: bool = False
+
     def __post_init__(self):
         # Swap in and swap out should never happen at the same time.
-        assert not (self.blocks_to_swap_in and self.blocks_to_swap_out)
+        # dAttn: disabling this check, as it can occur in dAttention's swapping support
+        # assert not (self.blocks_to_swap_in and self.blocks_to_swap_out)
 
         self.num_loras: int = len(self.lora_requests)
         if self.num_loras > 0:
@@ -311,11 +320,20 @@
         # LoRAs. This should be improved in the future.
         self.lora_config = lora_config
 
+        # The following are added for dAttention support
+        self.use_dattn = cache_config.use_dattn
+        # Note make sure that vmm_frequency is the same as that in block_manager_dattn.py
+        self.vmm_frequency = cache_config.vmm_frequency
+        self.vmm_frequency_mask = cache_config.vmm_frequency - 1
+        self.step_index = 0
+
         version = "v1"
         if self.scheduler_config.use_v2_block_manager:
             version = "v2"
         if self.scheduler_config.embedding_mode:
             version = "embedding"
+        if self.use_dattn:
+            version = "dattn"
 
         BlockSpaceManagerImpl = BlockSpaceManager.get_block_space_manager_class(
             version)
@@ -329,13 +347,24 @@
             num_cpu_blocks //= pipeline_parallel_size
 
         # Create the block space manager.
-        self.block_manager = BlockSpaceManagerImpl(
-            block_size=self.cache_config.block_size,
-            num_gpu_blocks=num_gpu_blocks,
-            num_cpu_blocks=num_cpu_blocks,
-            sliding_window=self.cache_config.sliding_window,
-            enable_caching=self.cache_config.enable_prefix_caching)
-
+        if not cache_config.use_dattn:
+            self.block_manager = BlockSpaceManagerImpl(
+                block_size=self.cache_config.block_size,
+                num_gpu_blocks=num_gpu_blocks,
+                num_cpu_blocks=num_cpu_blocks,
+                sliding_window=self.cache_config.sliding_window,
+                enable_caching=self.cache_config.enable_prefix_caching)
+        else: # use_dattn
+            self.block_manager = BlockSpaceManagerImpl(
+                block_size=self.cache_config.block_size,
+                num_gpu_blocks=num_gpu_blocks,
+                num_cpu_blocks=num_cpu_blocks,
+                sliding_window=self.cache_config.sliding_window,
+                enable_caching=self.cache_config.enable_prefix_caching,
+                num_caches=self.scheduler_config.max_num_seqs,
+                num_cpu_caches=self.cache_config.num_cpu_caches,  
+                vmm_frequency = self.vmm_frequency, 
+                )
         # Sequence groups in the WAITING state.
         # Contain new prefill or preempted requests.
         self.waiting: Deque[SequenceGroup] = deque()
@@ -345,6 +374,14 @@
         # Sequence groups in the SWAPPED state.
         # Contain decode requests that are swapped out.
         self.swapped: Deque[SequenceGroup] = deque()
+
+        # Two queues used for asynchronous swapping of dAttention
+        # Sequence groups in swapping_in
+        self.swapping_in: Deque[SequenceGroup] = deque()
+
+        # Sequence groups in swapping_out 
+        self.swapping_out: Deque[SequenceGroup] = deque()
+
         # Sequence groups finished requests ids since last step iteration.
         # It lets the model know that any state associated with these requests
         # can and must be released after the current step.
@@ -359,6 +396,7 @@
         # preemption mode, RECOMPUTE or SWAP
         self.user_specified_preemption_mode = scheduler_config.preemption_mode
 
+        print(f"self.user_specified_preemption_mode:{self.user_specified_preemption_mode}")
         # The following field is test-only. It is used to inject artificial
         # preemption.
         self.enable_artificial_preemption = ENABLE_ARTIFICIAL_PREEMPT
@@ -458,6 +496,7 @@
                     if seq.is_finished():
                         continue
                     seq.status = SequenceStatus.FINISHED_ABORTED
+                    print(f"aborted seq:{seq.seq_id}", file=sys.stderr)
                     self.free_seq(seq)
 
                 self._free_seq_group_cross_attn_blocks(aborted_group)
@@ -474,8 +513,15 @@
             self.block_manager.free_cross(seq_group)
 
     def has_unfinished_seqs(self) -> bool:
-        return len(self.waiting) != 0 or len(self.running) != 0 or len(
-            self.swapped) != 0
+        ret = len(self.waiting) != 0 or len(self.running) != 0 or len(
+            self.swapped) != 0 or len(self.swapping_in) != 0
+
+        if ret == False:
+            print(f"len(self.waiting):{len(self.waiting)}, self.swapped:{len(self.swapped)}, self.swapping:{len(self.swapping_in)}, self.swapping_out:{len(self.swapping_out)} at step-{self.step_index}")
+        return ret
+
+    def has_active_seqs(self) -> bool:
+        return len(self.running) != 0
 
     def get_prefix_cache_hit_rate(self, device: Device) -> float:
         return self.block_manager.get_prefix_cache_hit_rate(device)
@@ -539,6 +585,7 @@
 
         running_queue = self.running
         assert len(self._async_stopped) == 0
+
         while running_queue:
             seq_group = running_queue[0]
             num_running_tokens = self._get_num_new_tokens(
@@ -606,7 +653,10 @@
                     if preempted_mode == PreemptionMode.RECOMPUTE:
                         preempted.append(victim_seq_group)
                     else:
-                        swapped_out.append(victim_seq_group)
+                        if self.use_dattn == True:
+                            self.swapping_out.append(victim_seq_group)
+                        else:
+                            swapped_out.append(victim_seq_group)
 
                 if not cont_loop:
                     break
@@ -638,6 +688,8 @@
                 if curr_loras is not None and seq_group.lora_int_id > 0:
                     curr_loras.add(seq_group.lora_int_id)
 
+        #if len(self.running) == 0 and len(decode_seq_groups) == 0 and len(prefill_seq_groups) == 0:
+        #print(f"NOOOOOOOO, len(self.running):{len(self.running)}, len(decode_seq_groups):{len(decode_seq_groups)}, len(prefill_seq_groups):{len(prefill_seq_groups)} at step-{self.step_index}", file=sys.stderr)
         self._scheduler_running_outputs_cache[self.next_cache_id].reset()
         self._scheduled_seq_group_cache[self.next_cache_id].reset()
 
@@ -751,6 +803,135 @@
             infeasible_seq_groups=infeasible_seq_groups,
         )
 
+    # A new scheduling for dattn, where the swapped-in requests cannot be 
+    # inserted into the running queue directly, as the memory is not yet prepared
+    # Instead, the next epoch can be 
+    def _schedule_swapped_async(
+        self,
+        budget: SchedulingBudget,
+        curr_loras: Optional[Set[int]],
+        enable_chunking: bool = False,
+    ) -> SchedulerSwappedInOutputs:
+        """Schedule sequence groups that are swapped out. """
+        # Blocks that need to be swapped or copied before model execution.
+        blocks_to_swap_in: List[Tuple[int, int]] = []
+        blocks_to_copy: List[Tuple[int, int]] = []
+        decode_seq_groups: List[ScheduledSequenceGroup] = []
+        prefill_seq_groups: List[ScheduledSequenceGroup] = []
+        infeasible_seq_groups: List[SequenceGroup] = []
+        
+        # TODO: make sure that the time has passed a period (e.g. 16 steps)
+        #if (self.step_index & self.vmm_frequency_mask) and len(self.running) != 0:
+        if self.step_index & self.vmm_frequency_mask:
+            #print(f"self.step_index:{self.step_index}, self.vmm_frequency_mask:{self.vmm_frequency_mask}")
+            return SchedulerSwappedInOutputs(
+                decode_seq_groups=decode_seq_groups,
+                prefill_seq_groups=prefill_seq_groups,
+                blocks_to_swap_in=blocks_to_swap_in,
+                blocks_to_copy=blocks_to_copy,
+                num_lookahead_slots=self._get_num_lookahead_slots(is_prefill=False),
+                infeasible_seq_groups=infeasible_seq_groups,
+            )
+
+        # For all requests in the swapping_in queue, adding to the RUNNING queue. 
+        #to_check = False
+        for seq_group in list(self.swapping_in):
+            # No need to do anything for the group that is just swapping in
+            if self.step_index == seq_group.swapping_step_index:
+                continue
+            
+            #to_check = True
+            #print(f"NNNNNNNNNNN_schedule_running, swapping_in {seq_group.swapping_step_index},  adding seq_group-{seq_group.request_id} to self.running at step-{self.step_index}", file=sys.stderr)
+            for seq in seq_group.get_seqs(status=SequenceStatus.SWAPPING):
+                seq.status = SequenceStatus.RUNNING
+
+            is_prefill = seq_group.is_prefill()
+            if is_prefill:
+                prefill_seq_groups.append(
+                    ScheduledSequenceGroup(seq_group,
+                                           token_chunk_size=num_new_tokens))
+            else:
+                decode_seq_groups.append(
+                    ScheduledSequenceGroup(seq_group, token_chunk_size=1))
+
+            # Remove the current sequence group in self.swapping_in
+            self.swapping_in.popleft()
+
+        #if to_check:
+        #    print(f"swapping_in:{len(self.swapping_in)} decode_seq_groups:{len(decode_seq_groups)} at step-{self.step_index}", file=sys.stderr)
+
+        # Check all requests in the swapped queue, check whether it is necessary to 
+        # to swap in. 
+        swapped_queue = self.swapped
+        leftover_swapped: Deque[SequenceGroup] = deque()
+        while swapped_queue:
+            # NOTE: the swapping order is first-in-last-out
+            seq_group = swapped_queue[0]
+
+            is_prefill = seq_group.is_prefill()
+
+            # If the sequence group cannot be swapped in, stop.
+            alloc_status = self.block_manager.can_swap_in(
+                seq_group, self._get_num_lookahead_slots(is_prefill))
+            if alloc_status == AllocStatus.LATER:
+                break
+            elif alloc_status == AllocStatus.NEVER:
+                logger.warning(
+                    "Failing the request %s because there's not enough kv "
+                    "cache blocks to run the entire sequence.",
+                    seq_group.request_id)
+                for seq in seq_group.get_seqs():
+                    seq.status = SequenceStatus.FINISHED_IGNORED
+                infeasible_seq_groups.append(seq_group)
+                swapped_queue.popleft()
+                continue
+
+            
+            # Now alloc_status == AllocStatus.OK
+            # The total number of sequences in the RUNNING state should not
+            # exceed the maximum number of sequences.
+            num_new_seqs = seq_group.get_max_num_running_seqs()
+            num_new_tokens = self._get_num_new_tokens(seq_group,
+                                                      SequenceStatus.SWAPPED,
+                                                      enable_chunking, budget)
+
+            if (num_new_tokens == 0
+                    or not budget.can_schedule(num_new_tokens=num_new_tokens,
+                                               num_new_seqs=num_new_seqs)):
+                break
+
+            swapped_queue.popleft()
+
+            # We will invoke the asynchronous swapping_in
+            self._swap_in_async(seq_group, blocks_to_swap_in)
+
+            budget.add_num_batched_tokens(seq_group.request_id, num_new_tokens)
+            budget.add_num_seqs(seq_group.request_id, num_new_seqs)
+
+        # Now for all requests in the swapping_out queue, adding to the swapped queue. 
+        # Also, changed the status to swapped_out
+        for seq_group in list(self.swapping_out):
+            # No need to do anything for the group that is just swapping out
+            if self.step_index == seq_group.swapping_step_index:
+                continue
+
+            #print(f"NOOOOOOOO_schedule_running, swapping_out:{seq_group.swapping_step_index},  adding seq_group to self.swapped at step-{self.step_index}", file=sys.stderr)
+            for seq in seq_group.get_seqs(status=SequenceStatus.SWAPPING):
+                seq.status = SequenceStatus.SWAPPED
+            self.swapped.append(seq_group)
+
+            # Remove the current sequence group in self.swapping_in
+            self.swapping_out.popleft()
+
+        return SchedulerSwappedInOutputs(
+            decode_seq_groups=decode_seq_groups,
+            prefill_seq_groups=prefill_seq_groups,
+            blocks_to_swap_in=blocks_to_swap_in,
+            blocks_to_copy=blocks_to_copy,
+            num_lookahead_slots=self._get_num_lookahead_slots(is_prefill=False),
+            infeasible_seq_groups=infeasible_seq_groups,
+        )
+
     def _get_prompt_limit(self, seq_group: SequenceGroup) -> int:
         if self.scheduler_config.chunked_prefill_enabled:
             prompt_limit = self.scheduler_config.max_model_len
@@ -795,6 +976,9 @@
         Returns:
             SchedulerPrefillOutputs.
         """
+        #print(f"inside _schedule_prefills now", file=sys.stderr)
+        #self.prefillcount +=1
+
         ignored_seq_groups: List[SequenceGroup] = []
         seq_groups: List[ScheduledSequenceGroup] = []
 
@@ -826,8 +1010,10 @@
                 waiting_queue.popleft()
                 continue
 
+
             # If the sequence group cannot be allocated, stop.
             can_allocate = self.block_manager.can_allocate(seq_group)
+            #print(f"_schedule_prefills, can_allocate:{can_allocate}") 
             if can_allocate == AllocStatus.LATER:
                 break
             elif can_allocate == AllocStatus.NEVER:
@@ -924,13 +1110,17 @@
             running_scheduled = self._schedule_running(budget,
                                                        curr_loras,
                                                        enable_chunking=False)
-
             # If any sequence group is preempted, do not swap in any sequence
             # group. because it means there's no slot for new running requests.
-            if len(running_scheduled.preempted) + len(
+            if self.use_dattn and self.user_specified_preemption_mode == "swap":
+                swapped_in = self._schedule_swapped_async(budget, curr_loras)
+                #if len(swapped_in.decode_seq_groups) > 0:
+                #    print(f"schedule_async, with len(swapped_in.decode_seq_groups)-{len(swapped_in.decode_seq_groups)} at step-{self.step_index}", file=sys.stderr) 
+            elif len(running_scheduled.preempted) + len(
                     running_scheduled.swapped_out) == 0:
                 swapped_in = self._schedule_swapped(budget, curr_loras)
 
+
         assert (budget.num_batched_tokens <=
                 self.scheduler_config.max_num_batched_tokens)
         assert budget.num_curr_seqs <= self.scheduler_config.max_num_seqs
@@ -972,6 +1162,8 @@
         ignored_seq_groups = prefills.ignored_seq_groups
         ignored_seq_groups.extend(swapped_in.infeasible_seq_groups)
 
+        #if swapped_in.blocks_to_swap_in or running_scheduled.blocks_to_swap_out:
+        #    print(f"step-{self.step_index} has swapping blocks", file=sys.stderr) 
         return SchedulerOutputs(
             scheduled_seq_groups=scheduled_seq_groups,
             num_prefill_groups=num_prefill_groups,
@@ -1017,7 +1209,11 @@
         # If preemption happens, it means we don't have space for swap-in.
         if len(running_scheduled.preempted) + len(
                 running_scheduled.swapped_out) == 0:
-            swapped_in = self._schedule_swapped(budget, curr_loras)
+            if self.use_dattn and self.user_specified_preemption_mode == "swap":
+                swapped_in = self._schedule_swapped_async(budget, curr_loras)
+            else:
+                swapped_in = self._schedule_swapped(budget, curr_loras)
+
 
         # Schedule new prefills.
         prefills = self._schedule_prefills(budget,
@@ -1072,7 +1268,10 @@
     def _schedule(self) -> SchedulerOutputs:
         """Schedule queued requests."""
         if self.scheduler_config.chunked_prefill_enabled:
-            return self._schedule_chunked_prefill()
+            if self.use_dattn:
+                raise NotImplementedError("Chunked prefill is not supported with DATTN yet.")
+            else:
+                return self._schedule_chunked_prefill()
         else:
             return self._schedule_default()
 
@@ -1111,6 +1310,16 @@
 
         scheduler_outputs = self._schedule()
         now = time.time()
+        
+        is_prefill = False
+        if self.use_dattn:
+            # Collect the information related to cache update for dattn
+            scheduler_outputs.to_update_blocks, scheduler_outputs.immediate_allocate = self.block_manager.step()
+
+            # When there is no active requests, we will need to change immediate_allocate to be True
+            if self.has_active_seqs() == False:
+                #print(f"in scheduling step-{self.step_index}, FORCE to use immediate_allocate == TRUE", file=sys.stderr)
+                scheduler_outputs.immediate_allocate = True
 
         if not self.cache_config.enable_prefix_caching:
             common_computed_block_nums = []
@@ -1151,9 +1360,11 @@
             for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):
                 seq_id = seq.seq_id
                 seq_data[seq_id] = seq.data
-                block_tables[seq_id] = self.block_manager.get_block_table(seq)
-                self.block_manager.access_all_blocks_in_seq(seq, now)
 
+                if not self.use_dattn:
+                    block_tables[seq_id] = self.block_manager.get_block_table(seq)
+                    self.block_manager.access_all_blocks_in_seq(seq, now)
+                
             if self.cache_config.enable_prefix_caching:
                 common_computed_block_nums = (
                     self.block_manager.get_common_computed_block_ids(
@@ -1165,6 +1376,7 @@
             # is sent. Subsequent requests could be chunked prefill or decode.
             is_first_prefill = False
             if is_prompt:
+                is_prefill = True
                 seqs = seq_group.get_seqs()
                 # Prefill has only 1 sequence.
                 assert len(seqs) == 1
@@ -1250,6 +1462,10 @@
         # Move to next cache (if exists)
         self.cache_id = self.next_cache_id
 
+        # Update self.step_index for dAttention support
+        if is_prefill != True:
+            self.step_index += 1
+
         # Return results
         return (seq_group_metadata_list, scheduler_outputs,
                 allow_async_output_proc)
@@ -1259,12 +1475,14 @@
 
     def free_seq(self, seq: Sequence) -> None:
         """Free a sequence from a block table."""
+        #print(f"free_seq: seq:{seq.seq_id} at step-{self.step_index}", file=sys.stderr)
         self.block_manager.free(seq)
 
     def _free_finished_seqs(self, seq_group: SequenceGroup) -> None:
         """Free finished seqs in a sequence group."""
         for seq in seq_group.get_seqs():
             if seq.is_finished():
+                #print(f"_free_finished_seqs, seq->id:{seq.seq_id}", file=sys.stderr)
                 self.free_seq(seq)
 
     def _free_finished_seq_group(self, seq_group: SequenceGroup) -> None:
@@ -1372,6 +1590,7 @@
             self._preempt_by_recompute(seq_group)
         elif preemption_mode == PreemptionMode.SWAP:
             self._preempt_by_swap(seq_group, blocks_to_swap_out)
+            #print(f"_preempt_by_swap, request:{seq_group.request_id}, blocks_to_swap_out:{len(blocks_to_swap_out)}")
         else:
             raise AssertionError("Invalid preemption mode.")
         return preemption_mode
@@ -1392,7 +1611,10 @@
         seq_group: SequenceGroup,
         blocks_to_swap_out: List[Tuple[int, int]],
     ) -> None:
-        self._swap_out(seq_group, blocks_to_swap_out)
+        if self.use_dattn:
+            self._swap_out_async(seq_group, blocks_to_swap_out)
+        else:
+            self._swap_out(seq_group, blocks_to_swap_out)
 
     def _swap_in(
         self,
@@ -1403,6 +1625,20 @@
         blocks_to_swap_in.extend(mapping)
         for seq in seq_group.get_seqs(status=SequenceStatus.SWAPPED):
             seq.status = SequenceStatus.RUNNING
+            print(f"swap_in:{seq.seq_id} blocks:{int(seq.get_len()/16)} step:{self.step_index}", file=sys.stderr)
+
+    def _swap_in_async(
+        self,
+        seq_group: SequenceGroup,
+        blocks_to_swap_in: List[Tuple[int, int]],
+    ) -> None:
+        mapping = self.block_manager.swap_in(seq_group)
+        blocks_to_swap_in.extend(mapping)
+        for seq in seq_group.get_seqs(status=SequenceStatus.SWAPPED):
+            seq.status = SequenceStatus.SWAPPING
+            print(f"swap_in:{seq.seq_id} blocks:{int(seq.get_len()/16)} step:{self.step_index}", file=sys.stderr)
+        self.swapping_in.append(seq_group)
+        seq_group.swapping_step_index = self.step_index
 
     def _swap_out(
         self,
@@ -1419,6 +1655,26 @@
         blocks_to_swap_out.extend(mapping)
         for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):
             seq.status = SequenceStatus.SWAPPED
+            print(f"swap_out:{seq.seq_id} blocks:{int(seq.get_len()/16)} step:{self.step_index}", file=sys.stderr)
+
+    def _swap_out_async(
+        self,
+        seq_group: SequenceGroup,
+        blocks_to_swap_out: List[Tuple[int, int]],
+    ) -> None:
+        if not self.block_manager.can_swap_out(seq_group):
+            # FIXME(woosuk): Abort the sequence group instead of aborting the
+            # entire engine.
+            raise RuntimeError(
+                "Aborted due to the lack of CPU swap space. Please increase "
+                "the swap space to avoid this error.")
+        mapping = self.block_manager.swap_out(seq_group)
+        blocks_to_swap_out.extend(mapping)
+        for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):
+            seq.status = SequenceStatus.SWAPPING
+            print(f"swap_out:{seq.seq_id} blocks:{int(seq.get_len()/16)} step:{self.step_index}", file=sys.stderr)
+
+        seq_group.swapping_step_index = self.step_index
 
     def _passed_delay(self, now: float) -> bool:
         if self.prev_prompt:
diff -Nur orig/vllm/distributed/parallel_state.py dattn/vllm/distributed/parallel_state.py
--- orig/vllm/distributed/parallel_state.py	2024-12-30 22:26:31.417992716 +0000
+++ dattn/vllm/distributed/parallel_state.py	2024-12-30 22:27:03.190886843 +0000
@@ -47,7 +47,8 @@
 
 
 def _split_tensor_dict(
-    tensor_dict: Dict[str, Union[torch.Tensor, Any]]
+    tensor_dict: Dict[str, Union[torch.Tensor, Any]],
+    prefix: str = ""
 ) -> Tuple[List[Tuple[str, Any]], List[torch.Tensor]]:
     """Split the tensor dictionary into two parts:
     1. A list of (key, value) pairs. If the value is a tensor, it is replaced
@@ -66,6 +67,13 @@
             metadata_list.append(
                 (key, TensorMetadata(device, value.dtype, value.size())))
             tensor_list.append(value)
+        elif isinstance(value, dict):
+            if len(value) == 0:
+                metadata_list.append((prefix + key, value))
+            inner_metadata_list, inner_tensor_list = _split_tensor_dict(
+                    value, prefix + key + "%")
+            metadata_list.extend(inner_metadata_list)
+            tensor_list.extend(inner_tensor_list)
         else:
             metadata_list.append((key, value))
     return metadata_list, tensor_list
diff -Nur orig/vllm/engine/arg_utils.py dattn/vllm/engine/arg_utils.py
--- orig/vllm/engine/arg_utils.py	2024-12-30 22:26:31.417992716 +0000
+++ dattn/vllm/engine/arg_utils.py	2024-12-30 22:27:03.536885177 +0000
@@ -17,11 +17,11 @@
 from vllm.logger import init_logger
 from vllm.model_executor.layers.quantization import QUANTIZATION_METHODS
 from vllm.transformers_utils.utils import check_gguf_file
-from vllm.utils import FlexibleArgumentParser
-
+from vllm.utils import FlexibleArgumentParser, STR_DTYPE_TO_TORCH_DTYPE, get_dtype_size
 if TYPE_CHECKING:
     from vllm.transformers_utils.tokenizer_group import BaseTokenizerGroup
 
+_MB = 1 << 20
 logger = init_logger(__name__)
 
 ALLOWED_DETAILED_TRACE_MODULES = ["model", "worker", "all"]
@@ -105,10 +105,14 @@
     tensor_parallel_size: int = 1
     max_parallel_loading_workers: Optional[int] = None
     block_size: int = 16
+
+    # new add for dattn
+    use_dattn: bool = False
+
     enable_prefix_caching: bool = False
     disable_sliding_window: bool = False
     use_v2_block_manager: bool = False
-    swap_space: float = 4  # GiB
+    swap_space: float = 256  # GiB
     cpu_offload_gb: float = 0  # GiB
     gpu_memory_utilization: float = 0.90
     max_num_batched_tokens: Optional[int] = None
@@ -867,6 +871,14 @@
                     "supported for multimodal models and has been disabled.")
             self.enable_prefix_caching = False
 
+        if self.use_dattn:
+            logger.info(f"DATTN: use block size: {self.block_size}, preemption_mode:{self.preemption_mode}")
+            # TODO: support swap preemption mode for dattn
+            #self.preemption_mode = "recompute"
+            #logger.warning("Preemption only support recompute for dattn now.")
+        else:
+            logger.info(f"use normal block size: {self.block_size}")
+
         cache_config = CacheConfig(
             block_size=self.block_size if self.device != "neuron" else
             self.max_model_len,  # neuron needs block_size = max_model_len
@@ -877,7 +889,9 @@
             sliding_window=model_config.get_sliding_window(),
             enable_prefix_caching=self.enable_prefix_caching,
             cpu_offload_gb=self.cpu_offload_gb,
+            use_dattn=self.use_dattn,  
         )
+        
         parallel_config = ParallelConfig(
             pipeline_parallel_size=self.pipeline_parallel_size,
             tensor_parallel_size=self.tensor_parallel_size,
@@ -892,6 +906,7 @@
             ray_workers_use_nsight=self.ray_workers_use_nsight,
             distributed_executor_backend=self.distributed_executor_backend)
 
+        
         max_model_len = model_config.max_model_len
         use_long_context = max_model_len > 32768
         if self.enable_chunked_prefill is None:
diff -Nur orig/vllm/engine/llm_engine.py dattn/vllm/engine/llm_engine.py
--- orig/vllm/engine/llm_engine.py	2024-12-30 22:26:31.417992716 +0000
+++ dattn/vllm/engine/llm_engine.py	2024-12-30 22:27:03.537885172 +0000
@@ -53,6 +53,8 @@
                                   usage_message)
 from vllm.utils import Counter, Device, weak_bind
 from vllm.version import __version__ as VLLM_VERSION
+import os
+import sys
 
 logger = init_logger(__name__)
 _LOCAL_LOGGING_INTERVAL_SEC = 5
@@ -219,6 +221,9 @@
         usage_context: UsageContext = UsageContext.ENGINE_CONTEXT,
         stat_loggers: Optional[Dict[str, StatLoggerBase]] = None,
         input_registry: InputRegistry = INPUT_REGISTRY,
+
+        # support for dattn
+        use_dattn: bool = False,
     ) -> None:
         logger.info(
             "Initializing an LLM engine (v%s) with config: "
@@ -273,6 +278,7 @@
         from vllm.plugins import load_general_plugins
         load_general_plugins()
 
+        self.use_dattn = use_dattn
         self.model_config = model_config
         self.cache_config = cache_config
         self.lora_config = lora_config
@@ -409,6 +415,19 @@
             for v_id in range(parallel_config.pipeline_parallel_size)
         ]
 
+        # Add for supporting dAttention
+        self.step_index = 0
+
+        #self.profile = os.getenv("PROFILE", "False") == "True"
+        self.profile = False
+        if self.profile == True:
+            self.sched_time = 0
+            self.infer_time = 0
+            self.proc_time = 0
+            self.other_time = 0
+            self.total_time = 0
+            self.step_index = 0
+
         # Metric Logging.
         if self.log_stats:
             if stat_loggers is not None:
@@ -433,6 +452,7 @@
                 }
                 self.stat_loggers["prometheus"].info("cache_config",
                                                      self.cache_config)
+        
 
         self.tracer = None
         if self.observability_config.otlp_traces_endpoint:
@@ -464,6 +484,9 @@
         num_gpu_blocks, num_cpu_blocks = (
             self.model_executor.determine_num_available_blocks())
 
+        # DEBUG: in order to trigger the swap faster
+        print(f"num_gpu_blocks: {num_gpu_blocks}, num_cpu_blocks:{num_cpu_blocks}", file=sys.stderr)
+        #num_gpu_blocks = 158 
         if self.cache_config.num_gpu_blocks_override is not None:
             num_gpu_blocks_override = self.cache_config.num_gpu_blocks_override
             logger.info(
@@ -558,6 +581,7 @@
             log_stats=not engine_args.disable_log_stats,
             usage_context=usage_context,
             stat_loggers=stat_loggers,
+            use_dattn = engine_args.use_dattn,   
         )
 
         return engine
@@ -631,6 +655,7 @@
 
         seq = Sequence(seq_id, processed_inputs, block_size, eos_token_id,
                        lora_request, prompt_adapter_request)
+                       #lora_request, prompt_adapter_request, use_dattn=self.use_dattn)
 
         encoder_seq = None
         if 'encoder_prompt_token_ids' in processed_inputs:
@@ -1155,6 +1180,12 @@
                 "Pipeline parallelism is only supported through AsyncLLMEngine "
                 "as performance will be severely degraded otherwise.")
 
+        # Update the step index
+        #print(f"in the beginning of step-{self.step_index}", file=sys.stderr)
+
+        if self.profile == True:
+            T1 = time.time()
+
         # For llm_engine, there is no pipeline parallel support, so the engine
         # used is always 0.
         virtual_engine = 0
@@ -1171,7 +1202,7 @@
         # Clear outputs for each new scheduler iteration
         ctx.request_outputs.clear()
 
-        # Skip the scheduler if there are any remaining steps in the seq groups.
+        # Skip the scheduler if there are any remaining step_index in the seq groups.
         # This ensures that the scheduler is only called again when the current
         # batch has completed.
         if not self._has_remaining_steps(seq_group_metadata_list):
@@ -1198,6 +1229,9 @@
         assert seq_group_metadata_list is not None
         assert scheduler_outputs is not None
 
+        if self.profile == True:
+            T2 = time.time()
+
         if not scheduler_outputs.is_empty():
             finished_requests_ids = self.scheduler[
                 virtual_engine].get_and_reset_finished_requests_ids()
@@ -1209,6 +1243,7 @@
             last_sampled_token_ids = \
                 self._get_last_sampled_token_ids(virtual_engine)
 
+            #print(f"to_swap_out:{scheduler_outputs.blocks_to_swap_out}")
             execute_model_req = ExecuteModelRequest(
                 seq_group_metadata_list=seq_group_metadata_list,
                 blocks_to_swap_in=scheduler_outputs.blocks_to_swap_in,
@@ -1219,15 +1254,25 @@
                 finished_requests_ids=finished_requests_ids,
                 # We use ExecuteModelRequest to pass the last sampled_token_ids
                 # to each of the non-last PP stages for in-place prepare_input.
-                last_sampled_token_ids=last_sampled_token_ids)
+                last_sampled_token_ids=last_sampled_token_ids,
+                # dattn's support
+                immediate_alloc=scheduler_outputs.immediate_allocate, 
+                to_update_blocks=scheduler_outputs.to_update_blocks,
+                )
 
             if allow_async_output_proc:
                 execute_model_req.async_callback = self.async_callbacks[
                     virtual_engine]
+                
+            if self.profile == True:
+                T3 = time.time()
 
             outputs = self.model_executor.execute_model(
                 execute_model_req=execute_model_req)
 
+            if self.profile == True:
+                T4 = time.time()
+
             # We need to do this here so that last step's sampled_token_ids can
             # be passed to the next iteration for PP.
             if self.scheduler_config.is_multi_step:
@@ -1240,13 +1285,16 @@
             # No outputs in this case
             outputs = []
 
+        if self.profile == True:
+            T5 = time.time()
+
         # Finish the current step for all the sequence groups.
         if self.scheduler_config.is_multi_step:
             for seq_group in seq_group_metadata_list:
                 seq_group.finish_step()
 
         if not self._has_remaining_steps(seq_group_metadata_list):
-            # clear the cache if we have finished all the steps.
+            # clear the cache if we have finished all the step_index.
             if self.scheduler_config.is_multi_step:
                 self.cached_scheduler_outputs[0] = SchedulerOutputState()
 
@@ -1274,9 +1322,13 @@
 
                 # Tracing
                 self.do_tracing(scheduler_outputs)
+
         else:
             # Multi-step case
             return ctx.request_outputs
+            
+        if self.profile == True:
+            T6 = time.time()
 
         if not self.has_unfinished_requests():
             # Drain async postprocessor (if exists)
@@ -1291,7 +1343,24 @@
             # queued control plane messages, such as add/remove lora adapters.
             logger.debug("Stopping remote worker execution loop.")
             self.model_executor.stop_remote_worker_execution_loop()
+        
+        if self.profile == True:
+            T7 = time.time()
+            self.sched_time += T3 - T1
+            self.infer_time += T4 - T3
+            self.proc_time += T7 - T4
+            self.other_time += (T7 - T4)+(T3-T1)
+            self.total_time += T7 - T1
+
+            if self.step_index % 512 == 0:
+                print(f"STEP-{self.step_index}: sched-{self.sched_time}, inference-{self.infer_time}, proc-{self.proc_time}, other-{self.other_time}, total-{self.total_time}", file=sys.stderr)
+                self.sched_time = 0
+                self.infer_time = 0
+                self.proc_time = 0
+                self.other_time = 0
+                self.total_time = 0 
 
+        self.step_index += 1
         return ctx.request_outputs
 
     def _has_remaining_steps(
@@ -1302,7 +1371,7 @@
             return False
 
         # TODO(will) this is a sanity check for nowto make sure that all the
-        # seqs are on the same steps. Eventually we will want to do some sort of
+        # seqs are on the same step_index. Eventually we will want to do some sort of
         # dynamic scheduling when doing multi-step decoding.
         ref_remaining_steps = seq_group_metadata_list[0].state.remaining_steps
         if any([
@@ -1310,7 +1379,7 @@
                 for seq_group in seq_group_metadata_list[1:]
         ]):
             raise AssertionError(("All running sequence groups should "
-                                  "have the same remaining steps."))
+                                  "have the same remaining step_index."))
 
         return ref_remaining_steps > 0
 
diff -Nur orig/vllm/engine/output_processor/single_step.py dattn/vllm/engine/output_processor/single_step.py
--- orig/vllm/engine/output_processor/single_step.py	2024-12-30 22:26:31.418992711 +0000
+++ dattn/vllm/engine/output_processor/single_step.py	2024-12-30 22:27:03.536885177 +0000
@@ -11,6 +11,7 @@
                            SequenceOutput, SequenceStatus)
 from vllm.transformers_utils.detokenizer import Detokenizer
 from vllm.utils import Counter
+import inspect
 
 logger = init_logger(__name__)
 
@@ -112,6 +113,7 @@
     def _process_sequence_group_outputs(self, seq_group: SequenceGroup,
                                         outputs: SequenceGroupOutput,
                                         is_async: bool) -> None:
+        import sys
         sampling_params = seq_group.sampling_params
         if sampling_params.best_of == 1 and not sampling_params.use_beam_search:
             # only have one output sample
@@ -131,7 +133,9 @@
                 sampling_params,
                 lora_req=seq_group.lora_request,
             )
+
             if seq.is_finished():
+                #print(f"free_seq at line number: {inspect.currentframe().f_lineno}")
                 for scheduler in self.scheduler:
                     scheduler.free_seq(seq)
             return
diff -Nur orig/vllm/entrypoints/llm.py dattn/vllm/entrypoints/llm.py
--- orig/vllm/entrypoints/llm.py	2024-12-30 22:26:31.418992711 +0000
+++ dattn/vllm/entrypoints/llm.py	2024-12-30 22:27:03.537885172 +0000
@@ -127,7 +127,7 @@
         tokenizer_revision: Optional[str] = None,
         seed: int = 0,
         gpu_memory_utilization: float = 0.9,
-        swap_space: float = 4,
+        swap_space: float = 128,
         cpu_offload_gb: float = 0,
         enforce_eager: Optional[bool] = None,
         max_context_len_to_capture: Optional[int] = None,
diff -Nur orig/vllm/executor/gpu_executor.py dattn/vllm/executor/gpu_executor.py
--- orig/vllm/executor/gpu_executor.py	2024-12-30 22:26:31.419992706 +0000
+++ dattn/vllm/executor/gpu_executor.py	2024-12-30 22:27:03.539885162 +0000
@@ -34,7 +34,6 @@
         """
         assert self.parallel_config.world_size == 1, (
             "GPUExecutor only supports single GPU.")
-
         self.driver_worker = self._create_worker()
         self.driver_worker.init_device()
         self.driver_worker.load_model()
@@ -121,7 +120,6 @@
         # remains to abstract away the device for non-GPU configurations.
         logger.info("# GPU blocks: %d, # CPU blocks: %d", num_gpu_blocks,
                     num_cpu_blocks)
-
         self.driver_worker.initialize_cache(num_gpu_blocks, num_cpu_blocks)
 
     def execute_model(
@@ -175,6 +173,11 @@
     def stop_profile(self) -> None:
         self.driver_worker.stop_profile()
 
+    def update_cache_blocks(self, 
+                            virtual_engine: int, 
+                            immediate_allocate: bool, 
+                            to_update_blocks:List[List[int]]):
+        self.driver_worker.update_cache_blocks(virtual_engine, immediate_allocate, to_update_blocks)
 
 class GPUExecutorAsync(GPUExecutor, ExecutorAsyncBase):
 
diff -Nur orig/vllm/model_executor/models/opt.py dattn/vllm/model_executor/models/opt.py
--- orig/vllm/model_executor/models/opt.py	2024-12-30 22:26:31.429992659 +0000
+++ dattn/vllm/model_executor/models/opt.py	2024-12-30 22:27:03.551885105 +0000
@@ -19,6 +19,9 @@
 """Inference-only OPT model compatible with HuggingFace weights."""
 from typing import Iterable, List, Optional, Tuple
 
+import sys
+import time
+import os
 import torch
 from torch import nn
 from transformers import OPTConfig
@@ -93,6 +96,14 @@
                               scale=self.scaling,
                               cache_config=cache_config,
                               quant_config=quant_config)
+        self.profile = os.getenv("PROFILE", "False") == "True"
+        self.profile = True
+        if self.profile == True:
+            self.attn_time = 0
+            self.attn_prev = 0
+            self.attn_next = 0
+            self.total=0
+            self.step = 0
 
     def forward(
         self,
@@ -100,10 +111,37 @@
         kv_cache: torch.Tensor,
         attn_metadata: AttentionMetadata,
     ) -> torch.Tensor:
+        if self.profile == True:
+            self.step += 1
+            T1 = time.time()
+        
         qkv, _ = self.qkv_proj(hidden_states)
         q, k, v = qkv.chunk(chunks=3, dim=-1)
+
+        if self.profile == True:
+            T2 = time.time()
+
+        #print(f"OPT step-{self.step}, before attn", file=sys.stderr)
         attn_output = self.attn(q, k, v, kv_cache, attn_metadata)
+
+        if self.profile == True:
+            T3 = time.time()
+        
         output, _ = self.out_proj(attn_output)
+
+        if self.profile == True:
+            T4 = time.time()
+            self.attn_prev += T2 - T1
+            self.attn_time += T3 - T2
+            self.attn_next += T4 - T3
+            self.total += T4 - T1
+            if self.step % 512 == 0:
+                torch.set_printoptions(precision=2, sci_mode=False)
+                #print(f"OPTAttention: prev:{self.attn_prev*32}, attn:{self.attn_time*32}, next:{self.attn_next*32}. OPTAttention total:{self.total*32}", file=sys.stderr)
+                self.attn_prev = 0
+                self.attn_time = 0
+                self.attn_next = 0
+                self.total = 0
         return output
 
 
@@ -147,6 +185,9 @@
         self.final_layer_norm = nn.LayerNorm(
             self.embed_dim,
             elementwise_affine=config.layer_norm_elementwise_affine)
+        self.step = 0
+        
+        
 
     def forward(
         self,
@@ -156,12 +197,16 @@
     ) -> torch.Tensor:
         # Self Attention
         residual = hidden_states
+        #if hidden_states != None and kv_cache != None:
+        #    print(f"IN forward of layer, hidden_states:{hidden_states.shape}, kv_cache:{kv_cache.shape}, self.do_layer_norm_before:{self.do_layer_norm_before}")
         # 125m, 1.7B, ..., 175B applies layer norm BEFORE attention
         if self.do_layer_norm_before:
             hidden_states = self.self_attn_layer_norm(hidden_states)
+        
         hidden_states = self.self_attn(hidden_states=hidden_states,
                                        kv_cache=kv_cache,
                                        attn_metadata=attn_metadata)
+
         hidden_states = residual + hidden_states
         # 350m applies layer norm AFTER attention
         if not self.do_layer_norm_before:
@@ -179,6 +224,7 @@
         # 350m applies layer norm AFTER attention
         if not self.do_layer_norm_before:
             hidden_states = self.final_layer_norm(hidden_states)
+
         return hidden_states
 
 
@@ -236,6 +282,14 @@
             OPTDecoderLayer(config, cache_config, quant_config)
             for _ in range(config.num_hidden_layers)
         ])
+        self.profile = os.getenv("PROFILE", "False") == "True"
+        self.profile = False
+        if self.profile == True:
+            self.embed = 0
+            self.layer = 0
+            self.output = 0
+            self.step = 0
+            self.last = 0
 
     def get_input_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:
         return self.embed_tokens(input_ids)
@@ -248,6 +302,11 @@
         attn_metadata: AttentionMetadata,
         inputs_embeds: Optional[torch.Tensor] = None,
     ) -> torch.Tensor:
+
+        if self.profile == True:
+            self.step += 1
+            T1 = time.time()
+
         if inputs_embeds is None:
             inputs_embeds = self.get_input_embeddings(input_ids)
         pos_embeds = self.embed_positions(positions)
@@ -255,14 +314,33 @@
             inputs_embeds, _ = self.project_in(inputs_embeds)
         hidden_states = inputs_embeds + pos_embeds
 
+        if self.profile == True:
+            T2 = time.time()
+
+
         for i in range(len(self.layers)):
             layer = self.layers[i]
             hidden_states = layer(hidden_states, kv_caches[i], attn_metadata)
 
+        if self.profile == True:
+            T3 = time.time()
+
         if self.final_layer_norm is not None:
             hidden_states = self.final_layer_norm(hidden_states)
         if self.project_out is not None:
             hidden_states, _ = self.project_out(hidden_states)
+        
+        if self.profile == True:
+            T4 = time.time()
+            self.embed += T2 - T1
+            self.layer += T3 - T2
+            self.output += T4 - T3
+            if self.step % 512 == 0:
+                print(f"step-{self.step}, embed:{self.embed}, layers:{self.layer}, output:{self.output}")
+                self.embed = 0
+                self.layer = 0
+                self.output = 0
+
         return hidden_states
 
 
diff -Nur orig/vllm/sequence.py dattn/vllm/sequence.py
--- orig/vllm/sequence.py	2024-12-30 22:26:31.413992736 +0000
+++ dattn/vllm/sequence.py	2024-12-30 22:27:03.191886838 +0000
@@ -55,7 +55,8 @@
     WAITING = 0
     RUNNING = 1
     SWAPPED = 2
-    # Note: anything after SWAPPED (2) will be considered
+    SWAPPING = 7
+    # Note: anything after this will be considered
     # as a finished status.
     FINISHED_STOPPED = 3
     FINISHED_LENGTH_CAPPED = 4
@@ -166,6 +167,9 @@
     # is called.
     _new_appended_tokens: List[int] = msgspec.field(default_factory=list)
 
+    # support dattn
+    cache_id: int = -1
+    
     # It is used to compute mrope_position_ids.
     _mrope_position_delta: Optional[int] = None
 
@@ -324,6 +328,7 @@
                 f"prompt_token_ids={self._prompt_token_ids}, "
                 f"output_token_ids={self.output_token_ids}, "
                 f"cumulative_logprob={self.cumulative_logprob}, "
+                f"cache_id={self.cache_id}, "
                 f"get_num_computed_tokens={self.get_num_computed_tokens()}")
 
 
@@ -418,6 +423,10 @@
         # Input + output tokens
         self.tokens: Optional[List[str]] = None
 
+        #self.use_dattn = use_dattn
+        # support dattn
+        self.cache_id = -1
+
     @property
     def n_blocks(self) -> int:
         return (self.get_len() + self.block_size - 1) // self.block_size
@@ -657,6 +666,7 @@
         self.prompt_adapter_request = prompt_adapter_request
         self.encoder_seq = encoder_seq
         self.trace_headers = trace_headers
+        self.swapping_step_index = 0
 
     @property
     def prompt(self) -> Optional[str]:
@@ -930,6 +940,7 @@
     seq_data: Dict[int, SequenceData]
     sampling_params: Optional[SamplingParams]
     block_tables: Dict[int, List[int]]
+    #cache_id: int # supporting dattention. We cannot use request_id directly to find the cache's start address 
     do_sample: bool = True
     pooling_params: Optional[PoolingParams] = None
     lora_request: Optional[LoRARequest] = None
@@ -988,6 +999,20 @@
         assert self.state.current_step < self.state.num_steps
         self.state.current_step += 1
 
+    def __repr__(self) -> str:
+        return (f"SequenceGroupMetadata(\n"
+                f"-- request_id={self.request_id}, \n"
+                f"-- is_prompt={self.is_prompt}, \n"
+                f"-- seq_data={self.seq_data}, \n"
+                f"-- sampling_params={self.sampling_params}, \n"
+                f"-- block_tables={self.block_tables}, \n"
+                f"-- do_sample={self.do_sample}, \n"
+                f"-- token_chunk_size={self.token_chunk_size}, \n"
+                f"-- lora_request={self.lora_request}, \n"
+                f"-- computed_block_nums={self.computed_block_nums}, \n"
+                f"-- multi_modal_data={self.multi_modal_data}, \n"
+                f"-- encoder_seq_data={self.encoder_seq_data}, \n"
+                f"-- cross_block_table={self.cross_block_table})")
 
 class SequenceOutput(
         msgspec.Struct,
@@ -1267,6 +1292,9 @@
     last_sampled_token_ids: Optional[torch.Tensor] = None
     # Async callback
     async_callback: Optional[Callable] = None
+    # new add for dattn
+    immediate_alloc: bool = False
+    to_update_blocks: Dict[int, int]= msgspec.field(default_factory=dict),
 
     @property
     def is_first_multi_step(self) -> bool:
diff -Nur orig/vllm/worker/cache_engine.py dattn/vllm/worker/cache_engine.py
--- orig/vllm/worker/cache_engine.py	2024-12-30 22:26:31.434992635 +0000
+++ dattn/vllm/worker/cache_engine.py	2024-12-30 22:27:03.556885080 +0000
@@ -77,6 +77,7 @@
             num_blocks, self.block_size, self.num_kv_heads, self.head_size)
         pin_memory = is_pin_memory_available() if device == "cpu" else False
         kv_cache: List[torch.Tensor] = []
+        #print(f"_allocate_kv_cache type:{device}, shape:{kv_cache_shape}, pin_memory:{pin_memory}")
         for _ in range(self.num_attention_layers):
             # null block in CpuGpuBlockAllocator requires at least that
             # block to be zeroed-out.
diff -Nur orig/vllm/worker/cache_engine_dattn.py dattn/vllm/worker/cache_engine_dattn.py
--- orig/vllm/worker/cache_engine_dattn.py	1970-01-01 00:00:00.000000000 +0000
+++ dattn/vllm/worker/cache_engine_dattn.py	2024-12-30 22:27:03.557885076 +0000
@@ -0,0 +1,251 @@
+'''
+ Copyright (c) ByteDance Inc.
+ Authors: 
+  - Tongping Liu (tongping.liu@bytedance.com)
+  - https://github.com/vllm-project/vllm/pull/6102/commits
+'''
+"""CacheEngine class for managing the KV cache."""
+from typing import List, Dict, Tuple
+
+import torch
+
+from vllm.attention import get_attn_backend
+from vllm.config import CacheConfig, ModelConfig, ParallelConfig, SchedulerConfig, DeviceConfig
+from vllm.logger import init_logger
+from vllm.utils import STR_DTYPE_TO_TORCH_DTYPE, is_pin_memory_available, get_dtype_size
+
+from vllm import _dattn_ops as dattn
+import subprocess
+from enum import Enum
+import mmap, ctypes
+import re
+import sys
+
+logger = init_logger(__name__)
+
+class CacheEngineDAttn:
+    """Manages the KV cache.
+
+    This class is responsible for initializing and managing the GPU and CPU KV
+    caches. It also provides methods for performing KV cache operations, such
+    as swapping and copying.
+    """
+
+    def __init__(
+        self,
+        cache_config: CacheConfig,
+        model_config: ModelConfig,
+        parallel_config: ParallelConfig,
+        scheduler_config: SchedulerConfig,
+        device_config: DeviceConfig,
+    ) -> None:
+        if device_config.device_type != "cuda":
+            raise RuntimeError("DATTN only support cuda device.")
+
+        self.num_layers = model_config.get_num_layers(parallel_config)
+        
+        head_size = model_config.get_head_size()
+        num_kv_heads = model_config.get_num_kv_heads(parallel_config)
+        self.block_size = cache_config.block_size
+
+        if cache_config.cache_dtype == "auto":
+            dtype = model_config.dtype
+        else:
+            dtype = STR_DTYPE_TO_TORCH_DTYPE[cache_config.cache_dtype]
+
+        dtype_size = get_dtype_size(dtype)
+        self.block_bytes_size = (num_kv_heads * head_size * dtype_size * self.block_size) * self.num_layers  * 2 
+
+        max_batch_size = scheduler_config.max_num_seqs
+        max_seq_len = scheduler_config.max_model_len
+        
+        # If max_seq_len is not divisible by self.block_size,
+        # round up to the nearest value that is.
+        if max_seq_len % self.block_size != 0:
+            logger.warning("Note: max_seq_len mod self.block_size != 0")
+            exit(0)
+
+        token_size = num_kv_heads * head_size
+        sequence_buffer_size = max_seq_len * token_size
+        sequence_buffer_bytes_size = sequence_buffer_size * dtype_size
+        cache_space_size = max_batch_size * sequence_buffer_bytes_size
+        cache_space_bytes_size = cache_space_size * 2
+
+        # reserve size for both K/V cache
+        cache_space_per_req = sequence_buffer_bytes_size * self.num_layers * 2
+        if (cache_space_bytes_size) % self.block_bytes_size != 0:
+            print(f"cache_space_bytes_size:{cache_space_bytes_size}, self.block_bytes_size:{self.block_bytes_size}", file=sys.stderr) 
+            assert (cache_space_bytes_size) % self.block_bytes_size == 0, "cache_space_bytes_size must be divisible by block_bytes_size"
+        
+        cache_space_page_num = cache_space_bytes_size // self.block_bytes_size
+
+        logger.info("CacheEngineDAttn basic info: { block_size: %d, dtype_size: %d, head_size: %d, "
+                    "num_kv_heads: %d, max_seq_len: %d, max_batch_size: %d, self.num_layers: %d,"
+                    "token_size: %d, sequence_buffer_size: %d, cache_space_size: %d, "
+                    "cache_space_bytes_size: %d, cache_space_page_num: %d, cache_space_per_req: %d, cache_block_size: %x}",
+                    self.block_size, dtype_size, head_size,
+                    num_kv_heads, max_seq_len, max_batch_size, self.num_layers, 
+                    token_size, sequence_buffer_size, cache_space_size,
+                    cache_space_bytes_size, cache_space_page_num, cache_space_per_req, self.block_bytes_size)
+
+        max_gpu_memory_size = cache_config.num_gpu_blocks * self.block_bytes_size
+        
+        self.device_cache_allocator = dattn.kvCacheAllocator(max_gpu_memory_size, self.block_bytes_size, cache_space_per_req)
+
+        cpu_cache_num = cache_config.num_cpu_caches 
+
+        # Get attention backend.
+        self.attn_backend = get_attn_backend(
+            model_config.get_num_attention_heads(parallel_config),
+            head_size,
+            num_kv_heads,
+            model_config.get_sliding_window(),
+            model_config.dtype,
+            cache_config.cache_dtype,
+            self.block_size,
+        )
+
+        # A dummy mmap to hold cpu cache's addresses
+        self.mmap = []
+
+        self.kv_cache_ptrs = self._reserve_gpu_kv_cache(max_batch_size)
+        self.gpu_cache = self._create_fake_kv_cache(self.num_layers)
+        self.MADV_COLD = self._find_macro_value("MADV_COLD", "/usr/include/asm-generic/mman-common.h") 
+
+        self.cpu_cache = [None] * cpu_cache_num
+
+        self._reserve_cpu_kv_cache(cpu_cache_num, cache_space_per_req) 
+
+    def _find_macro_value(self, macro_name, header_file):
+        try:
+            # Run grep to find the macro definition in the specified header file
+            result = subprocess.run(
+                ['grep', f'#define {macro_name}', header_file],
+                text=True,
+                capture_output=True,
+                check=True
+            )
+            # Extract the macro value using regex
+            match = re.search(rf'#define {macro_name}\s+(\d+)', result.stdout)
+            if match:
+                return int(match.group(1))
+            else:
+                print(f"{macro_name} not found in {header_file}.", file=sys.stderr)
+                return None
+        except subprocess.CalledProcessError:
+            print(f"Failed to find {macro_name} in {header_file}.", file=sys.stderr)
+            return None
+
+    def get_n_blocks(num_tokens: int) -> int:
+        return (num_tokens + self.block_size - 1) % self.block_size
+    
+    """
+    In dAttention's design, we are required to pass the layer index so
+    that CUDA kernel could use it to get the kv_cache. For other mechanisms, like
+    PagedAttention or vAttention, they are passing different kv_vache for different layers.
+    """
+    def _create_fake_kv_cache(self, num_layers: int) -> List[torch.Tensor]: 
+        fake_kv_caches = []
+
+        for i in range(num_layers):
+            fake_kv_caches.append(torch.tensor(i))
+
+        return fake_kv_caches
+    
+    def _reserve_gpu_kv_cache(self, max_batch_size:int) -> List[int]:
+        kv_cache_ptrs = []
+
+        for i in range(max_batch_size):
+            # Invoke gpu region one by one, which returns the cuda address
+            kv_cache_ptr = self.device_cache_allocator.reserve_cache_region(i)
+            kv_cache_ptrs.append(kv_cache_ptr)
+
+        return kv_cache_ptrs
+
+    def _reserve_cpu_kv_cache(self, cache_num: int, cache_space_per_req: int) -> List[int]:
+        self.cpu_cache = self.device_cache_allocator.alloc_cpu_caches(cache_num, cache_space_per_req) 
+
+    def swap_in(self, src_to_dst: torch.Tensor) -> None:
+        to_swap_in_caches = []
+
+        for pair in src_to_dst:
+            item = pair.flatten()
+
+            cpu_cache_id = item[0]
+            gpu_cache_id = item[1]
+            blocks = item[2]
+
+            gpu_cache_address = self.kv_cache_ptrs[gpu_cache_id]
+            cpu_cache_address = self.cpu_cache[cpu_cache_id]
+
+            size = blocks * self.block_bytes_size
+            #print(f"swapin src:{cpu_cache_id} - address:{hex(cpu_cache_address)}, dest:{gpu_cache_id} - address:{hex(gpu_cache_address)}, blocks:{blocks}, size:{hex(size)}", file=sys.stderr)
+            to_swap_in_caches.append([cpu_cache_address, gpu_cache_id, blocks])
+
+        return to_swap_in_caches
+        #src_to_dests = torch.tensor(to_swap_in_caches, dtype=torch.int64)
+        #self.device_cache_allocator.swap_in_cache(to_swap_in_caches)
+
+    def swap_out(self, src_to_dst: torch.Tensor) -> None:
+        
+        #print(f"CacheEngineDAttn swap_out with src_to_dst:{src_to_dst}", file=sys.stderr)
+        to_swap_out_caches = []
+
+        for pair in src_to_dst:
+            item = pair.flatten()
+
+            gpu_cache_id = item[0]
+            cpu_cache_id = item[1]
+            blocks = item[2]
+
+            gpu_cache_address = self.kv_cache_ptrs[gpu_cache_id]
+            cpu_cache_address = self.cpu_cache[cpu_cache_id]
+            size = blocks * self.block_bytes_size 
+            
+            #print(f"Engine swapout src:{gpu_cache_id} - address:{hex(gpu_cache_address)}, dest:{cpu_cache_id} - address:{hex(cpu_cache_address)}, blocks:{blocks}, size:{hex(size)}", file=sys.stderr)
+            to_swap_out_caches.append([gpu_cache_id, cpu_cache_address, size])
+
+        return to_swap_out_caches
+        #src_to_dests = torch.tensor(to_swap_out_caches, dtype=torch.int64)
+        #self.device_cache_allocator.swap_out_cache(to_swap_out_caches)
+
+    # TODO: we need to implement the copy_blocks 
+    def copy(self, src_to_dsts: torch.Tensor) -> None:
+        self.device_cache_allocator.copy_blocks(self.gpu_cache, src_to_dsts)
+
+    @staticmethod
+    def get_cache_block_size(
+        cache_config: CacheConfig,
+        model_config: ModelConfig,
+        parallel_config: ParallelConfig,
+    ) -> int:
+        head_size = model_config.get_head_size()
+        num_heads = model_config.get_num_kv_heads(parallel_config)
+        num_attention_layers = model_config.get_num_attention_layers(
+            parallel_config)
+
+        key_cache_block = cache_config.block_size * num_heads * head_size
+        value_cache_block = key_cache_block
+        total = num_attention_layers * (key_cache_block + value_cache_block)
+        #print(f"CacheEngineDAttn:head_size:{head_size}, num_heads:{num_heads}, num_attention_layers:{num_attention_layers}, self.block_size: {cache_config.block_size}, key_cache_block:{key_cache_block},total:{total/1024}KB", file=sys.stderr)
+        if cache_config.cache_dtype == "auto":
+            dtype = model_config.dtype
+        else:
+            dtype = STR_DTYPE_TO_TORCH_DTYPE[cache_config.cache_dtype]
+        dtype_size = get_dtype_size(dtype)
+        #print(f"CacheEngineDAttn:cache_config.block_bytes_size:{dtype_size * total}", file=sys.stderr)
+        return dtype_size * total
+
+    def update_cache_blocks(self, immediate_allocate: bool, to_update_blocks: Dict[int, int], 
+                            to_swap_out: List[List[int]], to_swap_in: List[List[int]]):
+        #print(f"innnnnnnnn update_cache_blocks!", file=sys.stderr)
+        to_alloc_blocks = []
+        for cache_id, blocks in to_update_blocks.items():
+            #print(f"innnnnnnnn update_cache_blocks cacheid-{cache_id} blocks-{blocks}")
+            to_alloc_blocks.append([cache_id, blocks])
+
+        self.device_cache_allocator.update_cache_blocks(immediate_allocate, to_alloc_blocks, to_swap_out, to_swap_in)
+        
+
+def _get_dtype_size(dtype: torch.dtype) -> int:
+    return torch.tensor([], dtype=dtype).element_size()
diff -Nur orig/vllm/worker/model_runner.py dattn/vllm/worker/model_runner.py
--- orig/vllm/worker/model_runner.py	2024-12-30 22:26:31.434992635 +0000
+++ dattn/vllm/worker/model_runner.py	2024-12-30 22:27:03.557885076 +0000
@@ -13,6 +13,8 @@
 import torch
 import torch.distributed
 import torch.nn as nn
+import time
+import sys
 
 import vllm.envs as envs
 from vllm.attention import AttentionMetadata, get_attn_backend
@@ -197,6 +199,8 @@
             self.lora_requests.clear()  # type: ignore
             self.prompt_adapter_index_mapping.clear()  # type: ignore
             self.prompt_adapter_prompt_mapping.clear()  # type: ignore
+            self.cache_batch_idx = 0
+            
 
         def __init__(
             self,
@@ -358,7 +362,7 @@
             self.prompt_adapter_request = prompt_adapter_request
             self.multi_modal_inputs = multi_modal_inputs
             self.prefix_cache_hit = prefix_cache_hit
-
+            
             self.n_seqs = len(self.seq_ids)
 
             if not reinit:
@@ -438,7 +442,7 @@
         self.multi_modal_input_mapper = self.runner.multi_modal_input_mapper
         self.finished_requests_ids = finished_requests_ids
         self.decode_only = True
-
+        self.use_dattn = self.runner.use_dattn
         # Intermediate data (data in CPU before going to GPU) for
         # the current sequence group.
         self.inter_data_list: List[
@@ -458,6 +462,12 @@
             self.block_aligned_sliding_window = \
                 self.sliding_window_blocks * self.block_size
 
+        # new add for vmm/dAttention:  we don't need to prepare block_tables & slot_mapping, 
+        # but need to prepare cache_batch_idx, cache_row_mapping, cache_col_mapping
+        self.cache_batch_idx: List[int] = []
+        self.cache_row_mapping : List[int] = []
+        self.cache_col_mapping : List[int] = []
+ 
     def _compute_lens(self, inter_data: InterDataForSeqGroup, seq_idx: int,
                       seq_group_metadata: SequenceGroupMetadata):
         """Compute context length, sequence length and tokens
@@ -694,7 +704,7 @@
                 inter_data.mrope_input_positions[
                     seq_idx] = mrope_input_positions
 
-    def add_seq_group(self, seq_group_metadata: SequenceGroupMetadata):
+    def add_seq_group(self, seq_group_metadata: SequenceGroupMetadata, kv_ptr: int):
         """Add a sequence group to the builder."""
         seq_ids = seq_group_metadata.seq_data.keys()
         n_seqs = len(seq_ids)
@@ -721,12 +731,25 @@
 
         self.inter_data_list.append(inter_data)
 
+
         for seq_idx in range(n_seqs):
             for per_seq_fn in self.per_seq_compute_fns:
                 per_seq_fn(inter_data, seq_idx, seq_group_metadata)
+
         for per_seq_group_fn in self.per_seq_group_compute_fns:
             per_seq_group_fn(inter_data, seq_group_metadata)
 
+        if self.use_dattn:
+            req_id = int(seq_group_metadata.request_id)
+            #print(f"req_id:{req_id}, inter_data.query_lens:{len(inter_data.query_lens)},inter_data.context_lens:{len(inter_data.context_lens)}, ")
+            cache_batch_id = req_id
+            self.cache_batch_idx.append(cache_batch_id)
+            query_len=inter_data.query_lens[0]
+            context_len=inter_data.context_lens[0]
+            #print(f"req_id:{req_id}, query_len:{query_len}, context_len:{context_len}", file=sys.stderr)
+            self.cache_row_mapping.extend([kv_ptr] * query_len)
+            self.cache_col_mapping.extend(list(range(context_len, context_len + query_len)))
+
     def _use_captured_graph(self,
                             batch_size: int,
                             max_decode_seq_len: int,
@@ -838,7 +861,25 @@
 
         # Attention metadata.
         attn_metadata = self.attn_metadata_builder.build(
-            seq_lens, query_lens, cuda_graph_pad_size, batch_size)
+            seq_lens, query_lens, cuda_graph_pad_size, batch_size, self.use_dattn)
+
+        # support for dAttention
+        if self.use_dattn:
+            cache_batch_idx_tensor = torch.tensor(self.cache_batch_idx,
+                                           dtype=torch.int32,
+                                           device=self.runner.device)
+            cache_row_mapping_tensor = torch.tensor(self.cache_row_mapping, 
+                                             dtype=torch.int64, 
+                                             device=self.runner.device)
+            cache_col_mapping_tensor = torch.tensor(self. cache_col_mapping, 
+                                             dtype=torch.int64, 
+                                             device=self.runner.device)
+            attn_metadata.cache_batch_idx = cache_batch_idx_tensor
+            attn_metadata.cache_row_mapping = cache_row_mapping_tensor
+            attn_metadata.cache_col_mapping = cache_col_mapping_tensor
+            attn_metadata.use_dattn = True
+            attn_metadata.block_size = self.runner.block_size
+            attn_metadata.num_layers = self.runner.num_layers  
 
         # LoRA data.
         lora_requests = set()
@@ -996,6 +1037,7 @@
             .create_input_mapper(model_config)
         self.mm_registry.init_mm_limits_per_prompt(self.model_config)
 
+            
         # Lazy initialization
         self.model: nn.Module  # Set after load_model
         # Set after load_model.
@@ -1010,6 +1052,34 @@
         self.sampling_metadata_cache: SamplingMetadataCache = \
             SamplingMetadataCache()
 
+        # Add dAttention support
+        self.use_dattn = cache_config.use_dattn
+        if self.use_dattn:
+            if self.lora_config:
+                #TODO:
+                raise NotImplementedError("DATTN is not supported with LoRA ")
+            if self.sliding_window:
+                #TODO:
+                raise NotImplementedError("DATTN is not supported with sliding window")
+
+        self.kv_cache_ptrs: Optional[List[int]] = None
+        self.num_layers: int = 0
+
+        # The default cache index used for _warm_up_model()
+        self.default_cache_idx: int = 0
+
+    # Initialize kv_cache_ptrs when dAttention is used
+    def init_kv_cache_attribute(self, kv_cache_ptrs: List[int], block_size: int, num_layers: int) -> None:
+        self.kv_cache_ptrs = kv_cache_ptrs
+        self.block_size = block_size
+        self.num_layers = num_layers
+
+    def _get_kv_ptr(self, index: int) -> int:
+        if self.kv_cache_ptrs is not None:
+            return self.kv_cache_ptrs[index]
+        else: 
+            return 0
+    
     def load_model(self) -> None:
         logger.info("Starting to load model %s...", self.model_config.model)
         with CudaMemoryProfiler() as m:
@@ -1120,7 +1190,7 @@
     def _prepare_model_input_tensors(
         self,
         seq_group_metadata_list: List[SequenceGroupMetadata],
-        finished_requests_ids: Optional[List[str]] = None
+        finished_requests_ids: Optional[List[str]] = None,
     ) -> TModelInputForGPU:
         """Helper method to prepare the model input based on a given sequence
         group. Prepares metadata needed for the base model forward pass but not
@@ -1138,10 +1208,12 @@
         """
         builder = self._builder_cls(weakref.proxy(self), finished_requests_ids)
         for seq_group_metadata in seq_group_metadata_list:
-            builder.add_seq_group(seq_group_metadata)
+            request_id = int(seq_group_metadata.request_id)
+            cache_id = seq_group_metadata.seq_data[request_id].cache_id 
+            #print(f"in _prepare_model_input_tensors: request_id-{request_id}, cache_id:{cache_id} , kv_ptr:{hex(self._get_kv_ptr(cache_id))}", file=sys.stderr)
+            builder.add_seq_group(seq_group_metadata, self._get_kv_ptr(cache_id) if self.use_dattn else 0)
 
         builder.reset_cached_inter_data()
-
         return builder.build()  # type: ignore
 
     @torch.inference_mode()
@@ -1335,14 +1407,17 @@
                     "`gpu_memory_utilization` or enforcing eager mode. "
                     "You can also reduce the `max_num_seqs` as needed "
                     "to decrease memory usage.")
+        
         start_time = time.perf_counter()
-
         # Prepare dummy inputs. These will be reused for all batch sizes.
         max_batch_size = self.max_batchsize_to_capture
         input_tokens = torch.zeros(max_batch_size, dtype=torch.long).cuda()
         input_positions = torch.zeros(max_batch_size, dtype=torch.long).cuda()
         if self.model_is_mrope:
             input_positions = torch.tile(input_positions, (3, 1))
+        
+        #print(f"input_tokens:{input_tokens.shape},input_positions:{input_positions.shape} ")
+        
         # Prepare dummy previous_hidden_states only if needed by the model.
         # This is used by draft models such as EAGLE.
         previous_hidden_states = None
@@ -1378,13 +1453,35 @@
             # memory usage of CUDA graph.
             for virtual_engine in range(
                     self.parallel_config.pipeline_parallel_size):
+                if self.use_dattn:
+                    cache_batch_idx = self.default_cache_idx
+                    cache_address = self._get_kv_ptr(cache_batch_idx) 
+                
                 for batch_size in reversed(batch_size_capture_list):
+                    #print(f"batch_size_capture_list:{len(batch_size_capture_list)}, batch_size:{batch_size}")
                     attn_metadata = (
                         self.attn_state.graph_capture_get_metadata_for_batch(
                             batch_size,
                             is_encoder_decoder_model=self.model_config.
                             is_encoder_decoder_model))
 
+                    if self.use_dattn:
+                        #print(f"attn_metadata:{attn_metadata.seq_lens}, context_len:{attn_metadata}")
+                        attn_metadata.cache_batch_idx = torch.tensor([cache_batch_idx]*batch_size,
+                                                                    dtype=torch.int32,
+                                                                    device=self.device)
+                        attn_metadata.cache_row_mapping = torch.tensor([cache_address]*batch_size,
+                                                                    dtype=torch.int64,
+                                                                    device=self.device)
+                        attn_metadata.cache_col_mapping = torch.tensor([0] * batch_size,
+                                                                    dtype=torch.int64,
+                                                                    device=self.device)
+
+                        #print(f"attn_metadata.cache_batch_idx:{attn_metadata.cache_batch_idx.shape}, row_mapping:{attn_metadata.cache_row_mapping.shape}, col_mapping:{attn_metadata.cache_col_mapping}") 
+                        attn_metadata.use_dattn = True
+                        attn_metadata.block_size = self.block_size
+                        attn_metadata.num_layers = self.num_layers  
+
                     if self.lora_config:
                         lora_mapping = LoRAMapping(
                             **dict(index_mapping=[0] * batch_size,
@@ -1399,6 +1496,7 @@
                         )
                         self.set_active_prompt_adapters(
                             set(), prompt_adapter_mapping)
+
                     graph_runner = CUDAGraphRunner(
                         self.model, self.attn_backend.get_name(),
                         self.attn_state.graph_clone(batch_size),
@@ -1486,6 +1584,24 @@
         ModelInputForGPUWithSamplingMetadata)
     _builder_cls: Type[ModelInputForGPUBuilder] = ModelInputForGPUBuilder
 
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        self.profile = False
+        if self.profile == True:
+            self.input = 0
+            self.steps = 0
+            self.before = 0
+            self.execute = 0
+            self.after = 0
+            self.total = 0
+            self.sample = 0
+            self.start_event = torch.cuda.Event(enable_timing=True)
+            self.end_event = torch.cuda.Event(enable_timing=True)
+
+        #self.model_input = model_input_data
+        #self.builder = builder_data
+
+
     def make_model_input_from_broadcasted_tensor_dict(
         self,
         tensor_dict: Dict[str, Any],
@@ -1516,6 +1632,9 @@
 
         If cuda graph is required, this API automatically pads inputs.
         """
+        if self.profile == True:
+            T1 = time.time()
+
         model_input = self._prepare_model_input_tensors(
             seq_group_metadata_list, finished_requests_ids)
         if get_pp_group().is_last_rank:
@@ -1529,11 +1648,17 @@
             sampling_metadata = None
         is_prompt = (seq_group_metadata_list[0].is_prompt
                      if seq_group_metadata_list else None)
-        return dataclasses.replace(model_input,
+        res = dataclasses.replace(model_input,
                                    sampling_metadata=sampling_metadata,
                                    is_prompt=is_prompt,
                                    virtual_engine=virtual_engine)
 
+        if self.profile == True:
+            T2 = time.time()
+            self.input += T2 - T1
+
+        return res 
+
     @torch.inference_mode()
     @dump_input_when_exception(exclude_args=[0], exclude_kwargs=["self"])
     def execute_model(
@@ -1546,6 +1671,11 @@
         if num_steps > 1:
             raise ValueError("num_steps > 1 is not supported in ModelRunner")
 
+        if self.profile == True:
+            self.steps += 1
+            T1 = time.time()
+
+        #print(f"NOOOOOW,  execute_model now!!!", file=sys.stderr)
         if self.lora_config:
             assert model_input.lora_requests is not None
             assert model_input.lora_mapping is not None
@@ -1587,6 +1717,11 @@
             model_forward_end = torch.cuda.Event(enable_timing=True)
             model_forward_start.record()
 
+        if self.profile == True:
+            T2 = time.time()
+
+        #self.start_event.record()
+        
         hidden_or_intermediate_states = model_executable(
             input_ids=model_input.input_tokens,
             positions=model_input.input_positions,
@@ -1620,15 +1755,27 @@
                     torch.tensor(model_forward_time + orig_model_forward_time))
             return hidden_or_intermediate_states
 
+        #self.end_event.record()
+        #torch.cuda.synchronize()
+        if self.profile == True:
+            T3 = time.time()
+
         logits = self.model.compute_logits(hidden_or_intermediate_states,
                                            model_input.sampling_metadata)
-
         if not self.is_driver_worker:
             return []
 
         if model_input.async_callback is not None:
             model_input.async_callback()
 
+        
+        if self.profile == True:
+            T4 = time.time()
+
+        #import pdb 
+        #pdb.set_trace() 
+        #check_data=model_input.sampling_metadata.seq_groups[0].seq_data[0] 
+        #print(f"Sampling logits:{logits.shape}, sampling_metadata prompts-{len(check_data.prompt_token_ids)}, output-{len(check_data.output_token_ids)}\n")
         # Sample the next token.
         output: SamplerOutput = self.model.sample(
             logits=logits,
@@ -1666,6 +1813,24 @@
 
             output.hidden_states = hidden_states
 
+        if self.profile == True:
+            T5 = time.time()
+
+            self.before += T2 - T1
+            #self.execute += self.start_event.elapsed_time(self.end_event)
+            self.execute += T3 - T2
+            self.after += T4 - T3
+            self.sample += T5 - T4
+            self.total += T5 - T1
+            if self.steps % 512 == 0:
+                print(f"self.before:{self.before}, self.execute:{self.execute}, self.after:{self.after}, self.sample:{self.sample}, self.input:{self.input}, self.total:{self.total}", file=sys.stderr)
+                self.before = 0
+                self.execute = 0
+                self.after = 0
+                self.sample = 0
+                self.input = 0
+                self.total = 0
+
         return [output]
 
 
@@ -1702,6 +1867,7 @@
         **kwargs,
     ) -> Union[torch.Tensor, IntermediateTensors]:
         assert self._graph is None
+        #print(f"_NUM_WARMUP_ITERS:{_NUM_WARMUP_ITERS}, kv_caches:{kv_caches}")
         # Run the model a few times without capturing the graph.
         # This is to make sure that the captured graph does not include the
         # kernel launches for initial benchmarking (e.g., Triton autotune).
@@ -1784,7 +1950,8 @@
         # Copy the input tensors to the input buffers.
         self.input_buffers["input_ids"].copy_(input_ids, non_blocking=True)
         self.input_buffers["positions"].copy_(positions, non_blocking=True)
-        self.input_buffers["slot_mapping"].copy_(attn_metadata.slot_mapping,
+        if not attn_metadata.use_dattn:
+            self.input_buffers["slot_mapping"].copy_(attn_metadata.slot_mapping,
                                                  non_blocking=True)
         self.attn_state.prepare_graph_input_buffers(
             self.input_buffers, attn_metadata, self._is_encoder_decoder_model)
diff -Nur orig/vllm/worker/worker.py dattn/vllm/worker/worker.py
--- orig/vllm/worker/worker.py	2024-12-30 22:26:31.434992635 +0000
+++ dattn/vllm/worker/worker.py	2024-12-30 22:27:03.556885080 +0000
@@ -1,6 +1,7 @@
 """A GPU worker class."""
 import gc
 import os
+import sys
 from typing import Dict, List, Optional, Set, Tuple, Type, Union
 
 import torch
@@ -24,6 +25,7 @@
 from vllm.sequence import (ExecuteModelRequest, IntermediateTensors,
                            SequenceGroupMetadata, SequenceGroupMetadataDelta)
 from vllm.worker.cache_engine import CacheEngine
+from vllm.worker.cache_engine_dattn import CacheEngineDAttn
 from vllm.worker.embedding_model_runner import EmbeddingModelRunner
 from vllm.worker.enc_dec_model_runner import EncoderDecoderModelRunner
 from vllm.worker.model_runner import GPUModelRunnerBase, ModelRunner
@@ -69,6 +71,7 @@
         self.distributed_init_method = distributed_init_method
         self.lora_config = lora_config
         self.load_config = load_config
+        self.use_dattn = cache_config.use_dattn
         self.prompt_adapter_config = prompt_adapter_config
         self.is_driver_worker = is_driver_worker
         if parallel_config and is_driver_worker:
@@ -112,7 +115,7 @@
         )
         # Uninitialized cache engine. Will be initialized by
         # initialize_cache.
-        self.cache_engine: List[CacheEngine]
+        self.cache_engine: List[Union[CacheEngine, CacheEngineDAttn]]
         # Initialize gpu_cache as embedding models don't initialize kv_caches
         self.gpu_cache: Optional[List[List[torch.Tensor]]] = None
         self._seq_group_metadata_cache: Dict[str, SequenceGroupMetadata] = {}
@@ -218,13 +221,15 @@
         # cache blocks that can be allocated with the remaining free memory.
         torch.cuda.empty_cache()
 
+        print(f"Before profiling run", file=sys.stderr)
+
         # Execute a forward pass with dummy inputs to profile the memory usage
         # of the model.
         self.model_runner.profile_run()
 
+        print(f"in the end of profiling run", file=sys.stderr)
         # Calculate the number of blocks that can be allocated with the
         # profiled peak memory.
-        torch.cuda.synchronize()
         free_gpu_memory, total_gpu_memory = torch.cuda.mem_get_info()
         # NOTE(woosuk): Here we assume that the other processes using the same
         # GPU did not change their memory usage during the profiling.
@@ -239,6 +244,8 @@
         num_gpu_blocks = int(
             (total_gpu_memory * self.cache_config.gpu_memory_utilization -
              peak_memory) // cache_block_size)
+
+        print(f"self.cachex_config.swap_space:{self.cache_config.swap_space_bytes}, cache_block_size:{cache_block_size}-{hex(cache_block_size)}", file=sys.stderr)
         num_cpu_blocks = int(self.cache_config.swap_space_bytes //
                              cache_block_size)
         num_gpu_blocks = max(num_gpu_blocks, 0)
@@ -267,19 +274,35 @@
 
     def _init_cache_engine(self):
         assert self.cache_config.num_gpu_blocks is not None
-        self.cache_engine = [
-            CacheEngine(self.cache_config, self.model_config,
+        if self.use_dattn:   # Using DAttn
+            #print(f"\n\nNOOOOOW, before initialization of CacheEngineDAttn!")
+            self.cache_engine = [
+                CacheEngineDAttn(self.cache_config, self.model_config,
+                            self.parallel_config, self.scheduler_config,
+                            self.device_config)
+                for _ in range(self.parallel_config.pipeline_parallel_size)
+            ]
+
+            # Initialize kv_cache_ptrs immediately
+            for ve in range(self.parallel_config.pipeline_parallel_size):
+                self.model_runner.init_kv_cache_attribute(self.cache_engine[ve].kv_cache_ptrs, self.cache_engine[ve].block_size, self.cache_engine[ve].num_layers)
+        else: 
+            self.cache_engine = [
+                CacheEngine(self.cache_config, self.model_config,
                         self.parallel_config, self.device_config)
-            for _ in range(self.parallel_config.pipeline_parallel_size)
-        ]
+                for _ in range(self.parallel_config.pipeline_parallel_size)
+            ]
+
         self.gpu_cache = [
             self.cache_engine[ve].gpu_cache
             for ve in range(self.parallel_config.pipeline_parallel_size)
         ]
 
     def _warm_up_model(self) -> None:
+  
         if not self.model_config.enforce_eager:
-            self.model_runner.capture_model(self.gpu_cache)
+            self.model_runner.capture_model(self.gpu_cache) 
+        
         # Reset the seed to ensure that the random state is not affected by
         # the model initialization and profiling.
         set_random_seed(self.model_config.seed)
@@ -300,16 +323,28 @@
         num_seq_groups = len(execute_model_req.seq_group_metadata_list)
         # `blocks_to_swap_in` and `blocks_to_swap_out` are cpu tensors.
         # they contain parameters to launch cudamemcpyasync.
-        blocks_to_swap_in = torch.tensor(execute_model_req.blocks_to_swap_in,
+        if self.use_dattn:
+            blocks_to_swap_in = torch.tensor(execute_model_req.blocks_to_swap_in,
+                                         device="cpu",
+                                         dtype=torch.int64)
+            blocks_to_swap_out = torch.tensor(execute_model_req.blocks_to_swap_out,
+                                          device="cpu",
+                                          dtype=torch.int64)
+
+            blocks_to_copy = torch.tensor(execute_model_req.blocks_to_copy,
+                                      device=self.device,
+                                      dtype=torch.int64)
+        else:
+            blocks_to_swap_in = torch.tensor(execute_model_req.blocks_to_swap_in,
                                          device="cpu",
                                          dtype=torch.int64).view(-1, 2)
-        blocks_to_swap_out = torch.tensor(execute_model_req.blocks_to_swap_out,
+            blocks_to_swap_out = torch.tensor(execute_model_req.blocks_to_swap_out,
                                           device="cpu",
                                           dtype=torch.int64).view(-1, 2)
-        # `blocks_to_copy` is a gpu tensor. The src and tgt of
-        # blocks to copy are in the same device, and `blocks_to_copy`
-        # can be used directly within cuda kernels.
-        blocks_to_copy = torch.tensor(execute_model_req.blocks_to_copy,
+            # `blocks_to_copy` is a gpu tensor. The src and tgt of
+            # blocks to copy are in the same device, and `blocks_to_copy`
+            # can be used directly within cuda kernels.
+            blocks_to_copy = torch.tensor(execute_model_req.blocks_to_copy,
                                       device=self.device,
                                       dtype=torch.int64).view(-1, 2)
 
@@ -323,8 +358,29 @@
         )
 
     @torch.inference_mode()
+    def update_cache_blocks(self, virtual_engine: int, immediate_allocate: bool, to_update_blocks: Dict[int, int], to_swap_out: List[List[int]], to_swap_in: List[List[int]]) -> None:
+       self.cache_engine[virtual_engine].update_cache_blocks(immediate_allocate, to_update_blocks, to_swap_out, to_swap_in) 
+
+    @torch.inference_mode()
+    def execute_worker_dattn(self, worker_input: WorkerInput) -> Tuple[List[List[int]], List[List[int]]]:
+        #print(f"NOOOOOW, execute_worker before swapin and swapout")
+        virtual_engine = worker_input.virtual_engine
+        to_swap_out = None
+        to_swap_in  = None
+        if worker_input.blocks_to_swap_out is not None:
+            to_swap_out = self.cache_engine[virtual_engine].swap_out(
+                           worker_input.blocks_to_swap_out)
+        if worker_input.blocks_to_swap_in is not None:
+            to_swap_in = self.cache_engine[virtual_engine].swap_in(
+                         worker_input.blocks_to_swap_in)
+
+        return to_swap_out, to_swap_in
+ 
+    @torch.inference_mode()
     def execute_worker(self, worker_input: WorkerInput) -> None:
+        #print(f"NOOOOOW, execute_worker before swapin and swapout")
         virtual_engine = worker_input.virtual_engine
+
         # Issue cache operations.
         if (worker_input.blocks_to_swap_in is not None
                 and worker_input.blocks_to_swap_in.numel() > 0):
@@ -431,7 +487,12 @@
     def get_cache_block_size_bytes(self) -> int:
         """Get the size of the KV cache block size in bytes.
         """
-        return CacheEngine.get_cache_block_size(self.cache_config,
+        if self.use_dattn:
+            return CacheEngineDAttn.get_cache_block_size(self.cache_config,
+                                                       self.model_config,
+                                                       self.parallel_config)
+        else:
+            return CacheEngine.get_cache_block_size(self.cache_config,
                                                 self.model_config,
                                                 self.parallel_config)
 
diff -Nur orig/vllm/worker/worker_base.py dattn/vllm/worker/worker_base.py
--- orig/vllm/worker/worker_base.py	2024-12-30 22:26:31.434992635 +0000
+++ dattn/vllm/worker/worker_base.py	2024-12-30 22:27:03.557885076 +0000
@@ -20,6 +20,10 @@
                                            ModelRunnerBase,
                                            ModelRunnerInputBase)
 
+import sys
+import os
+import time
+
 logger = init_logger(__name__)
 
 
@@ -131,6 +135,7 @@
     blocks_to_swap_out: Optional[torch.Tensor] = None
     blocks_to_copy: Optional[torch.Tensor] = None
     virtual_engine: int = 0
+
     num_steps: int = 1
 
     @classmethod
@@ -162,6 +167,7 @@
             "blocks_to_swap_out": self.blocks_to_swap_out,
             "blocks_to_copy": self.blocks_to_copy,
             "virtual_engine": self.virtual_engine,
+            # new add for dattn
             "num_steps": self.num_steps,
         }
 
@@ -221,6 +227,14 @@
         """
         raise NotImplementedError
 
+    @abstractmethod
+    def update_cache_blocks(self, virtual_engine: int, immediate_allocate: bool, to_update_blocks: Dict[int, int], to_swap_out: List[List[int]], to_swap_in: List[List[int]]) -> None:
+        """
+        Process an execution request.
+        """
+        raise NotImplementedError
+
+
     def _get_worker_input_from_broadcast(
         self
     ) -> Optional[Tuple[BroadcastableModelInput, WorkerInput, Dict[
@@ -306,9 +320,21 @@
 
         model_input, worker_input, kwargs = inputs
         num_steps = worker_input.num_steps
+        if self.use_dattn:
+            to_swap_out_caches, to_swap_in_caches = self.execute_worker_dattn(worker_input)
 
-        self.execute_worker(worker_input)
+            # If any of these not zero
+            if (execute_model_req.to_update_blocks or to_swap_out_caches or to_swap_in_caches): 
+                # Perform the update cache blocks in a function. 
+                self.update_cache_blocks(model_input.virtual_engine, execute_model_req.immediate_alloc,  execute_model_req.to_update_blocks, to_swap_out_caches, to_swap_in_caches)
+        else:
+            self.execute_worker(worker_input)
+        #print(f"after execute worker now num_steps:{num_steps}", file=sys.stderr)
 
+        # Now let's update cache blocks. Note that this has to be done after execute_worker
+        # When swapping out, we may evict some GPU caches that can be done after the successful swapping out. 
+     
+        
         # If there is no input, we don't need to execute the model.
         if worker_input.num_seq_groups == 0:
             return []
@@ -366,6 +392,7 @@
         assert execute_model_req is not None, (
             "_execute_model_spmd() requires each worker to take in an "
             "ExecuteModelRequest")
+        
         worker_input: WorkerInput = self.prepare_worker_input(
             execute_model_req=execute_model_req)
         model_input: ModelRunnerInputBase = (
